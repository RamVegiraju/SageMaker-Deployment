{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c667fe87-7180-4aff-a14e-d1b9987635fb",
   "metadata": {},
   "source": [
    "# Pre-Trained Sklearn Model Batch Transform\n",
    "\n",
    "Setup: Studio ml.3.medium Data Science Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8d4c8d-6295-463c-8ae3-77102cf5bdc1",
   "metadata": {},
   "source": [
    "## Locally Train and Serialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4678f3b-7043-4612-b256-469ed9f89d0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse, os\n",
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "import joblib\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab5e6ec-2de3-4b1b-82f4-3c32a989ba9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('petrol_consumption.csv')\n",
    "    \n",
    "############\n",
    "#Preprocessing data\n",
    "############\n",
    "X = df.drop('Petrol_Consumption', axis = 1)\n",
    "y = df['Petrol_Consumption']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e1e8b5-1e1a-4677-8706-004c9cebab21",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "#Model Building\n",
    "###########\n",
    "regressor = RandomForestRegressor(n_estimators=20)\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6ed881-7dc7-4bbd-8c0f-f72d3d53cdd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eda6bd-042d-4580-865f-3883d7421aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(regressor, open(\"model.joblib\", 'wb')) #serialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969829b1-b3e7-4b60-bcc7-eb09f361d8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = joblib.load(open(\"model.joblib\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071c299d-f871-4868-b240-66d5bdcb7193",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = loaded_model.predict(X_test) #test the serialized model to ensure working properly\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0268f5e8-7dcd-46a5-9090-398759a56abd",
   "metadata": {},
   "source": [
    "## Tar the model artifact\n",
    "\n",
    "This can be a joblib or a pkl file as long as it is captured in the model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8162945f-cafb-4dde-a844-9e0afa32f472",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tar -cvpzf model.tar.gz model.joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9326454c-be9c-45f5-8ace-a7b2d26a36c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 cp model.tar.gz s3://pre-trained-sklearn/model.tar.gz #replace this path with where your S3 path (can be anything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0469f6e9-e633-443d-871f-49cc1300021e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-01 02:49:24      12498 model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://pre-trained-sklearn/ #make sure model.tar.gz was uploaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccf6848-cbf5-4b36-a7f3-5674c736c872",
   "metadata": {},
   "source": [
    "## Creating Test Data for Batch Inference\n",
    "\n",
    "The train part of this is not necessary for this example as we are not doing a SageMaker training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfcfd31-fd1a-450f-bed9-1e9863a944a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"petrol_consumption.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab68149-73a8-4b49-9831-0af66f0e198d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Splitting data in 80-20 split to use testing data for model inference later\n",
    "train = df.iloc[:35,:]\n",
    "test = df.iloc[36:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4a3a55-92b8-4707-bc1c-c97ce6948834",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Train and test csv\n",
    "train.to_csv('train.csv', index=False)\n",
    "test.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d59d808e-a58c-4156-9b22-07328d75fbd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create a sagemaker session to be able to upload data to s3\n",
    "import boto3\n",
    "import sagemaker\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "#Uploading data to S3 bucket titled \"tf-iris-data\"\n",
    "prefix = \"sklearn-petrol-data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3be0687b-79d8-4737-8613-dc840e12e0e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data_path = sagemaker_session.upload_data('test.csv', key_prefix=prefix + '/test') #replace with your s3 path if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cf96d4a-7b68-4757-a3ce-37c9552847ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-474422712127/sklearn-petrol-data/test/test.csv'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_path #this is what we pass into batch inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ee8f2ca-3228-46d0-a5d7-55476a04578a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-01 03:13:18        378 test.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://sagemaker-us-east-1-474422712127/sklearn-petrol-data/test/ #replace with your s3 path make sure data is present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43d4b467-95df-4103-b1f2-3f6e41d23a81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699cb692-a4e7-4ede-b791-a7cf72f4cca9",
   "metadata": {},
   "source": [
    "## Create SKLearn Model\n",
    "\n",
    "Here we pass the model data and the inference script which contains our pre and post processing. This is located in the same directory as this notebook. Adjust FWK version as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78c65002-6676-4523-a80f-fdb92a95725f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.sklearn import SKLearnModel\n",
    "\n",
    "sk_estimator = SKLearnModel(model_data = 's3://pre-trained-sklearn/model.tar.gz',\n",
    "                       entry_point= 'inference.py',\n",
    "                       role=role,\n",
    "                       framework_version='0.23-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28b7e30-eec0-4cde-b727-3e4413a72ebd",
   "metadata": {},
   "source": [
    "## Batch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "051a5c22-ed44-4da2-a4be-a3ffb2ad9f2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a SKLearn Transformer from the trained SKLearn Estimator\n",
    "transformer = sk_estimator.transformer(\n",
    "    instance_count=1, instance_type=\"ml.m5.xlarge\", assemble_with=\"Line\", accept=\"text/csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f82aace7-6514-4894-ba0c-d65829107ed0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...........................\n",
      "\u001b[34m2023-02-01 03:17:50,438 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-02-01 03:17:50,440 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-02-01 03:17:50,441 INFO - sagemaker-containers - nginx config: \u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[35m2023-02-01 03:17:50,438 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2023-02-01 03:17:50,440 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2023-02-01 03:17:50,441 INFO - sagemaker-containers - nginx config: \u001b[0m\n",
      "\u001b[35mworker_processes auto;\u001b[0m\n",
      "\u001b[35mdaemon off;\u001b[0m\n",
      "\u001b[35mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[35merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[35mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[35mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m2023-02-01 03:17:50,625 INFO - sagemaker-containers - Module inference does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35m2023-02-01 03:17:50,625 INFO - sagemaker-containers - Module inference does not provide a setup.py. \u001b[0m\n",
      "\u001b[35mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2023-02-01 03:17:50,625 INFO - sagemaker-containers - Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2023-02-01 03:17:50,626 INFO - sagemaker-containers - Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2023-02-01 03:17:50,626 INFO - sagemaker-containers - Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python3 -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: inference\n",
      "  Building wheel for inference (setup.py): started\n",
      "  Building wheel for inference (setup.py): finished with status 'done'\n",
      "  Created wheel for inference: filename=inference-1.0.0-py2.py3-none-any.whl size=3900 sha256=72c929fe8a87c1cc509579153316d9b96efa4512c664047b95b64d36a2abb1c2\n",
      "  Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-0qto1eap/wheels/3e/0f/51/2f1df833dd0412c1bc2f5ee56baac195b5be563353d111dca6\u001b[0m\n",
      "\u001b[34mSuccessfully built inference\u001b[0m\n",
      "\u001b[34mInstalling collected packages: inference\u001b[0m\n",
      "\u001b[34mSuccessfully installed inference-1.0.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.3.1 -> 23.0\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[35m2023-02-01 03:17:50,625 INFO - sagemaker-containers - Generating setup.cfg\u001b[0m\n",
      "\u001b[35m2023-02-01 03:17:50,626 INFO - sagemaker-containers - Generating MANIFEST.in\u001b[0m\n",
      "\u001b[35m2023-02-01 03:17:50,626 INFO - sagemaker-containers - Installing module with the following command:\u001b[0m\n",
      "\u001b[35m/miniconda3/bin/python3 -m pip install . \u001b[0m\n",
      "\u001b[35mProcessing /opt/ml/code\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: inference\n",
      "  Building wheel for inference (setup.py): started\n",
      "  Building wheel for inference (setup.py): finished with status 'done'\n",
      "  Created wheel for inference: filename=inference-1.0.0-py2.py3-none-any.whl size=3900 sha256=72c929fe8a87c1cc509579153316d9b96efa4512c664047b95b64d36a2abb1c2\n",
      "  Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-0qto1eap/wheels/3e/0f/51/2f1df833dd0412c1bc2f5ee56baac195b5be563353d111dca6\u001b[0m\n",
      "\u001b[35mSuccessfully built inference\u001b[0m\n",
      "\u001b[35mInstalling collected packages: inference\u001b[0m\n",
      "\u001b[35mSuccessfully installed inference-1.0.0\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35m[notice] A new release of pip available: 22.3.1 -> 23.0\u001b[0m\n",
      "\u001b[35m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[2023-02-01 03:17:53 +0000] [38] [INFO] Starting gunicorn 20.0.4\u001b[0m\n",
      "\u001b[34m[2023-02-01 03:17:53 +0000] [38] [INFO] Listening at: unix:/tmp/gunicorn.sock (38)\u001b[0m\n",
      "\u001b[34m[2023-02-01 03:17:53 +0000] [38] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2023-02-01 03:17:53 +0000] [41] [INFO] Booting worker with pid: 41\u001b[0m\n",
      "\u001b[34m[2023-02-01 03:17:53 +0000] [42] [INFO] Booting worker with pid: 42\u001b[0m\n",
      "\u001b[34m[2023-02-01 03:17:53 +0000] [43] [INFO] Booting worker with pid: 43\u001b[0m\n",
      "\u001b[34m[2023-02-01 03:17:53 +0000] [47] [INFO] Booting worker with pid: 47\u001b[0m\n",
      "\u001b[35m[2023-02-01 03:17:53 +0000] [38] [INFO] Starting gunicorn 20.0.4\u001b[0m\n",
      "\u001b[35m[2023-02-01 03:17:53 +0000] [38] [INFO] Listening at: unix:/tmp/gunicorn.sock (38)\u001b[0m\n",
      "\u001b[35m[2023-02-01 03:17:53 +0000] [38] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[35m[2023-02-01 03:17:53 +0000] [41] [INFO] Booting worker with pid: 41\u001b[0m\n",
      "\u001b[35m[2023-02-01 03:17:53 +0000] [42] [INFO] Booting worker with pid: 42\u001b[0m\n",
      "\u001b[35m[2023-02-01 03:17:53 +0000] [43] [INFO] Booting worker with pid: 43\u001b[0m\n",
      "\u001b[35m[2023-02-01 03:17:53 +0000] [47] [INFO] Booting worker with pid: 47\u001b[0m\n",
      "\u001b[34m2023-02-01 03:17:55,774 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator DecisionTreeRegressor from version 0.22.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator RandomForestRegressor from version 0.22.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [01/Feb/2023:03:17:56 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [01/Feb/2023:03:17:56 +0000] \"GET /execution-parameters HTTP/1.1\" 404 232 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m2023-02-01 03:17:56,369 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2023-02-01 03:17:55,774 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator DecisionTreeRegressor from version 0.22.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator RandomForestRegressor from version 0.22.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [01/Feb/2023:03:17:56 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [01/Feb/2023:03:17:56 +0000] \"GET /execution-parameters HTTP/1.1\" 404 232 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m2023-02-01 03:17:56,369 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator DecisionTreeRegressor from version 0.22.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator RandomForestRegressor from version 0.22.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\u001b[0m\n",
      "\u001b[34mInside the predict function\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [01/Feb/2023:03:17:56 +0000] \"POST /invocations HTTP/1.1\" 200 78 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator DecisionTreeRegressor from version 0.22.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator RandomForestRegressor from version 0.22.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\u001b[0m\n",
      "\u001b[35mInside the predict function\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [01/Feb/2023:03:17:56 +0000] \"POST /invocations HTTP/1.1\" 200 78 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2023-02-01T03:17:56.305:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "Waiting for transform job: sagemaker-scikit-learn-2023-02-01-03-13-32-143\n",
      "\u001b[34m2023-02-01 03:17:50,438 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-02-01 03:17:50,440 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-02-01 03:17:50,441 INFO - sagemaker-containers - nginx config: \u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[35m2023-02-01 03:17:50,438 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2023-02-01 03:17:50,440 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2023-02-01 03:17:50,441 INFO - sagemaker-containers - nginx config: \u001b[0m\n",
      "\u001b[35mworker_processes auto;\u001b[0m\n",
      "\u001b[35mdaemon off;\u001b[0m\n",
      "\u001b[35mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[35merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[35mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[35mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m2023-02-01 03:17:50,625 INFO - sagemaker-containers - Module inference does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35m2023-02-01 03:17:50,625 INFO - sagemaker-containers - Module inference does not provide a setup.py. \u001b[0m\n",
      "\u001b[35mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2023-02-01 03:17:50,625 INFO - sagemaker-containers - Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2023-02-01 03:17:50,626 INFO - sagemaker-containers - Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2023-02-01 03:17:50,626 INFO - sagemaker-containers - Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python3 -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: inference\n",
      "  Building wheel for inference (setup.py): started\n",
      "  Building wheel for inference (setup.py): finished with status 'done'\n",
      "  Created wheel for inference: filename=inference-1.0.0-py2.py3-none-any.whl size=3900 sha256=72c929fe8a87c1cc509579153316d9b96efa4512c664047b95b64d36a2abb1c2\n",
      "  Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-0qto1eap/wheels/3e/0f/51/2f1df833dd0412c1bc2f5ee56baac195b5be563353d111dca6\u001b[0m\n",
      "\u001b[34mSuccessfully built inference\u001b[0m\n",
      "\u001b[34mInstalling collected packages: inference\u001b[0m\n",
      "\u001b[34mSuccessfully installed inference-1.0.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.3.1 -> 23.0\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[35m2023-02-01 03:17:50,625 INFO - sagemaker-containers - Generating setup.cfg\u001b[0m\n",
      "\u001b[35m2023-02-01 03:17:50,626 INFO - sagemaker-containers - Generating MANIFEST.in\u001b[0m\n",
      "\u001b[35m2023-02-01 03:17:50,626 INFO - sagemaker-containers - Installing module with the following command:\u001b[0m\n",
      "\u001b[35m/miniconda3/bin/python3 -m pip install . \u001b[0m\n",
      "\u001b[35mProcessing /opt/ml/code\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: inference\n",
      "  Building wheel for inference (setup.py): started\n",
      "  Building wheel for inference (setup.py): finished with status 'done'\n",
      "  Created wheel for inference: filename=inference-1.0.0-py2.py3-none-any.whl size=3900 sha256=72c929fe8a87c1cc509579153316d9b96efa4512c664047b95b64d36a2abb1c2\n",
      "  Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-0qto1eap/wheels/3e/0f/51/2f1df833dd0412c1bc2f5ee56baac195b5be563353d111dca6\u001b[0m\n",
      "\u001b[35mSuccessfully built inference\u001b[0m\n",
      "\u001b[35mInstalling collected packages: inference\u001b[0m\n",
      "\u001b[35mSuccessfully installed inference-1.0.0\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35m[notice] A new release of pip available: 22.3.1 -> 23.0\u001b[0m\n",
      "\u001b[35m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[2023-02-01 03:17:53 +0000] [38] [INFO] Starting gunicorn 20.0.4\u001b[0m\n",
      "\u001b[34m[2023-02-01 03:17:53 +0000] [38] [INFO] Listening at: unix:/tmp/gunicorn.sock (38)\u001b[0m\n",
      "\u001b[34m[2023-02-01 03:17:53 +0000] [38] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2023-02-01 03:17:53 +0000] [41] [INFO] Booting worker with pid: 41\u001b[0m\n",
      "\u001b[34m[2023-02-01 03:17:53 +0000] [42] [INFO] Booting worker with pid: 42\u001b[0m\n",
      "\u001b[34m[2023-02-01 03:17:53 +0000] [43] [INFO] Booting worker with pid: 43\u001b[0m\n",
      "\u001b[34m[2023-02-01 03:17:53 +0000] [47] [INFO] Booting worker with pid: 47\u001b[0m\n",
      "\u001b[35m[2023-02-01 03:17:53 +0000] [38] [INFO] Starting gunicorn 20.0.4\u001b[0m\n",
      "\u001b[35m[2023-02-01 03:17:53 +0000] [38] [INFO] Listening at: unix:/tmp/gunicorn.sock (38)\u001b[0m\n",
      "\u001b[35m[2023-02-01 03:17:53 +0000] [38] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[35m[2023-02-01 03:17:53 +0000] [41] [INFO] Booting worker with pid: 41\u001b[0m\n",
      "\u001b[35m[2023-02-01 03:17:53 +0000] [42] [INFO] Booting worker with pid: 42\u001b[0m\n",
      "\u001b[35m[2023-02-01 03:17:53 +0000] [43] [INFO] Booting worker with pid: 43\u001b[0m\n",
      "\u001b[35m[2023-02-01 03:17:53 +0000] [47] [INFO] Booting worker with pid: 47\u001b[0m\n",
      "\u001b[34m2023-02-01 03:17:55,774 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator DecisionTreeRegressor from version 0.22.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator RandomForestRegressor from version 0.22.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [01/Feb/2023:03:17:56 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [01/Feb/2023:03:17:56 +0000] \"GET /execution-parameters HTTP/1.1\" 404 232 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m2023-02-01 03:17:56,369 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2023-02-01 03:17:55,774 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator DecisionTreeRegressor from version 0.22.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator RandomForestRegressor from version 0.22.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [01/Feb/2023:03:17:56 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [01/Feb/2023:03:17:56 +0000] \"GET /execution-parameters HTTP/1.1\" 404 232 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m2023-02-01 03:17:56,369 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator DecisionTreeRegressor from version 0.22.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator RandomForestRegressor from version 0.22.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\u001b[0m\n",
      "\u001b[34mInside the predict function\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [01/Feb/2023:03:17:56 +0000] \"POST /invocations HTTP/1.1\" 200 78 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator DecisionTreeRegressor from version 0.22.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator RandomForestRegressor from version 0.22.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\u001b[0m\n",
      "\u001b[35mInside the predict function\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [01/Feb/2023:03:17:56 +0000] \"POST /invocations HTTP/1.1\" 200 78 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2023-02-01T03:17:56.305:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Feed the test data\n",
    "transformer.transform(test_data_path, content_type=\"text/csv\")\n",
    "print(\"Waiting for transform job: \" + transformer.latest_transform_job.job_name)\n",
    "transformer.wait()\n",
    "output = transformer.output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dbb624-70da-43bd-bc4f-c07200d59fab",
   "metadata": {},
   "source": [
    "## View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a29f577-a325-4b70-934e-df997b11302c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "client = boto3.client('sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a99d401-8a6c-4db7-b276-89706a79d46d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-474422712127/sagemaker-scikit-learn-2023-02-01-03-13-32-143'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_path = client.describe_transform_job(TransformJobName = \"sagemaker-scikit-learn-2023-02-01-03-13-32-143\")['TransformOutput']['S3OutputPath']\n",
    "output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ac4291a-f11c-4407-a7c2-dc2a812d7df7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-01 03:17:57         78 test.csv.out\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls 's3://sagemaker-us-east-1-474422712127/sagemaker-scikit-learn-2023-02-01-03-13-32-143/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3cf89ccf-442c-4baa-aab9-82df7eb23743",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-1-474422712127/sagemaker-scikit-learn-2023-02-01-03-13-32-143/test.csv.out to ./results.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://sagemaker-us-east-1-474422712127/sagemaker-scikit-learn-2023-02-01-03-13-32-143/test.csv.out results.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87e6f3bc-f474-4173-8c86-8bda34e7f0d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>605.25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>651.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>741.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>816.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>578.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>662.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>629.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>581.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>732.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>491.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>639.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>530.45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    605.25\n",
       "0   651.20\n",
       "1   741.30\n",
       "2   816.35\n",
       "3   578.00\n",
       "4   662.70\n",
       "5   629.25\n",
       "6   581.30\n",
       "7   732.20\n",
       "8   491.25\n",
       "9   639.65\n",
       "10  530.45"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "res = pd.read_csv(\"results.csv\")\n",
    "res"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.c5.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
