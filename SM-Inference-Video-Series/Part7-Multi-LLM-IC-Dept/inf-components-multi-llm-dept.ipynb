{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4190b62-7da3-4526-b576-7263197a4c75",
   "metadata": {},
   "source": [
    "# Utilizing Inference Components (ICs) to Host Multiple LLMs on SageMaker Real-Time Endpoints\n",
    "In this example we utilize [SageMaker Inference Components](https://aws.amazon.com/blogs/aws/amazon-sagemaker-adds-new-inference-capabilities-to-help-reduce-foundation-model-deployment-costs-and-latency/) to host variants of both a [Qwen](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct?sagemaker_deploy=true) and [OpenLlama](https://huggingface.co/openlm-research/open_llama_7b) model on a singular endpoint. \n",
    "\n",
    "Unlike traditional SageMaker Real-Time Endpoints we follow the flow of Endpoint Config -> Endpoint -> IC (1...n). In this case we create the endpoint and then add both a Qwen and OpenLlama Model as their own ICs. ICs are similar to SageMaker Model objects we can define the model data and container information, the difference is we can enable AutoScaling as well at the IC level, we will explore this in the next notebook. For now you can imagine the IC architecture as the following: \n",
    "\n",
    "![ic-arch](ic-arch.png)\n",
    "\n",
    "An IC inherits the SageMaker model construct and adds two parameters:\n",
    "1. <b>Hardware Resource Requirements</b>: This is what you reserve for this specific component from a hardware perspective, in this case we work with an 8 GPU instance and reserve 4 for the OpenLlama model and 1 for the Qwen model. Obviously this is not optimal usage, but the idea is to showcase how you can leverage an endpoint to host multiple LLMs, ideally you want to setup appropriate scaling at the component level as well, which we will explore in coming sections.\n",
    "2. <b>Copy Count</b>: This is the new CW metric to be aware from which we can scale up the ICs individually. Each copy retains the hardware resource requirements you have allocated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aeac15c-4fea-4c7b-8e2b-a0d6290baea1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1d8192-2a59-4810-aed2-2ea30ff94a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker boto3 --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748bae5b-866c-42a7-8eb9-9e80045be714",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "\n",
    "#Setup\n",
    "client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime = boto3.client(service_name=\"sagemaker-runtime\")\n",
    "boto_session = boto3.session.Session()\n",
    "s3 = boto_session.resource('s3')\n",
    "region = boto_session.region_name\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "print(f\"Role ARN: {role}\")\n",
    "print(f\"Region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f62ee2e-6e32-4905-9f0d-50837fd7849e",
   "metadata": {},
   "source": [
    "## SM Endpoint Config and Endpoint Creation\n",
    "We first create the endpoint configuration and endpoint and then allocate the resources to each inference component afterwards. Note you might need a limit increase request for ml.g5.48xlarge for the instance type behind the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5140be-4f67-4058-a9fe-9fcf968724aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# endpoint config name\n",
    "epc_name = \"ic-epc\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(f\"Endpoint Config Name: {epc_name}\")\n",
    "\n",
    "# Container Parameters, increase health check for LLMs: \n",
    "variant_name = \"AllTraffic\"\n",
    "instance_type = \"ml.g5.48xlarge\"\n",
    "model_data_download_timeout_in_seconds = 3600\n",
    "container_startup_health_check_timeout_in_seconds = 3600\n",
    "\n",
    "# Setting up managed AutoScaling\n",
    "initial_instance_count = 1\n",
    "max_instance_count = 2\n",
    "print(f\"Initial instance count: {initial_instance_count}\")\n",
    "print(f\"Max instance count: {max_instance_count}\")\n",
    "\n",
    "# Endpoint Config Creation\n",
    "endpoint_config_response = client.create_endpoint_config(\n",
    "    EndpointConfigName=epc_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": variant_name,\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": model_data_download_timeout_in_seconds,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": container_startup_health_check_timeout_in_seconds,\n",
    "            \"ManagedInstanceScaling\": {\n",
    "                \"Status\": \"ENABLED\",\n",
    "                \"MinInstanceCount\": initial_instance_count,\n",
    "                \"MaxInstanceCount\": max_instance_count,\n",
    "            },\n",
    "            # can set to least outstanding or random: https://aws.amazon.com/blogs/machine-learning/minimize-real-time-inference-latency-by-using-amazon-sagemaker-routing-strategies/\n",
    "            \"RoutingConfig\": {\"RoutingStrategy\": \"LEAST_OUTSTANDING_REQUESTS\"},\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Configuration Arn: \" + endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0e4a04-29de-4d3a-b0c8-fab3d864e57d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Endpoint Creation\n",
    "endpoint_name = \"ic-ep\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "create_endpoint_response = client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=epc_name,\n",
    ")\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa79f393-0f83-45e8-8cf5-4bf058887a9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Monitor creation\n",
    "describe_endpoint_response = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "while describe_endpoint_response[\"EndpointStatus\"] == \"Creating\":\n",
    "    describe_endpoint_response = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    print(describe_endpoint_response[\"EndpointStatus\"])\n",
    "    time.sleep(60)\n",
    "print(describe_endpoint_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee352ae-9e5e-499b-870f-3fd15d978324",
   "metadata": {},
   "source": [
    "## Inference Component Creation\n",
    "For an IC you first create a SM Model object which you inherit from and then allocate the compute resource requirements and copy count in the <b>create_ic</b> API call: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker/client/create_inference_component.html."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5af319-b41e-417c-8fcd-3a0bebfe06a9",
   "metadata": {},
   "source": [
    "### Open Llama Inf Component Creation\n",
    "For Open Llama we will deploy using the LMI container with an Nvidia TensorRT Backend, reference this video for background around LMI: https://www.youtube.com/watch?v=Q-Kz5Yi0QiQ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f0b956-43ef-4191-b0d9-c2154179864a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First create the model object\n",
    "openllama_env = {\n",
    "    \"SERVING_LOAD_MODELS\": \"test::MPI=/opt/ml/model\",\n",
    "    \"OPTION_TENSOR_PARALLEL_DEGREE\": \"max\",\n",
    "    \"OPTION_MODEL_ID\": \"openlm-research/open_llama_7b\",\n",
    "    \"OPTION_ROLLING_BATCH\": \"trtllm\",\n",
    "    \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"16\"\n",
    "}\n",
    "# TRT Image URI for the OpenLlama container\n",
    "openllama_lmi_image_uri = \"763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.30.0-tensorrtllm0.12.0-cu125\"\n",
    "# create SM model object for OpenLlama\n",
    "ollama_model_name = sagemaker.utils.name_from_base(\"lmi-openllama-7b\")\n",
    "print(ollama_model_name)\n",
    "\n",
    "# model object for ollama LMI deployment\n",
    "create_model_response = client.create_model(\n",
    "    ModelName=ollama_model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\n",
    "        \"Image\": openllama_lmi_image_uri,\n",
    "        \"Environment\": openllama_env,\n",
    "    }\n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "print(f\"Created Model: {model_arn}\")\n",
    "\n",
    "# Inf Component Creation for Ollama\n",
    "ollama_ic_name = \"ollama-ic\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "variant_name = \"AllTraffic\"\n",
    "\n",
    "# inference component reaction\n",
    "create_ollama_ic_response = client.create_inference_component(\n",
    "    InferenceComponentName=ollama_ic_name,\n",
    "    EndpointName=endpoint_name,\n",
    "    VariantName=variant_name,\n",
    "    Specification={\n",
    "        \"ModelName\": ollama_model_name,\n",
    "        \"ComputeResourceRequirements\": {\n",
    "            # enables tensor parallel\n",
    "            \"NumberOfAcceleratorDevicesRequired\": 4,\n",
    "            \"NumberOfCpuCoresRequired\": 1,\n",
    "            \"MinMemoryRequiredInMb\": 1024,\n",
    "        },\n",
    "    },\n",
    "    # can setup autoscaling for copies\n",
    "    RuntimeConfig={\"CopyCount\": 1},\n",
    ")\n",
    "\n",
    "print(\"IC OpenLlama Arn: \" + create_ollama_ic_response[\"InferenceComponentArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267921be-853c-4c7c-b6a9-7c8c7e03ccee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "describe_ic_ollama_response = client.describe_inference_component(\n",
    "    InferenceComponentName=ollama_ic_name)\n",
    "\n",
    "while describe_ic_ollama_response[\"InferenceComponentStatus\"] == \"Creating\":\n",
    "    describe_ic_ollama_response = client.describe_inference_component(InferenceComponentName=ollama_ic_name)\n",
    "    print(describe_ic_ollama_response[\"InferenceComponentStatus\"])\n",
    "    time.sleep(30)\n",
    "print(describe_ic_ollama_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda07a4c-6be9-49c1-b263-689773060810",
   "metadata": {},
   "source": [
    "### Qwen Inference Component Creation\n",
    "In this IC for Qwen we use the TGI container which simplifies deployment from HF, also want to showcase how you can use different containers and serving stacks for models whereas with MME it's a single container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed835a13-df9b-4e59-bbfe-90df4a8c895f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "qwen_tgi_image_uri= get_huggingface_llm_image_uri(\"huggingface\",version=\"3.0.1\")\n",
    "qwen_model = {\"Image\": qwen_tgi_image_uri, \"Environment\": {'HF_MODEL_ID':'Qwen/Qwen2.5-7B-Instruct'}}\n",
    "qwen_model_name = \"qwen-model\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(f\"Qwen Model Name: {qwen_model_name}\")\n",
    "\n",
    "# create qwen model object\n",
    "create_qwen_model_response = client.create_model(\n",
    "    ModelName=qwen_model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    Containers=[qwen_model],\n",
    ")\n",
    "print(\"Qwen Model Arn: \" + create_qwen_model_response[\"ModelArn\"])\n",
    "\n",
    "qwen_ic_name = \"qwen-ic\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "variant_name = \"AllTraffic\"\n",
    "\n",
    "# qwen inference component reaction\n",
    "create_qwen_ic_response = client.create_inference_component(\n",
    "    InferenceComponentName=qwen_ic_name,\n",
    "    EndpointName=endpoint_name,\n",
    "    VariantName=variant_name,\n",
    "    Specification={\n",
    "        \"ModelName\": qwen_model_name,\n",
    "        \"ComputeResourceRequirements\": {\n",
    "            \"NumberOfAcceleratorDevicesRequired\": 1,\n",
    "            \"NumberOfCpuCoresRequired\": 1,\n",
    "            \"MinMemoryRequiredInMb\": 1024,\n",
    "        },\n",
    "    },\n",
    "    # can setup autoscaling for copies\n",
    "    RuntimeConfig={\"CopyCount\": 1},\n",
    ")\n",
    "\n",
    "print(\"IC Qwen Arn: \" + create_qwen_ic_response[\"InferenceComponentArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb0c76c-8974-4d85-b859-16d24ffe321d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "describe_ic_qwen_response = client.describe_inference_component(\n",
    "    InferenceComponentName=qwen_ic_name)\n",
    "\n",
    "while describe_ic_qwen_response[\"InferenceComponentStatus\"] == \"Creating\":\n",
    "    describe_ic_qwen_response = client.describe_inference_component(InferenceComponentName=qwen_ic_name)\n",
    "    print(describe_ic_qwen_response[\"InferenceComponentStatus\"])\n",
    "    time.sleep(60)\n",
    "print(describe_ic_qwen_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2315e4-307c-48f5-8c09-090d79bbccc7",
   "metadata": {},
   "source": [
    "## Sample Inference\n",
    "Here we use the same [invoke_endpoint](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker-runtime/client/invoke_endpoint.html) API call, but just have a header to specify the target IC name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b708cc-6e7f-4f39-a1ab-f9bbdd5ece8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# OpenLlama\n",
    "import json\n",
    "payload = \"What is the capitol of the United States?\"\n",
    "response = runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    InferenceComponentName=ollama_ic_name, #specify IC name\n",
    "    ContentType=\"application/json\",\n",
    "    Accept=\"application/json\",\n",
    "    Body=json.dumps(\n",
    "        {\n",
    "            \"inputs\": payload,\n",
    "            \"parameters\": {\n",
    "                \"max_new_tokens\": 200  # Adjust this value as needed\n",
    "                },\n",
    "        }\n",
    "    ),\n",
    ")\n",
    "result = json.loads(response[\"Body\"].read().decode())['generated_text']\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748dbc2a-543f-42cc-a7cf-2780ca350bdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Qwen\n",
    "response = runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    InferenceComponentName=qwen_ic_name, #specify IC name\n",
    "    ContentType=\"application/json\",\n",
    "    Accept=\"application/json\",\n",
    "    Body=json.dumps(\n",
    "        {\n",
    "            \"inputs\": payload,\n",
    "            \"parameters\": {\n",
    "                \"max_new_tokens\": 200  # Adjust this value as needed\n",
    "                },\n",
    "        }\n",
    "    ),\n",
    ")\n",
    "result = json.loads(response[\"Body\"].read().decode())[0]['generated_text']\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55673d53-a19b-4ad8-8473-3f042c3d0e07",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "Ensure to delete both the ICs and Endpoint resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc66fa35-f8f3-4e63-aed3-b8e576eefd62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client.delete_inference_component(InferenceComponentName=qwen_ic_name)\n",
    "client.delete_inference_component(InferenceComponentName=ollama_ic_name)\n",
    "client.delete_endpoint(EndpointName = endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
