{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71a329f0",
   "metadata": {},
   "source": [
    "# Using SageMaker Multi-Adapter Serving to host LoRA adapters at Scale\n",
    "In this example we explore how SageMaker Multi-Adapter Inference works with the [Gemma-2b model](https://huggingface.co/google/gemma-2-2b) and an [OSS adapter](https://huggingface.co/Kronu/gemma-2-2b-lean-expert-1760-complete) from HuggingFace. \n",
    "\n",
    "We'll build on this and explore more practical applications where we can actually see the value of LORA adapters from an evaluation/Data Science perspective. For this notebook we want to just understand the constructs of Multi-Adapter Inference and how you can set it up, if you have your own adapters and base model already try to plug them into these constructs with appropriate config to take it for a spin yourself!\n",
    "\n",
    "## Prerequisites\n",
    "- You also need a HuggingFace token to follow this sample: https://huggingface.co/docs/hub/en/security-tokens\n",
    "- If new to Inf Components/SM Inference follow this guide: https://www.youtube.com/watch?v=RcUNEeUqpNQ&t=11s\n",
    "\n",
    "## Additional Resources/Credits\n",
    "- vLLM LMI Engine Params/Docs: https://docs.djl.ai/v0.29.0/docs/serving/serving/docs/lmi/user_guides/vllm_user_guide.html\n",
    "- Multi-Adapter Inference Docs: https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-adapt.html\n",
    "\n",
    "## License\n",
    "- Model: Gemma-2 2B\n",
    "- License: Gemma License\n",
    "- Used under the terms of the Gemma License for research and deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8974eee7-cc31-4dac-8795-aec6b8765051",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fa3208",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install boto3 huggingface_hub sagemaker --upgrade --quiet --no-warn-conflicts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a27613-3975-4ddd-80ee-d22990647978",
   "metadata": {},
   "source": [
    "### Configure development environment and boto3 clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94eba2f6-e1b6-41c6-94d1-2b2bfbe3308b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import sagemaker\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "import tarfile\n",
    "import pathlib\n",
    "from huggingface_hub import snapshot_download\n",
    "import os\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "boto_session = boto3.session.Session()\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\")  # client to intreract with SageMaker\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")  # client to intreract with SageMaker Endpoints\n",
    "s3_client = boto3.client(\"s3\")\n",
    "s3 = boto_session.resource('s3')\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n",
    "print(f\"boto3 version: {boto3.__version__}\")\n",
    "print(f\"sagemaker version: {sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b359e35-e3f9-4619-925a-43b15f838c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"AWS_REGION\"] = \"us-east-1\"     # or your region\n",
    "os.environ[\"HF_TOKEN\"] = \"Enter HF token\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b1bc96-d68e-4fca-bf72-8bf9b50e52a7",
   "metadata": {},
   "source": [
    "## Container & Model Aritfacts Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358c2448-03f4-41b5-b72c-649820ac7cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTAINER_VERSION = \"0.34.0-lmi16.0.0-cu128\"\n",
    "inference_image = f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:{CONTAINER_VERSION}\"\n",
    "print(f\"Using image URI: {inference_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8bf3e3-e6d0-4150-8980-13eef86b0e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.g5.4xlarge\"\n",
    "num_gpu = 1\n",
    "\n",
    "model_name = \"gemma-2b\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "#utilize the vLLM async handler: \n",
    "env = {\n",
    "    \"HF_MODEL_ID\": \"google/gemma-2-2b\",\n",
    "    \"HF_TOKEN\": os.getenv(\"HF_TOKEN\"),\n",
    "    \"SERVING_FAIL_FAST\": \"true\",\n",
    "    \"OPTION_ASYNC_MODE\": \"true\",\n",
    "    \"OPTION_ROLLING_BATCH\": \"disable\",\n",
    "    \"OPTION_TENSOR_PARALLEL_DEGREE\": json.dumps(num_gpu),\n",
    "    \"OPTION_ENTRYPOINT\": \"djl_python.lmi_vllm.vllm_async_service\",\n",
    "    \"OPTION_TRUST_REMOTE_CODE\": \"true\",\n",
    "    \"OPTION_ENABLE_LORA\": \"true\",\n",
    "    \"OPTION_MAX_LORAS\": \"1\",\n",
    "    \"OPTION_MAX_CPU_LORAS\": \"2\",\n",
    "    \"OPTION_MAX_LORA_RANK\": \"16\",\n",
    "    \"OPTION_DTYPE\": \"bf16\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67604d66-ceff-4b87-93fb-693fbbb67cb2",
   "metadata": {},
   "source": [
    "### Model & Base IC Creation\n",
    "We first create an Inference Component to represent the base model, we will then associate the adapter component with this base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4dbedd-abc7-4b91-b00b-eac7d8ec2759",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response = sm_client.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = {\n",
    "        \"Image\": inference_image,\n",
    "        \"Environment\": env\n",
    "    },\n",
    ")\n",
    "print(json.dumps(model_response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc813b9-7b44-4ae2-ae08-9e52132a77d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deployment params\n",
    "model_data_download_timeout_in_seconds = 900\n",
    "container_startup_health_check_timeout_in_seconds = 900\n",
    "initial_instance_count = 1\n",
    "variant_name = \"main\"\n",
    "\n",
    "endpoint_config_name = \"gemma-epc\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    ProductionVariants = [\n",
    "        {\n",
    "            \"VariantName\": variant_name,\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialInstanceCount\": initial_instance_count,\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": model_data_download_timeout_in_seconds,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": container_startup_health_check_timeout_in_seconds,\n",
    "            \"RoutingConfig\": {\"RoutingStrategy\": \"LEAST_OUTSTANDING_REQUESTS\"},\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(json.dumps(endpoint_config_response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba882407-a880-4a96-a9b1-acc52a14c01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = \"gemma-ep\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName = endpoint_name, \n",
    "    EndpointConfigName = endpoint_config_name\n",
    ")\n",
    "print(json.dumps(endpoint_response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc340578-b385-4014-a0f9-cfc78811060b",
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_endpoint_response = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "while describe_endpoint_response[\"EndpointStatus\"] == \"Creating\":\n",
    "    describe_endpoint_response = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    print(describe_endpoint_response[\"EndpointStatus\"])\n",
    "    time.sleep(60)\n",
    "print(describe_endpoint_response)\n",
    "print(f\"Created endpoint: {endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5b246f-1ddd-4662-8add-3215d927beb7",
   "metadata": {},
   "source": [
    "### Base IC Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0b8a04-dbd4-4678-88b4-3e5c722a615d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "base_inference_component_name = f\"base-{model_name}\"\n",
    "print(f\"Base inference component name: {base_inference_component_name}\")\n",
    "\n",
    "# component level params\n",
    "initial_copy_count = 1\n",
    "min_memory_required_in_mb = 32000\n",
    "number_of_accelerator_devices_required = 1\n",
    "\n",
    "# create base IC\n",
    "base_create_inference_component_response = sm_client.create_inference_component(\n",
    "    InferenceComponentName = base_inference_component_name,\n",
    "    EndpointName = endpoint_name,\n",
    "    VariantName = variant_name,\n",
    "    Specification={\n",
    "        \"ModelName\": model_name,\n",
    "        \"StartupParameters\": {\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": model_data_download_timeout_in_seconds,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": container_startup_health_check_timeout_in_seconds,\n",
    "        },\n",
    "        \"ComputeResourceRequirements\": {\n",
    "            \"MinMemoryRequiredInMb\": min_memory_required_in_mb,\n",
    "            \"NumberOfAcceleratorDevicesRequired\": number_of_accelerator_devices_required,\n",
    "        },\n",
    "    },\n",
    "    RuntimeConfig={\n",
    "        \"CopyCount\": initial_copy_count,\n",
    "    },\n",
    ")\n",
    "\n",
    "sess.wait_for_inference_component(base_inference_component_name)\n",
    "print(f\"Created Base inference component ARN: {base_create_inference_component_response['InferenceComponentArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5868cd46-e60d-4d5c-87b6-e3c3b9eb8b93",
   "metadata": {},
   "source": [
    "### Invoke the Base Model Inference Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393f9090-f267-4f43-959b-9a8e9aad6dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "payload = \"Who is Rafael Nadal?\"\n",
    "response = smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    InferenceComponentName=base_inference_component_name, #specify IC name\n",
    "    ContentType=\"application/json\",\n",
    "    Accept=\"application/json\",\n",
    "    Body=json.dumps(\n",
    "        {\n",
    "            \"inputs\": payload,\n",
    "            \"parameters\": {\n",
    "                \"max_new_tokens\": 200  # Adjust this value as needed\n",
    "                },\n",
    "        }\n",
    "    ),\n",
    ")\n",
    "result = json.loads(response[\"Body\"].read().decode())['generated_text']\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b74904-2038-42ea-96aa-5e89d658d7bf",
   "metadata": {},
   "source": [
    "## Create Adapter IC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a241a6a6-4d69-471e-b291-db941c68889a",
   "metadata": {},
   "source": [
    "We have a few utility functions to upload the model data for the base adapter to an S3 bucket. This is expected in a model.tar.gz format, you can upload your own artifacts here or pull from HF if working with another OSS adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe5291e-6a4f-4f09-af8a-6720b068f0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_bucket(bucket_name: str, region: str = \"us-east-1\"):\n",
    "    \"\"\"Check if an S3 bucket exists; if not, create it.\"\"\"\n",
    "    s3 = boto3.client(\"s3\", region_name=region)\n",
    "    try:\n",
    "        s3.head_bucket(Bucket=bucket_name)\n",
    "        print(f\"✅ Bucket exists: s3://{bucket_name}\")\n",
    "    except ClientError as e:\n",
    "        code = e.response[\"Error\"][\"Code\"]\n",
    "        if code in (\"404\", \"NoSuchBucket\", \"NotFound\"):\n",
    "            print(f\"🪣 Creating bucket: {bucket_name}\")\n",
    "            params = {\"Bucket\": bucket_name}\n",
    "            if region != \"us-east-1\":\n",
    "                params[\"CreateBucketConfiguration\"] = {\"LocationConstraint\": region}\n",
    "            s3.create_bucket(**params)\n",
    "            print(f\"✅ Created new bucket: s3://{bucket_name}\")\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "\n",
    "def pull_from_hf(model_id: str, out_dir: str = \"./_download\") -> str:\n",
    "    \"\"\"\n",
    "    Download model/artifacts from Hugging Face and bundle into model.tar.gz.\n",
    "    Tarball contains only the artifact files at the root (SageMaker-compliant).\n",
    "    \"\"\"\n",
    "    pathlib.Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"📥 Downloading {model_id} from Hugging Face...\")\n",
    "    local_dir = snapshot_download(\n",
    "        repo_id=model_id,\n",
    "        local_dir=out_dir,\n",
    "        local_dir_use_symlinks=False,\n",
    "        token=os.getenv(\"HF_TOKEN\"),\n",
    "    )\n",
    "\n",
    "    tar_path = os.path.join(out_dir, \"model.tar.gz\")\n",
    "    with tarfile.open(tar_path, \"w:gz\") as tar:\n",
    "        # Add *contents* of the folder, not the folder itself\n",
    "        for item in pathlib.Path(local_dir).iterdir():\n",
    "            tar.add(item, arcname=item.name)\n",
    "\n",
    "    print(f\"✅ Created SageMaker-compliant tarball: {tar_path}\")\n",
    "    return tar_path\n",
    "\n",
    "\n",
    "def push_s3(bucket_name: str, file_path: str):\n",
    "    \"\"\"Upload model.tar.gz to the specified S3 bucket (auto-creates bucket).\"\"\"\n",
    "    ensure_bucket(bucket_name)\n",
    "    s3 = boto3.resource(\"s3\", region_name=\"us-east-1\")\n",
    "    key = os.path.basename(file_path)\n",
    "    print(f\"🚀 Uploading {file_path} → s3://{bucket_name}/{key}\")\n",
    "    s3.meta.client.upload_file(file_path, bucket_name, key)\n",
    "    print(f\"✅ Uploaded successfully: s3://{bucket_name}/{key}\")\n",
    "    return f\"s3://{bucket_name}/{key}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7077361f-41a4-4367-ab25-366e2f92f403",
   "metadata": {},
   "source": [
    "### Upload Adapter Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71629e0-4326-4073-868a-0d1a606f6140",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Kronu/gemma-2-2b-lean-expert-1760-complete\"\n",
    "bucket_name = \"gemma2-adapter-artifacts-rv\"\n",
    "\n",
    "# pull adapter from HF\n",
    "tarball = pull_from_hf(model_id)\n",
    "\n",
    "# push tarball to S3\n",
    "adapter_artifacts = push_s3(bucket_name, tarball)\n",
    "print(f\"Adapter artifacts location: {adapter_artifacts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683c18f2-4f08-4180-a2f6-ce8ef75c42f4",
   "metadata": {},
   "source": [
    "### Create & Invoke Adapter IC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f93c4f-a206-4da4-9588-33ff446c9bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_ic_name = f\"ic-adapter-{base_inference_component_name}\"\n",
    "\n",
    "sm_client.create_inference_component(\n",
    "    InferenceComponentName = adapter_ic_name,\n",
    "    EndpointName = endpoint_name,\n",
    "    # associate with the base IC we created\n",
    "    Specification={\n",
    "        \"BaseInferenceComponentName\": base_inference_component_name,\n",
    "        \"Container\": {\n",
    "            \"ArtifactUrl\": adapter_artifacts\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "sess.wait_for_inference_component(adapter_ic_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceb18a7-ba1c-4fe1-ba9b-b051f17b3790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "payload = \"Who is Rafael Nadal?\"\n",
    "response = smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    InferenceComponentName=adapter_ic_name, #specify adapterIC name\n",
    "    ContentType=\"application/json\",\n",
    "    Accept=\"application/json\",\n",
    "    Body=json.dumps(\n",
    "        {\n",
    "            \"inputs\": payload,\n",
    "            \"parameters\": {\n",
    "                \"max_new_tokens\": 200  # Adjust this value as needed\n",
    "                },\n",
    "        }\n",
    "    ),\n",
    ")\n",
    "result = json.loads(response[\"Body\"].read().decode())['generated_text']\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62238ad-8fa4-4df2-924d-d923f37f7ad5",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc65ab56-3290-4dde-84bb-4c8efcaf87d9",
   "metadata": {},
   "source": [
    "Deleting the base model IC will automatically delete the base IC and any associated adapter ICs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473d8b60-3aea-493b-b257-874a5e4c05b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_inference_component(adapter_ic_name, wait = True)\n",
    "\n",
    "print(f'Adapter Component {adapter_ic_name} deleted.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d98a123-105d-4c94-b658-67423c56e705",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_inference_component(base_inference_component_name, wait = True)\n",
    "\n",
    "print(f'Base Component {base_inference_component_name} deleted.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c2a06b-7f59-4a12-be34-467e313d2ab0",
   "metadata": {},
   "source": [
    "Clean up the running endpoint and its configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d674b41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess.delete_endpoint(endpoint_name)\n",
    "print(f'Endpoint {endpoint_name} deleted.')\n",
    "\n",
    "sess.delete_endpoint_config(endpoint_name)\n",
    "print(f'Endpoint Configuration {endpoint_name} deleted.')\n",
    "\n",
    "sess.delete_model(model_name)\n",
    "print(f'Model {model_name} deleted.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
