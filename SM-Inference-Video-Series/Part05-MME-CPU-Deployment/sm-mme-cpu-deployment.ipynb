{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa24d09a-e309-4240-99f8-1d9dedd5b76b",
   "metadata": {},
   "source": [
    "# Sample SKLearn Multi-Model Endpoint Deployment \n",
    "In this example we take a look at using a sample SKLearn model that we've been working with in this series and make a few hundred copies of this model artifact to simulate a Multi-Model Deployment on SageMaker Real-Time Inference. In this guide we'll go through taking these 300 models and deploying them on a SageMaker Endpoint using the Multi-Model Endpoints feature.\n",
    "\n",
    "Note MME with GPU based instance deployment is a little different and will be covered in the coming parts of this series, if you would like to get started early refer to this blog: https://medium.com/towards-data-science/host-hundreds-of-nlp-models-utilizing-sagemaker-multi-model-endpoints-backed-by-gpu-instances-1ec215886248?sk=def3b784378ab48190f37e6d1c2f3d00.\n",
    "\n",
    "## Setup & Environment\n",
    "We will be working in a ml.c5.4xlarge SageMaker Classic Notebook Instance using a conda_python3 kernel. You can also optionally use Studio or a smaller notebook instance.\n",
    "\n",
    "## Additional Resources\n",
    "- [Load Testing DJL MME AWS Blog](https://aws.amazon.com/blogs/machine-learning/run-ml-inference-on-unplanned-and-spiky-traffic-using-amazon-sagemaker-multi-model-endpoints/)\n",
    "- [Inference Playlist Series](https://www.youtube.com/watch?v=pVVKqiMiArc&list=PLThJtS7RDkOeo9mpNjFVnIGDyiazAm9Uk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dd53c7-57eb-4e5f-a51e-1336afc443dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sagemaker boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fa11f5-abe3-4e16-911a-af621f2e02f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import image_uris\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import joblib\n",
    "import pickle\n",
    "import tarfile\n",
    "import sagemaker\n",
    "from sagemaker.estimator import Estimator\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "import subprocess\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "region = sess._region_name\n",
    "account_id = sess.account_id()\n",
    "s3_model_prefix = \"djl-mme-sklearn-regression\" \n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3076d9e5-1881-4aa4-9077-ff409810a52f",
   "metadata": {},
   "source": [
    "## Tarball Creation & Multi-Model Copy Creation\n",
    "We take the artifacts that we have and wrap them into the model tarball and specifically for our MME dummy case here make 300 copies of the model in the same S3 bucket. These models all need to rest in the same S3 location for MME to work properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d0469a-03cf-40b2-a85d-73da62364def",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "python3 local_model.py\n",
    "tar -cvpzf model.tar.gz model.joblib requirements.txt model.py serving.properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73549ea1-4036-4feb-8db2-6cb1ba85465a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# we make a 300 copies of the tarball as a dummy, you can replace this with your actual model.joblibs in tarball\n",
    "for i in range(300):\n",
    "    with open(\"model.tar.gz\", \"rb\") as f:\n",
    "        s3_client.upload_fileobj(f, bucket, \"{}/sklearn-{}.tar.gz\".format(s3_model_prefix,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3c01fc-0f48-44bd-82ea-cdcf177d8b36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mme_artifacts = \"s3://{}/{}/\".format(bucket, s3_model_prefix)\n",
    "mme_artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eff2bbc-f550-4754-8d4b-1f533f422219",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#verify all 300 tar balls are present\n",
    "!aws s3 ls {mme_artifacts}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b771f0e-740b-4ef8-be86-2fe6bdb2d7c8",
   "metadata": {},
   "source": [
    "## SageMaker Inference Objects Creation\n",
    "Here we create the same inference constructs we always would, but also specify Multi-Model specifically for the inference mode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52857187-960c-4ce7-8813-604a0fddc566",
   "metadata": {},
   "source": [
    "### Model Creation\n",
    "In this case we use DJL Serving the same model server/container we used for Single Model Endpoints. Once again you can choose a container/server that you are comfortable with, in this case we use DJL as there's per model worker scaling enabled as well which allows for us to handle different traffic patterns more robustly at the serving level itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5593559d-7406-4abf-ad2a-c16126b47cf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# replace this with your ECR image URI based off of your region, we are utilizing the CPU image here\n",
    "inference_image_uri = '763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.29.0-cpu-full'\n",
    "\n",
    "#Step 1: Model Creation\n",
    "mme_model_name = \"sklearn-djl-mme\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(\"Model name: \" + mme_model_name)\n",
    "\n",
    "# here we specify the mode as Multi-Model and the S3 path with all our artifacts\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=mme_model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\"Image\": inference_image_uri, \"Mode\": \"MultiModel\", \"ModelDataUrl\": mme_artifacts},\n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Created Model: {model_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540467b7-feb8-4832-9dc0-31ddf0312998",
   "metadata": {},
   "source": [
    "### EPC Creation\n",
    "Here ensure you have the capacity/instance limit needed, worst case put a limit increase request. For deciding capacity behind an MME endpoint, you want to think of the number of models, average model size, and the memory behind the instance type you choose. MME loads these models into the instance's memory when invoked so you want to think about your traffic patterns as well in regards to how many models might be loaded in memory at the same time and if there will be enougn available memory for what you might be experiencing at peak traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc909a9-eefa-43c9-b95a-f1b87911f6f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Step 2: EPC Creation\n",
    "mme_epc_name = \"sklearn-djl-mme-epc\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=mme_epc_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"sklearnvariant\",\n",
    "            \"ModelName\": mme_model_name,\n",
    "            \"InstanceType\": \"ml.c5d.4xlarge\",\n",
    "            \"InitialInstanceCount\": 2\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(\"Endpoint Configuration Arn: \" + endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cafe2a-b044-43b0-8ed2-e3a53f5f6621",
   "metadata": {},
   "source": [
    "### Endpoint Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb35553f-ac32-42fc-ade1-c9ca2b0cd49f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Step 3: EP Creation\n",
    "mme_endpoint_name = \"sklearn-djl-ep-mme\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=mme_endpoint_name,\n",
    "    EndpointConfigName=mme_epc_name,\n",
    ")\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ac2dfe-4814-4e17-8aa1-34f42cc9a5ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Monitor creation\n",
    "describe_endpoint_response = sm_client.describe_endpoint(EndpointName=mme_endpoint_name)\n",
    "while describe_endpoint_response[\"EndpointStatus\"] == \"Creating\":\n",
    "    describe_endpoint_response = sm_client.describe_endpoint(EndpointName=mme_endpoint_name)\n",
    "    print(describe_endpoint_response[\"EndpointStatus\"])\n",
    "    time.sleep(15)\n",
    "print(describe_endpoint_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14485e91-1264-4521-b727-45b6e964f769",
   "metadata": {},
   "source": [
    "## Sample Inference\n",
    "Here we have the same invoke_endpoint API call to interact with the endpoint: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker-runtime/client/invoke_endpoint.html. The key difference from single model endpoints is we specify a TargetModel parameter which is the \"model-1.tar.gz\" filename you have uploaded to S3 (this can obviously change depending on what you named your files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffcccf1-869f-4d42-9e1f-b55e0a1a9ceb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "content_type = \"application/json\"\n",
    "request_body = '[[0.5]]' #replace with your request body\n",
    "\n",
    "\n",
    "# sample inference, the target model string should look like sklearn-modelversion.tar.gz\n",
    "# initial request might take a little longer as the model is loaded into memory\n",
    "response = smr_client.invoke_endpoint(\n",
    "    EndpointName=mme_endpoint_name,\n",
    "    ContentType=content_type,\n",
    "    TargetModel = \"sklearn-290.tar.gz\",\n",
    "    Body=request_body)\n",
    "result = json.loads(response['Body'].read().decode())\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a82eb2-e95a-4838-bfe6-fc95dc353d02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sample inference across many models, might take a little to run this cell\n",
    "import random\n",
    "\n",
    "for i in range(500):\n",
    "    random_model = random.randint(1,300) #randomly pick from our 300 models we have behind endpoint\n",
    "    target_model = f\"sklearn-{random_model}.tar.gz\"\n",
    "    print(f\"Invoking following model: {target_model}\")\n",
    "    response = smr_client.invoke_endpoint(\n",
    "        EndpointName=mme_endpoint_name,\n",
    "        ContentType=content_type,\n",
    "        TargetModel = target_model,\n",
    "        Body=request_body)\n",
    "    result = json.loads(response['Body'].read().decode())\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6960ecc-aa75-409d-a8fa-b0d793205b77",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "Ensure to delete your endpoint to not incur any further costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca71d19-b0b0-451a-9ff3-289b08a9df30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint(EndpointName = mme_endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
