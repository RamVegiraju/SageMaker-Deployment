{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5bd7035-81b8-4bcb-adf6-2b71f6faa665",
   "metadata": {},
   "source": [
    "# SageMaker Batch Transform: Offline Inference\n",
    "In the case that you don't want to use an endpoint driven option and just want offline inference on a dataset you can use [SageMaker Batch Transform](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html). In this example we'll take a sample [Distillbert model](https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english) (Apache 2.0 license) and showcase how you can create a Transform Job.\n",
    "\n",
    "### Additional Resources/Credits\n",
    "- [HuggingFace Workshop](https://github.com/philschmid/huggingface-sagemaker-workshop-series/blob/main/workshop_2_going_production/lab2_batch_transform.ipynb)\n",
    "- [HF Custom Inference Script Implementation](https://github.com/huggingface/notebooks/blob/main/sagemaker/17_custom_inference_script/code/inference.py)\n",
    "- [Batch Transform Examples](https://github.com/RamVegiraju/SageMaker-Deployment/tree/master/BatchTransform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882ff349-87f1-4375-9e1b-a5a917c4d31f",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be06891b-ce95-4ac2-ab54-7bbab930b334",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet sagemaker jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd9455b-2dae-4388-bc69-d1a9470b23af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "import sagemaker\n",
    "\n",
    "# setup role and sm session and default bucket\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "\n",
    "# HuggingFace Model Object\n",
    "hub = {\n",
    "\t'HF_MODEL_ID':'distilbert/distilbert-base-uncased-finetuned-sst-2-english',\n",
    "\t'HF_TASK':'text-classification'\n",
    "}\n",
    "\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    transformers_version='4.37.0',\n",
    "    pytorch_version='2.1.0',\n",
    "    py_version='py310',\n",
    "    env=hub,\n",
    "    role=role\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd2bfe8-ff1b-42d7-afc1-57eea625196f",
   "metadata": {},
   "source": [
    "## Create Test Dataset\n",
    "We make a mock dataset with 50 copies of the same input payload, adjust this accordingly for your use-case. In this case we have a JSON file for each datapoint, optionally you can also have a singular JSONLines with all 50, ensure to adjust the transformers mime types in that scenario like in the HF workshop above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28974d5-3aeb-4efd-ab36-f377342c3387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "# s3 bucket subfolder with test data\n",
    "prefix = 'batch-input/'\n",
    "\n",
    "# JSON payload\n",
    "payload = {\"inputs\": \"I am super happy right now.\"}\n",
    "\n",
    "# S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Upload json objects for each sample data point\n",
    "for i in range(50):\n",
    "    file_content = json.dumps(payload)\n",
    "    file_name = f\"{prefix}input_{i:03}.json\"\n",
    "    \n",
    "    s3.put_object(\n",
    "        Bucket=default_bucket,\n",
    "        Key=file_name,\n",
    "        Body=file_content,\n",
    "        ContentType='application/json'\n",
    "    )\n",
    "    print(f\"Uploaded: s3://{default_bucket}/{file_name}\")\n",
    "    \n",
    "input_data_path = f\"s3://{default_bucket}/{prefix}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0660929-4916-4a7e-aff1-2be0b987b816",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls {input_data_path} #verify input data points there"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7c954e-7928-4b55-b7f9-1c8d460d210a",
   "metadata": {},
   "source": [
    "## Transform Job\n",
    "Here we kick off a transform job, adjust instance type, count, and also the mime type depending on the data format you're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbb6e09-e0b4-4a99-a1d3-e6296108e200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# where to dump output results\n",
    "out_prefix = \"transform-results/\"\n",
    "output_data_path = f\"s3://{default_bucket}/{out_prefix}\"\n",
    "\n",
    "# transformer object\n",
    "transformer = huggingface_model.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    strategy='SingleRecord',\n",
    "    assemble_with='Line',\n",
    "    output_path=output_data_path,\n",
    "    accept=\"application/json\"\n",
    ")\n",
    "\n",
    "# Feed the test data\n",
    "transformer.transform(input_data_path, \n",
    "                      content_type=\"application/json\", \n",
    "                      split_type='Line')\n",
    "\n",
    "# Wait for job to complete\n",
    "print(\"Waiting for transform job: \" + transformer.latest_transform_job.job_name)\n",
    "transformer.wait()\n",
    "output = transformer.output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478acfee-80cf-4292-aaeb-6b45018a3fbe",
   "metadata": {},
   "source": [
    "## Parse Output Results\n",
    "Pull down the results from S3 and read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903d6c71-0990-4d6e-aa41-e3ca772ea275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output dataset s3 path\n",
    "output_results = transformer.output_path\n",
    "output_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fc7f87-8231-4b93-9ff1-292fb331e038",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls {output_results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a0b17f-c7f5-4322-a051-349942e9de20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy over the s3 output data results locally\n",
    "import subprocess\n",
    "subprocess.run(f\"mkdir -p results && aws s3 cp {output_results} ./results/ --recursive\", shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb30e70-4c82-4287-a6fe-6e4af3d3183c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read a sample output data point\n",
    "import json\n",
    "with open('results/input_005.json.out', 'r') as f:\n",
    "    data = json.load(f)\n",
    "print(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
