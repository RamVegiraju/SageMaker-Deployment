{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a24d9f72-a40d-41b3-bf4e-77c4652d90b4",
   "metadata": {},
   "source": [
    "# Load Testing & Profiling SageMaker Endpoints\n",
    "\n",
    "In this sample we use the same notebook code from Part 1 of the Inference Video Series to create a SageMaker Endpoint: https://www.youtube.com/watch?v=omFOOr4elnc&list=PLThJtS7RDkOeo9mpNjFVnIGDyiazAm9Uk&index=2. For further context around the code and more details please follow the original notebook. \n",
    "\n",
    "In this notebook we'll quickly create the endpoint and focus on how you can load test using an open source Python load testing tool known as [Locust](https://locust.io/). For further information on Locust please refer to the official documentation and attached blog below:\n",
    "- <b>Docs</b>: https://docs.locust.io/en/stable/\n",
    "- <b>Starter Blog</b>: https://towardsdatascience.com/why-load-testing-is-essential-to-take-your-ml-app-to-production-faab0df1c4e1?sk=408d5c906510883bd6ac615df24103d2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f1e3c4-3b0b-4b97-8907-a1e0cfebba3d",
   "metadata": {},
   "source": [
    "## Setup & Environment\n",
    "We will be working in a ml.c5.4xlarge SageMaker Classic Notebook Instance using a conda_python3 kernel. Note you can scale this instance type to one with more CPU cores/compute if you want to increase the concurrency for your load tests, we keep it very minimal in this sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301b970b-57d1-4511-8406-aaae025ad449",
   "metadata": {},
   "source": [
    "## Endpoint Creation\n",
    "For an end to end explained guide on pre-trained deployment please refer to the earlier notebook here: https://github.com/RamVegiraju/SageMaker-Deployment/blob/master/SM-Inference-Video-Series/Pre-Trained-Model-Dept/pre-trained-sklearn-model-dept.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9398e4-0819-49a6-85bd-994b63a5c135",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U sagemaker boto3 scikit-learn locust --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6941896-5228-4f6d-aa3d-40ee34359bbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import image_uris\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import joblib\n",
    "import pickle\n",
    "import tarfile\n",
    "import sagemaker\n",
    "from sagemaker.estimator import Estimator\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "import subprocess\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "region = sess._region_name\n",
    "account_id = sess.account_id()\n",
    "s3_model_prefix = \"djl-sme-sklearn-regression\" \n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e796c9a-2344-4214-86ca-4e789d422397",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "python3 local_model.py\n",
    "tar -cvpzf model.tar.gz model.joblib requirements.txt model.py serving.properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fc57c0-e25e-48e8-a13b-75e25f8d24d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# upload model data to S3\n",
    "with open(\"model.tar.gz\", \"rb\") as f:\n",
    "    s3_client.upload_fileobj(f, bucket, \"{}/model.tar.gz\".format(s3_model_prefix))\n",
    "sme_artifacts = \"s3://{}/{}/{}\".format(bucket, s3_model_prefix, \"model.tar.gz\")\n",
    "# replace this with your ECR image URI based off of your region, we are utilizing the CPU image here\n",
    "inference_image_uri = '763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.29.0-cpu-full'\n",
    "print(f\"Pushing the data to the following location: {sme_artifacts}\")\n",
    "print(f\"Using the following serving image: {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe02e24-0e3f-4b87-830c-c09c21e58df7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Step 1: Model Creation\n",
    "sme_model_name = \"sklearn-djl-sme\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(\"Model name: \" + sme_model_name)\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=sme_model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\"Image\": inference_image_uri, \"Mode\": \"SingleModel\", \"ModelDataUrl\": sme_artifacts},\n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Created Model: {model_arn}\")\n",
    "\n",
    "#Step 2: EPC Creation\n",
    "sme_epc_name = \"sklearn-djl-sme-epc\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=sme_epc_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"sklearnvariant\",\n",
    "            \"ModelName\": sme_model_name,\n",
    "            \"InstanceType\": \"ml.c5.xlarge\",\n",
    "            \"InitialInstanceCount\": 1\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(\"Endpoint Configuration Arn: \" + endpoint_config_response[\"EndpointConfigArn\"])\n",
    "\n",
    "#Step 3: EP Creation\n",
    "sme_endpoint_name = \"sklearn-djl-ep-sme\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=sme_endpoint_name,\n",
    "    EndpointConfigName=sme_epc_name,\n",
    ")\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])\n",
    "\n",
    "#Monitor creation\n",
    "describe_endpoint_response = sm_client.describe_endpoint(EndpointName=sme_endpoint_name)\n",
    "while describe_endpoint_response[\"EndpointStatus\"] == \"Creating\":\n",
    "    describe_endpoint_response = sm_client.describe_endpoint(EndpointName=sme_endpoint_name)\n",
    "    print(describe_endpoint_response[\"EndpointStatus\"])\n",
    "    time.sleep(15)\n",
    "print(describe_endpoint_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946d9b57-a4ed-414c-bb0c-829f8bd8dcde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sample invocation\n",
    "import json\n",
    "content_type = \"application/json\"\n",
    "request_body = '[[0.5]]' #replace with your request body\n",
    "\n",
    "response = smr_client.invoke_endpoint(\n",
    "    EndpointName=sme_endpoint_name,\n",
    "    ContentType=content_type,\n",
    "    Body=request_body)\n",
    "result = json.loads(response['Body'].read().decode())\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ee60b7-402f-4192-aa8c-323f4403fd5d",
   "metadata": {},
   "source": [
    "## Load Testing & Profiling SM Endpoint W/ Locust\n",
    "Note that if you would like to scale the throughput/concurrency you can toggle/play with the user and worker count in the distributed.sh shell script. But as you increase your worker count the environment in which you run your load tests you should also ensure has enough compute to generate the load that you are trying to achieve. With [Locust Distributed Mode](https://docs.locust.io/en/stable/running-distributed.html) you can also run load tests across multiple machines (ex: EC2 instances) as you scale your traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a514efd4-9b9b-4106-b113-980d773741d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash -s \"$sme_endpoint_name\"\n",
    "chmod +x distributed.sh\n",
    "./distributed.sh $1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1222c88-6689-42cc-9318-0b267a632978",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "locust_data = pd.read_csv('results_stats.csv')\n",
    "for index, row in locust_data.head(n=2).iterrows():\n",
    "     print(index, row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4fcb33-1a10-413f-9af4-1bb4b5414a2f",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "Ensure to delete your endpoint to avoid incurring further costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e162b187-2c99-460a-a560-68cbbf50acbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint(EndpointName = sme_endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
