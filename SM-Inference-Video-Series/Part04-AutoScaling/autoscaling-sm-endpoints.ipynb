{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f6f151b-dc2f-4e65-9263-98a9bb0c4fea",
   "metadata": {},
   "source": [
    "# AutoScaling SageMaker Endpoints\n",
    "\n",
    "In this sample we use the same notebook code from Part 1 of the Inference Video Series to create a SageMaker Endpoint: https://www.youtube.com/watch?v=omFOOr4elnc&list=PLThJtS7RDkOeo9mpNjFVnIGDyiazAm9Uk&index=2. For further context around the code and more details please follow the original notebook.\n",
    "\n",
    "Specifically for this notebook we explore how we can enable AutoScaling for SageMaker Endpoints expanding upon the previous section which covered [Load Testing SM Endpoints](https://www.youtube.com/watch?v=ZURoZZbiqj0&t=1120s). Note there's also features such as [Scale Down to Zero](https://aws.amazon.com/blogs/machine-learning/unlock-cost-savings-with-the-new-scale-down-to-zero-feature-in-amazon-sagemaker-inference/) for Real-Time Endpoints that we'll explore more in depth in future sections.\n",
    "\n",
    "### Additional Resources/Credits/References\n",
    "\n",
    "- [AutoScaling Blog](https://towardsdatascience.com/autoscaling-sagemaker-real-time-endpoints-b1b6e6731c59/)\n",
    "- [Scale Down To Zero Blog](https://aws.amazon.com/blogs/machine-learning/unlock-cost-savings-with-the-new-scale-down-to-zero-feature-in-amazon-sagemaker-inference/)\n",
    "- [AutoScaling Docs](https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling.html)\n",
    "- [Transformers AutoScaling Sample](https://github.com/philschmid/huggingface-sagemaker-workshop-series/blob/main/workshop_2_going_production/lab3_autoscaling.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4b1206-1cf7-4dec-8940-735a1bccec67",
   "metadata": {},
   "source": [
    "## Setup & Environment\n",
    "We will be working in a ml.c5.2xlarge SageMaker Classic Notebook Instance using a conda_python3 kernel. You can also optionally use SageMaker Studio or an environment where you have proper credentials for working with SageMaker. Here we scale our endpoint to have two c5.xlarge instances so ensure that you have access to this amount or put in a limit request for your account."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4948431a-e4f4-4241-ba93-9cb4895c67b2",
   "metadata": {},
   "source": [
    "## Endpoint Creation\n",
    "For an end to end explained guide on pre-trained deployment please refer to the earlier notebook here: https://github.com/RamVegiraju/SageMaker-Deployment/blob/master/SM-Inference-Video-Series/Pre-Trained-Model-Dept/pre-trained-sklearn-model-dept.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bf9d4a-8c9e-4c02-8d4d-a8181f7d5fe1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U sagemaker boto3 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d11eeec-6a74-4a23-8bd0-9548d672df5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import image_uris\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import joblib\n",
    "import pickle\n",
    "import tarfile\n",
    "import sagemaker\n",
    "from sagemaker.estimator import Estimator\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "import subprocess\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "region = sess._region_name\n",
    "account_id = sess.account_id()\n",
    "s3_model_prefix = \"djl-sme-sklearn-regression\" \n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f310dc5f-0609-47f7-b31c-8c6bee27a925",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "python3 local_model.py\n",
    "tar -cvpzf model.tar.gz model.joblib requirements.txt model.py serving.properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2294865-8631-4ea0-b739-c02ef87b43cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# upload model data to S3\n",
    "with open(\"model.tar.gz\", \"rb\") as f:\n",
    "    s3_client.upload_fileobj(f, bucket, \"{}/model.tar.gz\".format(s3_model_prefix))\n",
    "sme_artifacts = \"s3://{}/{}/{}\".format(bucket, s3_model_prefix, \"model.tar.gz\")\n",
    "# replace this with your ECR image URI based off of your region, we are utilizing the CPU image here\n",
    "inference_image_uri = '763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.29.0-cpu-full'\n",
    "print(f\"Pushing the data to the following location: {sme_artifacts}\")\n",
    "print(f\"Using the following serving image: {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d7be50-4357-4ead-bd84-f20e74dd09d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Step 1: Model Creation\n",
    "sme_model_name = \"sklearn-djl-sme\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(\"Model name: \" + sme_model_name)\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=sme_model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\"Image\": inference_image_uri, \"Mode\": \"SingleModel\", \"ModelDataUrl\": sme_artifacts},\n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Created Model: {model_arn}\")\n",
    "\n",
    "#Step 2: EPC Creation\n",
    "variant_name = \"sklearnvariant\"\n",
    "sme_epc_name = \"sklearn-djl-sme-epc\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=sme_epc_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": variant_name,\n",
    "            \"ModelName\": sme_model_name,\n",
    "            \"InstanceType\": \"ml.c5.xlarge\",\n",
    "            \"InitialInstanceCount\": 1\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(\"Endpoint Configuration Arn: \" + endpoint_config_response[\"EndpointConfigArn\"])\n",
    "\n",
    "#Step 3: EP Creation\n",
    "sme_endpoint_name = \"sklearn-djl-ep-sme\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=sme_endpoint_name,\n",
    "    EndpointConfigName=sme_epc_name,\n",
    ")\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])\n",
    "\n",
    "#Monitor creation\n",
    "describe_endpoint_response = sm_client.describe_endpoint(EndpointName=sme_endpoint_name)\n",
    "while describe_endpoint_response[\"EndpointStatus\"] == \"Creating\":\n",
    "    describe_endpoint_response = sm_client.describe_endpoint(EndpointName=sme_endpoint_name)\n",
    "    print(describe_endpoint_response[\"EndpointStatus\"])\n",
    "    time.sleep(15)\n",
    "print(describe_endpoint_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad99e48-2764-46fb-be64-a8c000abaf9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sample invocation\n",
    "import json\n",
    "content_type = \"application/json\"\n",
    "request_body = '[[0.5]]' #replace with your request body\n",
    "\n",
    "response = smr_client.invoke_endpoint(\n",
    "    EndpointName=sme_endpoint_name,\n",
    "    ContentType=content_type,\n",
    "    Body=request_body)\n",
    "result = json.loads(response['Body'].read().decode())\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b80fca-b5b6-4d97-a0ec-36affa7a2c07",
   "metadata": {},
   "source": [
    "## Enabling AutoScaling for SageMaker Endpoint\n",
    "SageMaker Real-Time Endpoints are integrated with <b>Application AutoScaling</b>: https://docs.aws.amazon.com/autoscaling/application/userguide/what-is-application-auto-scaling.html. \n",
    "\n",
    "In this case with a SageMaker Endpoint variant we can define different types of scaling policies with CloudWatch metrics that are supported by SM Endpoints: https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html#cloudwatch-metrics-endpoint-invocation. For this case we use the <b>invocations per instance</b> as the target metric, but you can also AutoScale on the metric of your choice (some use CPU or GPU Utilization if you want scale based off of hardware saturation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b906733-da2c-40e1-b60d-d10897fda954",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# AutoScaling client\n",
    "asg = boto3.client('application-autoscaling')\n",
    "\n",
    "# Resource type is variant and the unique identifier is the resource ID.\n",
    "resource_id=f\"endpoint/{sme_endpoint_name}/variant/{variant_name}\"\n",
    "\n",
    "# Instance count\n",
    "min_instance_count = 1\n",
    "max_instance_count = 2\n",
    "\n",
    "# scaling configuration\n",
    "response = asg.register_scalable_target(\n",
    "    ServiceNamespace='sagemaker', #\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension='sagemaker:variant:DesiredInstanceCount', \n",
    "    MinCapacity=min_instance_count,\n",
    "    MaxCapacity=max_instance_count\n",
    ")\n",
    "\n",
    "#Target Scaling\n",
    "# Metric we use is invocations per instance: https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html#cloudwatch-metrics-endpoint-invocation\n",
    "response = asg.put_scaling_policy(\n",
    "    PolicyName=f'Request-ScalingPolicy-{sme_endpoint_name}',\n",
    "    ServiceNamespace='sagemaker',\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension='sagemaker:variant:DesiredInstanceCount',\n",
    "    PolicyType='TargetTrackingScaling',\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        'TargetValue': 10.0, # Threshold setting to 10 invocations per minute\n",
    "        'PredefinedMetricSpecification': {\n",
    "            'PredefinedMetricType': 'SageMakerVariantInvocationsPerInstance',\n",
    "        },\n",
    "        'ScaleInCooldown': 400, # duration until scale in, increasing so we can display instance count rising later\n",
    "        'ScaleOutCooldown': 60 # duration between scale out\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deff58e0-a020-4e04-97fd-8db6a50e9868",
   "metadata": {},
   "source": [
    "## Test AutoScaling\n",
    "Here we send requests for a certain duration that will hit our scaling target of 10 invocations per minute, we should see our instance count scale to two and the status of the endpoint changing to updating during that timeframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b4f3d2-2fd7-4bab-be66-146af82f148f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# code snippet borrowed from following NB: https://github.com/philschmid/huggingface-sagemaker-workshop-series/blob/main/workshop_2_going_production/lab3_autoscaling.ipynb\n",
    "request_duration = 250\n",
    "end_time = time.time() + request_duration\n",
    "print(f\"test will run for {request_duration} seconds\")\n",
    "while time.time() < end_time:\n",
    "    resp = smr_client.invoke_endpoint(EndpointName=sme_endpoint_name, \n",
    "                                      Body=request_body, \n",
    "                                      ContentType=content_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8ae4d2-f5ed-4cc0-839c-8b43e2b8903a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# re-run this cell if needed, the status should be updating during AutoScaling\n",
    "response = sm_client.describe_endpoint(EndpointName=sme_endpoint_name)\n",
    "status = response['EndpointStatus']\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "# check the endpoint status to get the instance count and see it increase over time\n",
    "while status=='Updating':\n",
    "    time.sleep(15)\n",
    "    response = sm_client.describe_endpoint(EndpointName=sme_endpoint_name)\n",
    "    status = response['EndpointStatus']\n",
    "    instance_count = response['ProductionVariants'][0]['CurrentInstanceCount']\n",
    "    print(f\"Status: {status}\")\n",
    "    print(f\"Current Instance count: {instance_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b99e3e-f12a-41e0-91f0-b8630eead4b3",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "Ensure to delete your endpoint to not incur any further costs, you can also enable scale down to zero optionally. Make sure to also turn off your notebook instance after usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd84b0b9-df92-4f38-b17b-0db415b1c234",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint(EndpointName = sme_endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
