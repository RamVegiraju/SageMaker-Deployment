{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8db7047-935e-4f53-a3fe-1f186a11e4eb",
   "metadata": {},
   "source": [
    "# Bring Your Own Container (BYOC) SageMaker Inference\n",
    "In this example we take a look at how to implement a BYOC approach. This approach can be used if you want to bring your own serving stack and don't want to use one of the available managed SageMaker/AWS images. Here we implement a Flask/Guincorn based serving stack, but you can bring your own serving engine and enable it to listen on the right port (8080) and accept requests on the paths that are defined to be compatible with SageMaker AI.\n",
    "\n",
    "In this example we will train a sample SKLearn model locally and then adopt it using the BYOC approach, you can substitute this with your own model or whatever frameworks/packages you are using.\n",
    "\n",
    "## Environment\n",
    "Can run on a ml.t3.medium SM Classic Notebook Instance, if elsewhere ensure to have Docker installed and running.\n",
    "\n",
    "## Additional Resources\n",
    "Official Docs: https://docs.aws.amazon.com/sagemaker/latest/dg/adapt-inference-container.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5218c6ae-e1e2-4f7f-8fc8-9d7780aa7a03",
   "metadata": {},
   "source": [
    "## Local Model Training\n",
    "Let's first train a sample SKLearn model and then setup our Dockerfile and serving stack to serve this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ad458b4-3dbc-4d4f-a840-cd0d8a2c7ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker boto3 numpy scikit-learn==1.7.1 --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c098b0e8-3cd3-49e6-9baf-8ed122f875fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Generate dummy data\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 1)\n",
    "y = 2 * X + 1 + 0.1 * np.random.randn(100, 1)  \n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Linear Regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save the trained model to a file\n",
    "import joblib\n",
    "model_filename = \"model.joblib\"\n",
    "joblib.dump(model, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2647df1c-71f0-4dde-9939-d3a0a1daeb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "serialized_model = joblib.load(model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b182853-7c34-42a6-9369-c3438ee55ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample inference\n",
    "payload = [[0.5]]\n",
    "res = serialized_model.predict(payload).tolist()[0]\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d0aa4d-85a0-4fb9-9c00-05943743694d",
   "metadata": {},
   "source": [
    "## Constructs for BYOC\n",
    "We need a \n",
    "- <b>Dockerfile</b> to install the dependecies we need and point towards our webserver\n",
    "- <b>predictor.py</b> implements our Flask Web Server, your model's inference is here, adjust depending on your model and serving logic\n",
    "- <b>serve</b> and <b>nginx</b> you can keep as is, adjust this if you want to change your serving logic\n",
    "\n",
    "\n",
    "We will create these files then build our image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9789239-d46b-4c04-baa4-67f8c9900498",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "# Build an image that can do training and inference in SageMaker\n",
    "# This is a Python 3 image that uses the nginx, gunicorn, flask stack\n",
    "# for serving inferences in a stable way.\n",
    "\n",
    "FROM python:3.10-slim\n",
    "\n",
    "MAINTAINER Amazon AI <sage-learner@amazon.com>\n",
    "\n",
    "RUN apt-get -y update && apt-get install -y --no-install-recommends \\\n",
    "         wget \\\n",
    "         python3 \\\n",
    "         nginx \\\n",
    "         ca-certificates \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Here we get all python packages.\n",
    "# There's substantial overlap between scipy and numpy that we eliminate by\n",
    "# linking them together. Likewise, pip leaves the install caches populated which uses\n",
    "# a significant amount of space. These optimizations save a fair amount of space in the\n",
    "# image, which reduces start up time.\n",
    "RUN pip --no-cache-dir install numpy pandas flask gunicorn boto3 joblib scikit-learn==1.7.1\n",
    "\n",
    "# Set some environment variables. PYTHONUNBUFFERED keeps Python from buffering our standard\n",
    "# output stream, which means that logs can be delivered to the user quickly. PYTHONDONTWRITEBYTECODE\n",
    "# keeps Python from writing the .pyc files which are unnecessary in this case. We also update\n",
    "# PATH so that the train and serve programs are found when the container is invoked.\n",
    "\n",
    "ENV PYTHONUNBUFFERED=TRUE\n",
    "ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
    "ENV PATH=\"/opt/program:${PATH}\"\n",
    "\n",
    "# Set up the program in the image\n",
    "COPY regressor /opt/program\n",
    "WORKDIR /opt/program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f423330-d9c2-4607-ac71-7c93b6fac705",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile nginx.conf\n",
    "worker_processes 1;\n",
    "daemon off; # Prevent forking\n",
    "\n",
    "\n",
    "pid /tmp/nginx.pid;\n",
    "error_log /var/log/nginx/error.log;\n",
    "\n",
    "events {\n",
    "  # defaults\n",
    "}\n",
    "\n",
    "http {\n",
    "  include /etc/nginx/mime.types;\n",
    "  default_type application/octet-stream;\n",
    "  access_log /var/log/nginx/access.log combined;\n",
    "  \n",
    "  upstream gunicorn {\n",
    "    server unix:/tmp/gunicorn.sock;\n",
    "  }\n",
    "\n",
    "  server {\n",
    "    listen 8080 deferred;\n",
    "    client_max_body_size 5m;\n",
    "\n",
    "    keepalive_timeout 5;\n",
    "    proxy_read_timeout 1200s;\n",
    "\n",
    "    location ~ ^/(ping|invocations) {\n",
    "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
    "      proxy_set_header Host $http_host;\n",
    "      proxy_redirect off;\n",
    "      proxy_pass http://gunicorn;\n",
    "    }\n",
    "\n",
    "    location / {\n",
    "      return 404 \"{}\";\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7ab9d5-8632-4dd0-97f4-8a49f4210fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile predictor.py\n",
    "from flask import Flask\n",
    "import flask\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import joblib\n",
    "\n",
    "model = joblib.load(os.path.join(\"model.joblib\"))\n",
    "\n",
    "# The flask app for serving predictions\n",
    "app = Flask(__name__)\n",
    "@app.route('/ping', methods=['GET'])\n",
    "def ping():\n",
    "    # Check if the classifier was loaded correctly\n",
    "    health = model is not None\n",
    "    status = 200 if health else 404\n",
    "    return flask.Response(response= '\\n', status=status, mimetype='application/json')\n",
    "\n",
    "\n",
    "@app.route('/invocations', methods=['POST'])\n",
    "def transformation():\n",
    "    \n",
    "    #Process input\n",
    "    input_json = flask.request.get_json()\n",
    "    if \"input\" not in input_json:\n",
    "        return flask.Response(json.dumps({\"error\":\"missing 'input'\"}), status=400, mimetype=\"application/json\")\n",
    "    resp = input_json['input']\n",
    "    \n",
    "    #model inference\n",
    "    output = model.predict(resp).tolist()[0]\n",
    "\n",
    "    # Transform predictions to JSON\n",
    "    result = {\n",
    "        'output': output\n",
    "        }\n",
    "\n",
    "    resultjson = json.dumps(result)\n",
    "    return flask.Response(response=resultjson, status=200, mimetype='application/json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166db0dd-2a10-4ca4-8ffd-8856e607b266",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile serve\n",
    "#!/usr/bin/env python\n",
    "\n",
    "# This file implements the scoring service shell. You don't necessarily need to modify it for various\n",
    "# algorithms. It starts nginx and gunicorn with the correct configurations and then simply waits until\n",
    "# gunicorn exits.\n",
    "#\n",
    "# The flask server is specified to be the app object in wsgi.py\n",
    "#\n",
    "# We set the following parameters:\n",
    "#\n",
    "# Parameter                Environment Variable              Default Value\n",
    "# ---------                --------------------              -------------\n",
    "# number of workers        MODEL_SERVER_WORKERS              the number of CPU cores\n",
    "# timeout                  MODEL_SERVER_TIMEOUT              60 seconds\n",
    "\n",
    "import multiprocessing\n",
    "import os\n",
    "import signal\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "cpu_count = multiprocessing.cpu_count()\n",
    "\n",
    "model_server_timeout = os.environ.get('MODEL_SERVER_TIMEOUT', 60)\n",
    "model_server_workers = int(os.environ.get('MODEL_SERVER_WORKERS', cpu_count))\n",
    "\n",
    "def sigterm_handler(nginx_pid, gunicorn_pid):\n",
    "    try:\n",
    "        os.kill(nginx_pid, signal.SIGQUIT)\n",
    "    except OSError:\n",
    "        pass\n",
    "    try:\n",
    "        os.kill(gunicorn_pid, signal.SIGTERM)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    sys.exit(0)\n",
    "\n",
    "def start_server():\n",
    "    print('Starting the inference server with {} workers.'.format(model_server_workers))\n",
    "\n",
    "\n",
    "    # link the log streams to stdout/err so they will be logged to the container logs\n",
    "    subprocess.check_call(['ln', '-sf', '/dev/stdout', '/var/log/nginx/access.log'])\n",
    "    subprocess.check_call(['ln', '-sf', '/dev/stderr', '/var/log/nginx/error.log'])\n",
    "\n",
    "    nginx = subprocess.Popen(['nginx', '-c', '/opt/program/nginx.conf'])\n",
    "    gunicorn = subprocess.Popen(['gunicorn',\n",
    "                                 '--timeout', str(model_server_timeout),\n",
    "                                 '-k', 'sync',\n",
    "                                 '-b', 'unix:/tmp/gunicorn.sock',\n",
    "                                 '-w', str(model_server_workers),\n",
    "                                 'wsgi:app'])\n",
    "\n",
    "    signal.signal(signal.SIGTERM, lambda a, b: sigterm_handler(nginx.pid, gunicorn.pid))\n",
    "\n",
    "    # If either subprocess exits, so do we.\n",
    "    pids = set([nginx.pid, gunicorn.pid])\n",
    "    while True:\n",
    "        pid, _ = os.wait()\n",
    "        if pid in pids:\n",
    "            break\n",
    "\n",
    "    sigterm_handler(nginx.pid, gunicorn.pid)\n",
    "    print('Inference server exiting')\n",
    "\n",
    "# The main routine just invokes the start function.\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2897e1b5-7155-4cbc-9e7b-a1d863ea3575",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wsgi.py\n",
    "import predictor as myapp\n",
    "\n",
    "# This is just a simple wrapper for gunicorn to find your app.\n",
    "# If you want to change the algorithm file, simply change \"predictor\" above to the\n",
    "# new file.\n",
    "\n",
    "app = myapp.app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3939700a-252e-4acd-b726-01b9b274907f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "mkdir container\n",
    "cd container\n",
    "mkdir regressor\n",
    "cd ..\n",
    "mv Dockerfile container/\n",
    "mv model.joblib nginx.conf predictor.py serve wsgi.py container/regressor/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8fdccf-05a2-4622-ae3a-63bb191861b5",
   "metadata": {},
   "source": [
    "## Build Docker Image\n",
    "Here we can build our Docker image that will install the necessary requirements and point towards the web server you implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775f3ab8-ec9a-4599-906f-56b61e3ff45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# Name of algo -> ECR\n",
    "algorithm_name=sm-pretrained-sklearn-byoc\n",
    "\n",
    "cd container\n",
    "\n",
    "#make serve executable\n",
    "chmod +x regressor/serve\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Region, defaults to us-west-2\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-east-1}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "aws ecr get-login-password --region ${region}|docker login --username AWS --password-stdin ${fullname}\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build  -t ${algorithm_name} .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5f92be-a195-4d10-b4a7-3d29093d4575",
   "metadata": {},
   "source": [
    "## SageMaker RT Endpoint Constructs\n",
    "- SageMaker Model: Model Data & Container\n",
    "- SageMaker EPC: Hardware Details & Variants\n",
    "- SageMaker EP: REST Endpoint to invoke\n",
    "\n",
    "Full Video Explanation: https://www.youtube.com/watch?v=omFOOr4elnc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9263bc3-b0ca-4cc1-9e95-0715a82e9a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sm_client = boto3.client(service_name='sagemaker')\n",
    "runtime_sm_client = boto3.client(service_name='sagemaker-runtime')\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity()['Account']\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "#not really used in this use case, use when need to store model artifacts (Ex: MME)\n",
    "s3_bucket = 'regressor-sagemaker-BYOC'\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44d8439-bf5c-40d7-a4a1-9efb212dc978",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "model_name = 'regressor-byoc-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "model_url = 's3://{}/regressor/'.format(s3_bucket) ## MODEL S3 URL\n",
    "# replace string with your ECR name, algorithm_name in shell file above\n",
    "container = '{}.dkr.ecr.{}.amazonaws.com/sm-pretrained-sklearn-byoc:latest'.format(account_id, region)\n",
    "instance_type = 'ml.c5d.xlarge'\n",
    "\n",
    "print('Model name: ' + model_name)\n",
    "print('Model data Url: ' + model_url)\n",
    "print('Container image: ' + container)\n",
    "\n",
    "container = {\n",
    "    'Image': container\n",
    "}\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    Containers = [container])\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response['ModelArn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6761b2c1-bb6d-419a-b862-712798c2bfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = 'regressor-byoc-config' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print('Endpoint config name: ' + endpoint_config_name)\n",
    "\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType': instance_type,\n",
    "        'InitialInstanceCount': 1,\n",
    "        'InitialVariantWeight': 1,\n",
    "        'ModelName': model_name,\n",
    "        'VariantName': 'AllTraffic'}])\n",
    "\n",
    "print(\"Endpoint config Arn: \" + create_endpoint_config_response['EndpointConfigArn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4742cb-c31f-4c0c-861f-2a1a3ad4d272",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import time\n",
    "\n",
    "endpoint_name = 'regressor-byoc-endpoint' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print('Endpoint name: ' + endpoint_name)\n",
    "\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name)\n",
    "print('Endpoint Arn: ' + create_endpoint_response['EndpointArn'])\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp['EndpointStatus']\n",
    "print(\"Endpoint Status: \" + status)\n",
    "\n",
    "print('Waiting for {} endpoint to be in service...'.format(endpoint_name))\n",
    "waiter = sm_client.get_waiter('endpoint_in_service')\n",
    "waiter.wait(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f19e98-9d0f-4191-9b52-eb6eff80e4d0",
   "metadata": {},
   "source": [
    "## Invoke Endpoint\n",
    "Boto3 Invoke_EP: https://boto3.amazonaws.com/v1/documentation/api/1.9.42/reference/services/sagemaker-runtime.html#SageMakerRuntime.Client.invoke_endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e91bd6-7e83-4c24-89e4-12371600f2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "content_type = \"application/json\"\n",
    "request_body = {\"input\": [[0.5]]}\n",
    "\n",
    "#Serialize data for endpoint\n",
    "data = json.loads(json.dumps(request_body))\n",
    "payload = json.dumps(data)\n",
    "\n",
    "#Endpoint invocation\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=content_type,\n",
    "    Body=payload)\n",
    "\n",
    "#Parse results\n",
    "result = json.loads(response['Body'].read().decode())['output']\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159911a2-2afa-4655-9609-b83dc01b9a3a",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9522759-84da-4915-bccc-e23bef92114e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint(EndpointName = endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
