{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db1b40c1-0db4-44d4-a24c-0627465d5144",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Open-Llama LMI Container Deployment on Amazon SageMaker Real-Time Endpoints\n",
    "In this notebook we take a look at how we can leverage the Large Model Inference (LMI) Container to deploy a sample [OSS Llama variant](https://huggingface.co/openlm-research/open_llama_7b) on [SageMaker Real-Time Endpoints](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html). We explore how we can leverage different LLM serving backends such as TensorRT-LLM and vLLM via the LMI container to deploy Open-Llama. In the coming sections we'll leverage Inference Components to deploy multiple LLMs on a singular endpoint in an efficient manner.\n",
    "\n",
    "#### License: Apache-2.0\n",
    "\n",
    "## Credits/Resources\n",
    "- [Original Notebook](https://github.com/aws-samples/sagemaker-genai-hosting-examples/blob/main/Open-Llama/LMI/open_llama_7b.ipynb)\n",
    "- [LMI Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-container-docs.html)\n",
    "- [TRT-LLM User Guide](https://docs.djl.ai/master/docs/serving/serving/docs/lmi/user_guides/trt_llm_user_guide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf14f91-9617-4b80-91bd-abbfad68d18e",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Instantiate our usual SM clients and setup S3 buckets for model data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e2758a-922c-4046-bebe-3af603702cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import jinja2\n",
    "from sagemaker import image_uris\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab2a882-1e38-4453-930d-e85ad4e713aa",
   "metadata": {},
   "source": [
    "## Container Specification\n",
    "To work with the LMI container you can either provide a serving.properties file as we did with our traditional ML model examples or a Python dictionary with the environment variables for serving the LLM such as [Tensor Parallel](https://docs.djl.ai/master/docs/serving/serving/docs/lmi/deployment_guide/configurations.html#tensor-parallelism-configuration) and [Server Side Batching](https://aws.amazon.com/blogs/machine-learning/improve-throughput-performance-of-llama-2-models-using-amazon-sagemaker/), we also define the backend LLM serving engine we want to use which in this case is [Nvidia's TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM). \n",
    "\n",
    "Note that we also specify the <b>HF Model ID</b> which pulls down the model artifacts from that repo, optionally if you have a custom model you can specify the S3 path with the model data in the model_id key listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb423759-c7b7-4453-ab53-e440a10f57e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = {\n",
    "    \"SERVING_LOAD_MODELS\": \"test::MPI=/opt/ml/model\",\n",
    "    \"OPTION_TENSOR_PARALLEL_DEGREE\": \"max\",\n",
    "    \"OPTION_MODEL_ID\": \"openlm-research/open_llama_7b\",\n",
    "    \"OPTION_ROLLING_BATCH\": \"trtllm\",\n",
    "    \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"16\"\n",
    "}\n",
    "\n",
    "# TRT Image URI\n",
    "trt_llm_image_uri = \"763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.30.0-tensorrtllm0.12.0-cu125\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131d3d46-6de9-45b6-9f3f-2ab379279d7b",
   "metadata": {},
   "source": [
    "## SageMaker Constructs Creation\n",
    "Here we define the usual objects to capture model data, container and specify the hardware requirements for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c1dfe4-f27d-4c31-bb8f-1648bec05b76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = sagemaker.utils.name_from_base(\"lmi-openllama-7b\")\n",
    "print(model_name)\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\n",
    "        \"Image\": trt_llm_image_uri,\n",
    "        \"Environment\": env,\n",
    "    }\n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Created Model: {model_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f2a948-e08d-4b34-a69e-65f2f4718e23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_config_name = f\"{model_name}-config\"\n",
    "endpoint_name = f\"{model_name}-endpoint\"\n",
    "\n",
    "# enabled LOR: https://aws.amazon.com/blogs/machine-learning/minimize-real-time-inference-latency-by-using-amazon-sagemaker-routing-strategies/\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InstanceType\": \"ml.g5.12xlarge\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 2400,\n",
    "            \"RoutingConfig\": {\n",
    "                'RoutingStrategy': 'LEAST_OUTSTANDING_REQUESTS'\n",
    "            },\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "endpoint_config_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb2ff9f-db23-4a93-beb2-bc6cd8d01c4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=f\"{endpoint_name}\", EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(f\"Created Endpoint: {create_endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97472eb5-4545-4248-9aa3-ea0f5413d60b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba8a343-aa1a-4d02-98bc-a9d2d241696e",
   "metadata": {},
   "source": [
    "### Sample Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d107ce5-ba92-42e6-af4f-e4f97b50667c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# boto3 inference sample\n",
    "import json\n",
    "content_type = \"application/json\"\n",
    "payload = {\"inputs\": \"Who is Roger Federer?\"} #optionally add any parameters for your model\n",
    "\n",
    "# sample inference\n",
    "response = smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=content_type,\n",
    "    Body=json.dumps(payload))\n",
    "result = json.loads(response['Body'].read().decode())['generated_text']\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d93049-a98e-49dc-9a7b-455f44ec88c4",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c849df8-dbcc-4241-bf68-216873cb11d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint(EndpointName = endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
