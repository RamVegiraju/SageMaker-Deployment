{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1c0cbdf-7048-4768-89dc-3c13443ca817",
   "metadata": {},
   "source": [
    "# Enabling Tool Calling with SageMaker Real-Time Endpoints utilizing vLLM via the LMI Container\n",
    "\n",
    "In this notebook we'll explore how you can enable tool-calling directly to SageMaker Hosted LLMs via vLLM. While you can use orchestration frameworks such as LangChain with their higher level Agent constructs, vLLM also supports native tool-calling via it's serving engine. \n",
    "\n",
    "For more complicated Agentic workflows with multiple agents and built-in memory/session management (note you can also enable sticky session routing with vLLM) I'd recommend utilizing LangChain and multi-agent frameworks such as LangGraph. However, in the case you have more simple tool specs and workflows and want to stick to native vLLM specs this is a great option to consider.\n",
    "\n",
    "### Additional Resources/Credits\n",
    "\n",
    "- Great blog by my colleague [Davide Gallitelli](https://www.linkedin.com/in/dgallitelli/) that I used to help understand this functionality: https://dgallitelli95.medium.com/tool-calling-with-amazon-sagemaker-ai-and-djl-serving-inference-6a97dc854881. Check out some of his other work he puts some great stuff out there.\n",
    "- vLLM Official Tool Calling Docs: https://docs.vllm.ai/en/stable/features/tool_calling.html\n",
    "- LMI Container Intro: https://www.youtube.com/watch?v=N0r5AWZe2HU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fe46e0-00aa-4d39-b5e1-8cfc38a79bfb",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Executing in a SM Classic NB Instance on a c5.2xlarge instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b708d3c6-1446-403d-988b-9a2ed2063709",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sagemaker --upgrade --quiet --no-warn-conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0225fa-6824-44e0-bc21-df793f74a6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "account_id = sess.account_id()\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\")  # client to intreract with SageMaker\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")  # client to intreract with SageMaker Endpoints\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n",
    "print(f\"sagemaker version: {sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bb74e0-31f2-4896-ae5f-03a555371d3e",
   "metadata": {},
   "source": [
    "## Sample Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97122934-a9e3-4e09-b066-be0a456fa318",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify hardware\n",
    "instance_type = \"ml.g5.12xlarge\"\n",
    "num_gpu = 4\n",
    "\n",
    "# specify container LMIv16\n",
    "CONTAINER_VERSION = \"0.34.0-lmi16.0.0-cu128\"\n",
    "inference_image = f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:{CONTAINER_VERSION}\"\n",
    "print(f\"Using image URI: {inference_image}\")\n",
    "\n",
    "#utilize the vLLM async handler: \n",
    "vllm_env = {\n",
    "    \"HF_MODEL_ID\": \"Qwen/Qwen3-8B\",\n",
    "    \"HF_TOKEN\": \"Enter HF Token\",\n",
    "    \"SERVING_FAIL_FAST\": \"true\",\n",
    "    \"OPTION_ASYNC_MODE\": \"true\",\n",
    "    \"OPTION_ROLLING_BATCH\": \"disable\",\n",
    "    \"OPTION_TENSOR_PARALLEL_DEGREE\": json.dumps(num_gpu),\n",
    "    \"OPTION_ENTRYPOINT\": \"djl_python.lmi_vllm.vllm_async_service\",\n",
    "    \"OPTION_TRUST_REMOTE_CODE\": \"true\",\n",
    "    \"OPTION_ENABLE_AUTO_TOOL_CHOICE\": \"true\",\n",
    "    \"OPTION_TOOL_CALL_PARSER\": \"hermes\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78403a27-c9b3-4fc0-b353-504cbe8d8cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.compute_resource_requirements.resource_requirements import ResourceRequirements\n",
    "\n",
    "# SageMaker Constructs\n",
    "model_name = sagemaker.utils.name_from_base(\"lmi-qwen\")\n",
    "endpoint_name = model_name\n",
    "inference_component_name = f\"ic-{model_name}\"\n",
    "\n",
    "# SageMaker Model Object -> vLLM env\n",
    "lmi_model = sagemaker.Model(\n",
    "    image_uri=inference_image,\n",
    "    env=vllm_env,\n",
    "    role=role,\n",
    "    name=model_name,\n",
    ")\n",
    "\n",
    "lmi_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    container_startup_health_check_timeout=600,\n",
    "    endpoint_name=endpoint_name,\n",
    "    endpoint_type=sagemaker.enums.EndpointType.INFERENCE_COMPONENT_BASED,\n",
    "    inference_component_name=inference_component_name,\n",
    "    #check the memory available for your instance\n",
    "    resources=ResourceRequirements(requests={\"num_accelerators\": 4, \"memory\": 1024*50, \"copies\": 1,}),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc93b34-01df-4f65-b84e-13e8797c6b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "content_type = \"application/json\"\n",
    "\n",
    "# Adjust payload and parameters as needed\n",
    "payload = \"Who is Roger Federer?\"\n",
    "response = smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    InferenceComponentName=inference_component_name, #specify IC name\n",
    "    ContentType=content_type,\n",
    "    Accept=content_type,\n",
    "    Body=json.dumps(\n",
    "        {\n",
    "            \"inputs\": payload,\n",
    "            \"parameters\": {\n",
    "                \"max_new_tokens\": 100  # Adjust this value as needed\n",
    "                },\n",
    "        }\n",
    "    ),\n",
    ")\n",
    "result = json.loads(response[\"Body\"].read().decode())['generated_text']\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a291be8-eb14-470a-bd4a-23147e333d72",
   "metadata": {},
   "source": [
    "## Building our Mock Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35de492-a006-4a5b-b5be-de87307bf62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# --- Model and data ---\n",
    "class BioData(BaseModel):\n",
    "    age: int\n",
    "    gender: str\n",
    "    occupation: str\n",
    "    interests: list[str]\n",
    "\n",
    "people_db = {\n",
    "    \"Ram\":   BioData(age=25, gender=\"Male\",   occupation=\"Engineer\", interests=[\"Reading\", \"Traveling\"]),\n",
    "    \"Shyam\": BioData(age=22, gender=\"Male\",   occupation=\"Doctor\",   interests=[\"Reading\", \"Traveling\"]),\n",
    "    \"Sita\":  BioData(age=21, gender=\"Female\", occupation=\"Teacher\",  interests=[\"Reading\", \"Traveling\"]),\n",
    "    \"Gita\":  BioData(age=23, gender=\"Female\", occupation=\"Lawyer\",   interests=[\"Reading\", \"Traveling\"]),\n",
    "    \"Hari\":  BioData(age=24, gender=\"Male\",   occupation=\"Engineer\", interests=[\"Reading\", \"Traveling\"]),\n",
    "}\n",
    "\n",
    "def return_biodata(name: str) -> dict:\n",
    "    \"\"\"Return biodata as a plain JSON-serializable dictionary.\"\"\"\n",
    "    if name not in people_db:\n",
    "        return {\"error\": f\"Person '{name}' not found\"}\n",
    "    return people_db[name].model_dump()\n",
    "\n",
    "# --- Tool spec and registry ---\n",
    "biodata_tool_spec = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"return_biodata\",\n",
    "        \"description\": \"Return biodata for a known person name.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\"name\": {\"type\": \"string\"}},\n",
    "            \"required\": [\"name\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "tools = [biodata_tool_spec]\n",
    "TOOLS_REGISTRY = {\"return_biodata\": return_biodata}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365e0304-25df-4471-ac10-6931de457c0f",
   "metadata": {},
   "source": [
    "## Tool Calling Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020cb0ef-3fe8-4a52-93c1-69d9cfce9787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any, Dict, List, Optional, Tuple, Callable\n",
    "\n",
    "def execute_tool(tool_name: str, raw_arguments: str):\n",
    "    \"\"\"\n",
    "    Execute tool with input args detected\n",
    "    \"\"\"\n",
    "    args = json.loads(raw_arguments or \"{}\")\n",
    "    func = TOOLS_REGISTRY.get(tool_name)\n",
    "    if not func:\n",
    "        return {\"error\": f\"Unknown tool: {tool_name}\"}\n",
    "    return func(**args)\n",
    "\n",
    "def handle_tool_calls(\n",
    "    result: Dict[str, Any],\n",
    "    messages: List[Dict[str, Any]],\n",
    "    executor: Callable[[str, str], Any],\n",
    ") -> Tuple[Optional[str], List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Handle tool calls emitted by the model.\n",
    "    If no tools are called, return (assistant_content, messages).\n",
    "    If tools are called, execute each tool, append the results, and return (None, updated_messages).\n",
    "    \"\"\"\n",
    "    msg = result[\"choices\"][0][\"message\"]\n",
    "    tool_calls = msg.get(\"tool_calls\", [])\n",
    "\n",
    "    if not tool_calls:\n",
    "        return msg.get(\"content\"), messages\n",
    "\n",
    "    # Append assistant message with its tool calls\n",
    "    messages.append({k: v for k, v in msg.items() if k in (\"role\", \"content\", \"tool_calls\")})\n",
    "\n",
    "    # Execute each tool sequentially and append outputs\n",
    "    for tc in tool_calls:\n",
    "        tool_id = tc[\"id\"]\n",
    "        tool_name = tc[\"function\"][\"name\"]\n",
    "        raw_args = tc[\"function\"][\"arguments\"]\n",
    "        tool_output = executor(tool_name, raw_args)\n",
    "\n",
    "        messages.append({\n",
    "            \"role\": \"tool\",\n",
    "            \"tool_call_id\": tool_id,\n",
    "            \"name\": tool_name,\n",
    "            \"content\": json.dumps(tool_output, ensure_ascii=False),\n",
    "        })\n",
    "\n",
    "    return None, messages\n",
    "\n",
    "\n",
    "def _invoke_ep(\n",
    "    smr_client,\n",
    "    endpoint_name: str,\n",
    "    inference_component_name: str,\n",
    "    payload: Dict[str, Any],\n",
    "    content_type: str = \"application/json\",\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Helper: Call the SageMaker endpoint and return the decoded JSON result.\n",
    "    \"\"\"\n",
    "    resp = smr_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        InferenceComponentName=inference_component_name,\n",
    "        ContentType=content_type,\n",
    "        Accept=content_type,\n",
    "        Body=json.dumps(payload),\n",
    "    )\n",
    "    return json.loads(resp[\"Body\"].read().decode())\n",
    "\n",
    "\n",
    "def chat_with_tools(\n",
    "    smr_client,\n",
    "    endpoint_name: str,\n",
    "    inference_component_name: str,\n",
    "    messages: List[Dict[str, Any]],\n",
    "    tools: List[Dict[str, Any]],\n",
    "    executor: Callable[[str, str], Any],\n",
    ") -> Tuple[Dict[str, Any], List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Run a full inference loop:\n",
    "    1. Invoke the model once with messages + tools.\n",
    "    2. If tool calls exist, execute them and append their outputs.\n",
    "    3. Re-invoke the model with the updated messages.\n",
    "    Returns (final_result, updated_messages).\n",
    "    \"\"\"\n",
    "    payload = {\"messages\": messages, \"tools\": tools, \"tool_choice\": \"auto\"}\n",
    "\n",
    "    first = _invoke_ep(smr_client, endpoint_name, inference_component_name, payload)\n",
    "    assistant_content, updated_messages = handle_tool_calls(first, messages, executor)\n",
    "\n",
    "    # If no tool calls were made, return first result\n",
    "    if assistant_content is not None:\n",
    "        return first, updated_messages\n",
    "\n",
    "    # Reinvoke after tool execution\n",
    "    final_payload = {\"messages\": updated_messages, \"tools\": tools, \"tool_choice\": \"auto\"}\n",
    "    final = _invoke_ep(smr_client, endpoint_name, inference_component_name, final_payload)\n",
    "    return final, updated_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c098fe3-1df4-4a01-8864-201846ce19e0",
   "metadata": {},
   "source": [
    "## Sample Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42c4d1f-878e-4674-918a-608fb1c678cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- No tool call example ---\n",
    "messages_no_tool = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are a helpful assistant with access to tools. \"\n",
    "            \"Use the 'return_biodata' tool only when the user asks for information about specific people. \"\n",
    "            \"For all general knowledge questions, rely on your own knowledge.\"\n",
    "        ),\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "]\n",
    "\n",
    "\n",
    "# --- Single tool call example ---\n",
    "messages_single_tool = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are a helpful assistant with access to tools. \"\n",
    "            \"Use the 'return_biodata' tool only when the user asks for information about specific people. \"\n",
    "            \"For all general knowledge questions, rely on your own knowledge.\"\n",
    "        ),\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"Who is Shyam?\"},\n",
    "]\n",
    "\n",
    "\n",
    "# --- Multi-tool (complex) example ---\n",
    "messages_multi_tool = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are a helpful assistant with access to tools. \"\n",
    "            \"Use the 'return_biodata' tool for any information requested about people. \"\n",
    "            \"For all other queries, use your general knowledge.\"\n",
    "        ),\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"What is the combined age of Ram and Shyam?\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e5cd0c-f371-43c9-a08e-63cea5ac46f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No Tool\n",
    "final_result, updated_messages = chat_with_tools(\n",
    "    smr_client,\n",
    "    endpoint_name,\n",
    "    inference_component_name,\n",
    "    messages_no_tool,\n",
    "    tools,\n",
    "    execute_tool,\n",
    ")\n",
    "\n",
    "print(json.dumps(final_result[\"choices\"][0][\"message\"][\"content\"], indent=2))\n",
    "\n",
    "\n",
    "# Single Tool\n",
    "final_result, updated_messages = chat_with_tools(\n",
    "    smr_client,\n",
    "    endpoint_name,\n",
    "    inference_component_name,\n",
    "    messages_single_tool,\n",
    "    tools,\n",
    "    execute_tool,\n",
    ")\n",
    "\n",
    "print(json.dumps(final_result[\"choices\"][0][\"message\"][\"content\"], indent=2))\n",
    "\n",
    "\n",
    "# Multi-Tool\n",
    "final_result, updated_messages = chat_with_tools(\n",
    "    smr_client,\n",
    "    endpoint_name,\n",
    "    inference_component_name,\n",
    "    messages_multi_tool,   # or messages_no_tool / messages_single_tool\n",
    "    tools,\n",
    "    execute_tool,\n",
    ")\n",
    "\n",
    "print(json.dumps(final_result[\"choices\"][0][\"message\"][\"content\"], indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
