{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0e8e9e0-6333-4ba2-a6b0-7e4ea0ee7447",
   "metadata": {},
   "source": [
    "## Utilizing SageMaker Inference Components to Host Multiple LLMs on a Single Endpoint\n",
    "\n",
    "In this example we utilize SageMaker Inference Components to host both a Falcon and Flan Model on a singular endpoint. Unlike traditional SageMaker Real-Time Endpoints we follow the flow of Endpoint Config -> Endpoint -> IC (1...n). In this case we create the endpoint and then add both a Falcon and Flan Model as their own ICs. ICs are similar to SageMaker Model objects we can define the model data and container information, the difference is we can enable AutoScaling as well at the IC level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c56594-8977-4f28-817c-0ad7db811438",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f00a55-ffe2-4daf-83bc-aeed5618557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sagemaker --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35dca9a-2a05-44c4-846e-578b8db36464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "\n",
    "#Setup\n",
    "client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime = boto3.client(service_name=\"sagemaker-runtime\")\n",
    "boto_session = boto3.session.Session()\n",
    "s3 = boto_session.resource('s3')\n",
    "region = boto_session.region_name\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "print(f\"Role ARN: {role}\")\n",
    "print(f\"Region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b05c6a-c518-4b5d-a910-2c7c827719c5",
   "metadata": {},
   "source": [
    "### Create Endpoint Config and Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f14d21-a6c0-4cac-8465-287af8e0616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# endpoint config name\n",
    "epc_name = \"ic-epc\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(f\"Endpoint Config Name: {epc_name}\")\n",
    "\n",
    "# Container Parameters, increase health check for LLMs: \n",
    "variant_name = \"AllTraffic\"\n",
    "instance_type = \"ml.g5.24xlarge\"\n",
    "model_data_download_timeout_in_seconds = 3600\n",
    "container_startup_health_check_timeout_in_seconds = 3600\n",
    "\n",
    "# Setting up managed AutoScaling\n",
    "initial_instance_count = 1\n",
    "max_instance_count = 2\n",
    "print(f\"Initial instance count: {initial_instance_count}\")\n",
    "print(f\"Max instance count: {max_instance_count}\")\n",
    "\n",
    "# Endpoint Config Creation\n",
    "endpoint_config_response = client.create_endpoint_config(\n",
    "    EndpointConfigName=epc_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": variant_name,\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": model_data_download_timeout_in_seconds,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": container_startup_health_check_timeout_in_seconds,\n",
    "            \"ManagedInstanceScaling\": {\n",
    "                \"Status\": \"ENABLED\",\n",
    "                \"MinInstanceCount\": initial_instance_count,\n",
    "                \"MaxInstanceCount\": max_instance_count,\n",
    "            },\n",
    "            # can set to least outstanding or random: https://aws.amazon.com/blogs/machine-learning/minimize-real-time-inference-latency-by-using-amazon-sagemaker-routing-strategies/\n",
    "            \"RoutingConfig\": {\"RoutingStrategy\": \"LEAST_OUTSTANDING_REQUESTS\"},\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Configuration Arn: \" + endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d0d051-91b5-441e-957a-d2d557d7fbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Endpoint Creation\n",
    "endpoint_name = \"ic-ep\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "create_endpoint_response = client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=epc_name,\n",
    ")\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c813a9d-1e88-4dc4-8498-a36df21f3501",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Monitor creation\n",
    "describe_endpoint_response = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "while describe_endpoint_response[\"EndpointStatus\"] == \"Creating\":\n",
    "    describe_endpoint_response = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    print(describe_endpoint_response[\"EndpointStatus\"])\n",
    "    time.sleep(15)\n",
    "print(describe_endpoint_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb9b74c-002c-4d44-9896-dc5ae4deaea9",
   "metadata": {},
   "source": [
    "### Inference Component Creation\n",
    "\n",
    "First we define the SageMaker model objects which have our container and model data info, the Inference Component directly takes this metadata from the SageMaker Model Object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00af58f2-0563-4e7f-883a-e0090cf12680",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "import json\n",
    "\n",
    "# utilizing huggingface TGI container\n",
    "image_uri = get_huggingface_llm_image_uri(\"huggingface\",version=\"1.1.0\")\n",
    "print(f\"TGI Image: {image_uri}\")\n",
    "\n",
    "# Flan T5 TGI Model\n",
    "flant5_model = {\"Image\": image_uri, \"Environment\": {\"HF_MODEL_ID\": \"google/flan-t5-xxl\"}}\n",
    "flant5_model_name = \"flant5-model\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(f\"Flan Model Name: {flant5_model_name}\")\n",
    "\n",
    "#note: falcon 7b takes just one GPU, sharding is not supported\n",
    "falcon7b_model = {\"Image\": image_uri, \"Environment\": {'HF_MODEL_ID':'tiiuae/falcon-7b'}}\n",
    "falcon7b_model_name = \"falcon7b-model\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(f\"Falcon Model Name: {falcon7b_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25fd3f1-3397-4b76-a056-31fee21656d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model object for flan t5\n",
    "create_flan_model_response = client.create_model(\n",
    "    ModelName=flant5_model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    Containers=[flant5_model],\n",
    ")\n",
    "print(\"Flan Model Arn: \" + create_flan_model_response[\"ModelArn\"])\n",
    "\n",
    "# create falcon model object\n",
    "create_falcon_model_response = client.create_model(\n",
    "    ModelName=falcon7b_model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    Containers=[falcon7b_model],\n",
    ")\n",
    "print(\"Falcon Model Arn: \" + create_falcon_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228383da-023e-4ab6-ab55-9a1cdc2110f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "flant5_ic_name = \"flant5-ic\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "variant_name = \"AllTraffic\"\n",
    "\n",
    "# flan inference component reaction\n",
    "create_flan_ic_response = client.create_inference_component(\n",
    "    InferenceComponentName=flant5_ic_name,\n",
    "    EndpointName=endpoint_name,\n",
    "    VariantName=variant_name,\n",
    "    Specification={\n",
    "        \"ModelName\": flant5_model_name,\n",
    "        \"ComputeResourceRequirements\": {\n",
    "            # enables tensor parallel via TGI, reserving 2 GPUs (g5.24xlarge has 4 GPUs)\n",
    "            \"NumberOfAcceleratorDevicesRequired\": 2,\n",
    "            \"NumberOfCpuCoresRequired\": 1,\n",
    "            \"MinMemoryRequiredInMb\": 1024,\n",
    "        },\n",
    "    },\n",
    "    # can setup autoscaling for copies\n",
    "    RuntimeConfig={\"CopyCount\": 1},\n",
    ")\n",
    "\n",
    "print(\"IC Flan Arn: \" + create_flan_ic_response[\"InferenceComponentArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d85b2d-b675-412d-901e-f7974f3d7376",
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_ic_flan_response = client.describe_inference_component(\n",
    "    InferenceComponentName=flant5_ic_name)\n",
    "\n",
    "while describe_ic_flan_response[\"InferenceComponentStatus\"] == \"Creating\":\n",
    "    describe_ic_flan_response = client.describe_inference_component(InferenceComponentName=flant5_ic_name)\n",
    "    print(describe_ic_flan_response[\"InferenceComponentStatus\"])\n",
    "    time.sleep(30)\n",
    "print(describe_ic_flan_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eae5d7-92eb-4ac4-bfa9-a3a4873e6ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "payload = \"What is the capitol of the United States?\"\n",
    "response = runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    InferenceComponentName=flant5_ic_name, #specify IC name\n",
    "    ContentType=\"application/json\",\n",
    "    Accept=\"application/json\",\n",
    "    Body=json.dumps(\n",
    "        {\n",
    "            \"inputs\": payload,\n",
    "            \"parameters\": {\n",
    "                \"early_stopping\": True,\n",
    "                \"length_penalty\": 2.0,\n",
    "                \"max_new_tokens\": 50,\n",
    "                \"temperature\": 1,\n",
    "                \"min_length\": 10,\n",
    "                \"no_repeat_ngram_size\": 3,\n",
    "                },\n",
    "        }\n",
    "    ),\n",
    ")\n",
    "result = json.loads(response[\"Body\"].read().decode())\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27883243-1792-471e-94a3-df4c6d71c096",
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_ic_name = \"falcon-ic\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "variant_name = \"AllTraffic\"\n",
    "\n",
    "create_falcon_ic_response = client.create_inference_component(\n",
    "    InferenceComponentName=falcon_ic_name,\n",
    "    EndpointName=endpoint_name,\n",
    "    VariantName=variant_name,\n",
    "    Specification={\n",
    "        \"ModelName\": falcon7b_model_name,\n",
    "        \"ComputeResourceRequirements\": {\n",
    "            # For falcon 7b only one GPU is needed: https://github.com/huggingface/text-generation-inference/issues/418#issuecomment-1579186709\n",
    "            \"NumberOfAcceleratorDevicesRequired\": 1,\n",
    "            \"NumberOfCpuCoresRequired\": 1,\n",
    "            \"MinMemoryRequiredInMb\": 1024,\n",
    "        },\n",
    "    },\n",
    "    # can setup autoscaling for copies\n",
    "    RuntimeConfig={\"CopyCount\": 1},\n",
    ")\n",
    "\n",
    "print(\"IC Falcon Arn: \" + create_falcon_ic_response[\"InferenceComponentArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ccae4b-f49a-44dd-9685-a1f62d4621f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_ic_falcon_response = client.describe_inference_component(\n",
    "    InferenceComponentName=falcon_ic_name)\n",
    "\n",
    "while describe_ic_falcon_response[\"InferenceComponentStatus\"] == \"Creating\":\n",
    "    describe_ic_falcon_response = client.describe_inference_component(InferenceComponentName=falcon_ic_name)\n",
    "    print(describe_ic_falcon_response[\"InferenceComponentStatus\"])\n",
    "    time.sleep(60)\n",
    "print(describe_ic_falcon_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9462d82f-1d7b-4245-b279-ba269f02ec32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "payload = \"What is the capitol of the United States?\"\n",
    "response = runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    InferenceComponentName=falcon_ic_name, #specify IC name\n",
    "    ContentType=\"application/json\",\n",
    "    Accept=\"application/json\",\n",
    "    Body=json.dumps(\n",
    "        {\n",
    "            \"inputs\": payload,\n",
    "            \"parameters\": {\n",
    "                \"early_stopping\": True,\n",
    "                \"length_penalty\": 2.0,\n",
    "                \"max_new_tokens\": 50,\n",
    "                \"temperature\": 1,\n",
    "                \"min_length\": 10,\n",
    "                \"no_repeat_ngram_size\": 3,\n",
    "                },\n",
    "        }\n",
    "    ),\n",
    ")\n",
    "result = json.loads(response[\"Body\"].read().decode())\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
