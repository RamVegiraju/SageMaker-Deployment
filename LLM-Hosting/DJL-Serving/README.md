# DJL Serving

Amazon's Large Model Inference (LMI) Containers are powered by the DJL Serving model server and allow for various optimizations such as tensor parallel, server side batching, and more that we can utilize specifically with LLMs.
