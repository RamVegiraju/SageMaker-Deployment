{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5b94c05-fc64-45d4-9427-e9dc21708c3c",
   "metadata": {},
   "source": [
    "# Mistral 7B TensorRT LLM Deployment\n",
    "In this notebook we will take a look at deploying Mistral 7B utilizing the Amazon Large Model Inference Container powered by [Nvidia TRT LLM](https://github.com/NVIDIA/TensorRT-LLM). Note that you can also utilize different engines/backends please refer [here](https://docs.djl.ai/docs/serving/serving/docs/lmi/tuning_guides/trtllm_tuning_guide.html) for the different options and how you can tune your configuration. In this case with the TensorRT container, rolling batch is enabled by default. We will use a g5.12xlarge to apply a tensor parallel of 4, you can tune this depending on your hardware, Mistral 7B can also be hosted on a g5.2xlarge if opting for a smaller instance type.\n",
    "\n",
    "### Table of Contents\n",
    "- Setup & Endpoint Creation\n",
    "- Load Testing & AutoScaling\n",
    "- Cleanup\n",
    "\n",
    "### Credits/References\n",
    "- [LMI Configuration Documentation](https://docs.djl.ai/docs/serving/serving/docs/lmi/configurations_large_model_inference_containers.html)\n",
    "- [DJL-Demo Samples](https://github.com/deepjavalibrary/djl-demo/tree/2a5152f578f5954b8b68acdee18eed4e2a75c81f/aws/sagemaker/large-model-inference/sample-llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcd5626-83fc-4e05-84e8-5d8dd365ef27",
   "metadata": {},
   "source": [
    "## Setup & Endpoint Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d176b908-8105-4707-8a82-860dac76cfc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import Model, image_uris, serializers, deserializers\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "account_id = sess.account_id() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dca6dd-885b-45fd-8d94-8a1daccda326",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile serving.properties\n",
    "engine=MPI\n",
    "option.tensor_parallel_degree=4\n",
    "option.model_id=mistralai/Mistral-7B-v0.1\n",
    "option.max_rolling_batch_size=16\n",
    "option.rolling_batch=auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d18b09f-911a-4d5a-a31d-06f62ae400e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "mkdir mymodel\n",
    "mv serving.properties mymodel/\n",
    "tar czvf mymodel.tar.gz mymodel/\n",
    "rm -rf mymodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9b5a7d-c9f6-4947-98fa-2f90f7885c6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# retreive TensorRT image\n",
    "image_uri = image_uris.retrieve(\n",
    "        framework=\"djl-tensorrtllm\",\n",
    "        region=sess.boto_session.region_name,\n",
    "        version=\"0.26.0\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b114a494-8909-4c17-8b18-e2fc1761118c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_code_prefix = \"large-model-lmi/code\"\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "code_artifact = sess.upload_data(\"mymodel.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {code_artifact}\")\n",
    "\n",
    "model = Model(image_uri=image_uri, model_data=code_artifact, role=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcaf8a7-e064-4a89-93ad-07fc264f8fdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this step can take around ~ 10 minutes for creation, the model artifacts are being pulled from HF Hub\n",
    "instance_type = \"ml.g5.12xlarge\"\n",
    "endpoint_name = sagemaker.utils.name_from_base(\"lmi-trt-mistral\")\n",
    "\n",
    "model.deploy(initial_instance_count=1,\n",
    "             instance_type=instance_type,\n",
    "             endpoint_name=endpoint_name\n",
    "            )\n",
    "\n",
    "# our requests and responses will be in json format so we specify the serializer and the deserializer\n",
    "predictor = sagemaker.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    serializer=serializers.JSONSerializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88fc047-456a-4eba-b0da-ee19f28f629a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# inference via sagemaker python SDK\n",
    "predictor.predict(\n",
    "    {\"inputs\": \"Who is Roger Federer?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb877f8-3a19-46ff-8059-47fd4c21f9d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# boto3 inference sample\n",
    "import json\n",
    "runtime_client = boto3.client('sagemaker-runtime')\n",
    "content_type = \"application/json\"\n",
    "payload = {\"inputs\": \"Who is Roger Federer?\"} #optionally add any parameters for your model\n",
    "\n",
    "# sample inference\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=content_type,\n",
    "    Body=json.dumps(payload))\n",
    "result = json.loads(response['Body'].read().decode())['generated_text']\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de6e06f-4ded-46c7-93a0-c457030a915c",
   "metadata": {},
   "source": [
    "## Load Testing & Enabling AutoScaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf699f92-705a-4e52-9698-5b2b8804ba8f",
   "metadata": {},
   "source": [
    "### Load Testing\n",
    "\n",
    "For Load Testing we'll use the open source Python framework: Locust. With Locust we can simulate concurrent users to generate traffic, for a deeper guide please refer to this [blog](https://aws.amazon.com/blogs/machine-learning/best-practices-for-load-testing-amazon-sagemaker-real-time-inference-endpoints/). For the test we have will two scripts we provide:\n",
    "\n",
    "- <b>distributed.sh</b>: Can control users and workers to increase traffic (TPS)\n",
    "- <b>locust_script.py</b>: Python script that defines task to test on, in this case it is our invoke_endpoint REST API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf522dc-124b-46c3-89c4-9820b5ef33c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install locust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e55fa0c-7e77-4bd8-ac90-a9cc84927603",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!which locust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de1c5cf-ce94-4aaf-a35a-0aac4f1b2c83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat distributed.sh #adjust users and workers to increase traffic, users are a multiple of the workers in locust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0fdc65-d442-4bdb-95f9-12e9fb322a84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash -s \"$endpoint_name\"\n",
    "./distributed.sh $1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45809cfc-16c1-4eb5-a82d-d3cff96a74d4",
   "metadata": {},
   "source": [
    "We can take a look at the Locust generated metrics to understand our end to end latency. We also take a look at the built-in CloudWatch metrics on the SageMaker UI to further understand our hardware utilization and invocation metrics (container latency, etc). We look at the maximum GPU Utilization (400% available with 4 GPUs) and the Invocations Per Minute generated by the Locust test. Understanding these metrics will help us provide a prescriptive AutoScaling policy. To understand further about CW Metrics integrated with SageMaker Real-Time Inference please refer to the following [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html).\n",
    "\n",
    "<div style=\"display: flex;\">\n",
    "    <img src=\"images/invocations.png\" alt=\"Invocations\" style=\"width: 50%;\">\n",
    "    <img src=\"images/hardware-utilization.png\" alt=\"Hardware\" style=\"width: 50%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c3c3f4-e842-4994-a2a7-462581158d4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "locust_data = pd.read_csv('results_stats.csv')\n",
    "for index, row in locust_data.head(n=2).iterrows():\n",
    "     print(index, row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dae2214-4cbd-4c32-a0eb-22f35161001f",
   "metadata": {},
   "source": [
    "### AutoScaling\n",
    "\n",
    "You can also enable AutoScaling at an endpoint level on Amazon SageMaker. Before getting to AutoScaling it is recommended that you load test a single instance behind the endpoint, this will help you determine how much you are getting out of a singular instance. One this has been derived and the appropriate instance is chosen you can determine your scaling policy with Managed AutoScaling. For a deeper dive blog into AutoScaling with SageMaker Inference, refer to this [blog](https://towardsdatascience.com/autoscaling-sagemaker-real-time-endpoints-b1b6e6731c59). <b>Please also ensure that you have the necessary limits request for the scaling you set for your endpoint. In this case 4 g5.12xlarge instances are needed.</b>\n",
    "\n",
    "We will work with setting up a Managed AutoScaling policy via Application AutoScaling using the Boto3 SDK. We should see this reflected in the SageMaker Endpoint UI as well:\n",
    "\n",
    "![autoscaling](images/autoscaling-setup.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b757f2-7e7a-46dc-a0fb-d24eeb1e6b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoScaling client\n",
    "asg = boto3.client('application-autoscaling')\n",
    "\n",
    "# Resource type is variant and the unique identifier is the resource ID.\n",
    "# default VariantName is AllTraffic adjust for your use-case\n",
    "resource_id=f\"endpoint/{predictor.endpoint_name}/variant/AllTraffic\"\n",
    "\n",
    "# scaling configuration\n",
    "response = asg.register_scalable_target(\n",
    "    ServiceNamespace='sagemaker', #\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension='sagemaker:variant:DesiredInstanceCount', \n",
    "    MinCapacity=1,\n",
    "    MaxCapacity=4\n",
    ")\n",
    "\n",
    "#Target Scaling\n",
    "response = asg.put_scaling_policy(\n",
    "    PolicyName=f'Request-ScalingPolicy-{endpoint_name}',\n",
    "    ServiceNamespace='sagemaker',\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension='sagemaker:variant:DesiredInstanceCount',\n",
    "    PolicyType='TargetTrackingScaling',\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        'TargetValue': 5.0, # Threshold, 5 requests in a minute\n",
    "        'PredefinedMetricSpecification': {\n",
    "            'PredefinedMetricType': 'SageMakerVariantInvocationsPerInstance',\n",
    "        },\n",
    "        'ScaleInCooldown': 300, # duration until scale in\n",
    "        'ScaleOutCooldown': 60 # duration between scale out\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ec0dde-9ff2-416c-b994-30aae5b51af4",
   "metadata": {},
   "source": [
    "Let's send requests for 15 minutes to see our hardware scale up as we defined. We should see our endpoint updating to four instances.\n",
    "\n",
    "![Updating Endpoint](images/updating-endpoint.png)\n",
    "![Updated Endpoint](images/updated-endpoint.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c566c81-37ec-4d9f-b326-2ba815cbc47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "request_duration = 60 * 15 # 15 minutes\n",
    "end_time = time.time() + request_duration\n",
    "print(f\"test will run for {request_duration} seconds\")\n",
    "while time.time() < end_time:\n",
    "    response = runtime_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType=content_type,\n",
    "        Body=json.dumps(payload))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618d617f-db3f-471c-863b-9ecc326ae333",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecaecd2-9b3d-4b41-b7d0-ea4fcf2f05f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
