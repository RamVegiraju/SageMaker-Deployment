{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1abf811-eefb-407d-81e0-d61e782d4e98",
   "metadata": {},
   "source": [
    "## SageMaker Asynchronous Inference With LLMs\n",
    "\n",
    "In this example we'll take a look at using the Asynchronous Inference Hosting Option to host the [Flan LLM](https://huggingface.co/google/flan-t5-xxl).\n",
    "\n",
    "- Async Inference Launch Code Sample/Reference: https://github.com/aws/amazon-sagemaker-examples/blob/main/async-inference/Async-Inference-Walkthrough-SageMaker-Python-SDK.ipynb\n",
    "- This example uses some Reference Code from my colleague Abhi Sodhani's original example: https://github.com/abhisodhani/sagemaker-falcon-asyn-hosting/blob/main/notebook/huggingface-large-model-aync-inference.ipynb. We will extend this example for our own LLM use-case/model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f959de9-cfa7-44d4-8d71-7d2a0220d215",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b1cc0e-ce67-4295-9949-5898ca1f8094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "\n",
    "try:\n",
    "\trole = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "\tiam = boto3.client('iam')\n",
    "\trole = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "print(f\"Role: {role}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6deeb0-4c70-4d06-abad-38d5e9285872",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "bucket_prefix = \"async-llm-output\"\n",
    "async_output_path = f\"s3://{default_bucket}/{bucket_prefix}/output\"\n",
    "print(f\"My model inference outputs will be stored at this S3 path: {async_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02964c3e-10cc-41e1-8851-a35e20261e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.async_inference.async_inference_config import AsyncInferenceConfig\n",
    "\n",
    "async_config = AsyncInferenceConfig(\n",
    "    output_path=async_output_path,\n",
    "    max_concurrent_invocations_per_instance=10,\n",
    "    # Optionally specify Amazon SNS topics\n",
    "    # notification_config = {\n",
    "    # \"SuccessTopic\": \"arn:aws:sns:<aws-region>:<account-id>:<topic-name>\",\n",
    "    # \"ErrorTopic\": \"arn:aws:sns:<aws-region>:<account-id>:<topic-name>\",\n",
    "    # }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38921ec1-7286-4b65-8b12-578f0f60ad34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directly grab huggingface hub deploy code and add async config\n",
    "hub = {\n",
    "\t'HF_MODEL_ID':'google/flan-t5-xxl',\n",
    "\t'SM_NUM_GPUS': json.dumps(4)\n",
    "}\n",
    "\n",
    "huggingface_model = HuggingFaceModel(\n",
    "\timage_uri=get_huggingface_llm_image_uri(\"huggingface\",version=\"1.1.0\"),\n",
    "\tenv=hub,\n",
    "\trole=role, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868cd3c0-c163-4ef6-a35b-c6c218fb85d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "\tinitial_instance_count=1,\n",
    "\tinstance_type=\"ml.g5.12xlarge\",\n",
    "\tcontainer_startup_health_check_timeout=300,\n",
    "    async_inference_config=async_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7949a2f7-4b4e-48cf-97ce-5f1600264d1d",
   "metadata": {},
   "source": [
    "### Sample Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386afcff-33cc-46cd-a5ea-85d29a454695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# singular invocation\n",
    "\n",
    "payload = \"What is the capitol of the United States?\"\n",
    "input_data = {\n",
    "    \"inputs\": payload,\n",
    "    \"parameters\": {\n",
    "        \"early_stopping\": True,\n",
    "        \"length_penalty\": 2.0,\n",
    "        \"max_new_tokens\": 50,\n",
    "        \"temperature\": 1,\n",
    "        \"min_length\": 10,\n",
    "        \"no_repeat_ngram_size\": 3,\n",
    "        },\n",
    "}\n",
    "predictor.predict(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaa6a86-127d-40d2-a64b-9a8590a93b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "output_directory = 'inputs'\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "for i in range(1, 20):\n",
    "    json_data = [input_data.copy()]\n",
    "\n",
    "    file_path = os.path.join(output_directory, f'input_{i}.jsonl')\n",
    "    with open(file_path, 'w') as input_file:\n",
    "        for line in json_data:\n",
    "            json.dump(line, input_file)\n",
    "            input_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15b567c-231d-4122-a052-c97f8d19c25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_prefix_input = \"input-data-llm\"\n",
    "input_location = \"inputs.jsonl\"\n",
    "\n",
    "def upload_file(input_location):\n",
    "    prefix = f\"{bucket_prefix}/input\"\n",
    "    return sagemaker_session.upload_data(\n",
    "        input_location,\n",
    "        bucket=default_bucket,\n",
    "        key_prefix=prefix,\n",
    "        extra_args={\"ContentType\": \"application/json\"} #make sure to specify\n",
    "    )\n",
    "\n",
    "sample_data_point = upload_file(\"inputs/input_1.jsonl\")\n",
    "print(f\"Sample data point uploaded: {sample_data_point}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76791c5a-ad05-48ac-b4db-d0c4d24f54d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "runtime = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "response = runtime.invoke_endpoint_async(\n",
    "    EndpointName=predictor.endpoint_name,\n",
    "    InputLocation=sample_data_point,\n",
    "    Accept='application/json',\n",
    "    ContentType=\"application/json\"\n",
    ")\n",
    "\n",
    "output_location = response[\"OutputLocation\"]\n",
    "print(f\"OutputLocation: {output_location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559192e3-2a85-4004-8147-64ecc70b9c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib, time\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# function reference/credit: https://github.com/aws/amazon-sagemaker-examples/blob/main/async-inference/Async-Inference-Walkthrough-SageMaker-Python-SDK.ipynb\n",
    "def get_output(output_location):\n",
    "    output_url = urllib.parse.urlparse(output_location)\n",
    "    bucket = output_url.netloc\n",
    "    key = output_url.path[1:]\n",
    "    while True:\n",
    "        try:\n",
    "            return sagemaker_session.read_s3_file(bucket=output_url.netloc, key_prefix=output_url.path[1:])\n",
    "        except ClientError as e:\n",
    "            if e.response[\"Error\"][\"Code\"] == \"NoSuchKey\":\n",
    "                print(\"waiting for output...\")\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb04a33-a387-4051-b5e8-8263ae19eaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = get_output(output_location)\n",
    "print(f\"Output: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5cfd09-0919-4e29-8eab-b3913465f63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferences = []\n",
    "for i in range(1,20):\n",
    "    input_file = f\"inputs/input_{i}.jsonl\"\n",
    "    input_file_s3_location = upload_file(input_file)\n",
    "    print(f\"Invoking Endpoint with {input_file}\")\n",
    "    async_response = predictor.predict_async(input_path=input_file_s3_location)\n",
    "    output_location = async_response.output_path\n",
    "    print(output_location)\n",
    "    inferences += [(input_file, output_location)]\n",
    "    time.sleep(0.5)\n",
    "\n",
    "for input_file, output_location in inferences:\n",
    "    output = get_output(output_location)\n",
    "    print(f\"Input File: {input_file}, Output: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4a2454-2be9-47b0-a04e-7dd5f6ff7d7d",
   "metadata": {},
   "source": [
    "### AutoScaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16fe5c5-8df4-410a-8807-a2e383847f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client(\n",
    "    \"application-autoscaling\"\n",
    ")  # Common class representing Application Auto Scaling for SageMaker amongst other services\n",
    "\n",
    "resource_id = (\n",
    "    \"endpoint/\" + predictor.endpoint_name + \"/variant/\" + \"AllTraffic\"\n",
    ")  # This is the format in which application autoscaling references the endpoint\n",
    "\n",
    "# Configure Autoscaling on asynchronous endpoint down to zero instances\n",
    "response = client.register_scalable_target(\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    MinCapacity=0,\n",
    "    MaxCapacity=5,\n",
    ")\n",
    "\n",
    "response = client.put_scaling_policy(\n",
    "    PolicyName=\"Invocations-ScalingPolicy\",\n",
    "    ServiceNamespace=\"sagemaker\",  # The namespace of the AWS service that provides the resource.\n",
    "    ResourceId=resource_id,  # Endpoint name\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",  # SageMaker supports only Instance Count\n",
    "    PolicyType=\"TargetTrackingScaling\",  # 'StepScaling'|'TargetTrackingScaling'\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        \"TargetValue\": 5.0,  # The target value for the metric. - here the metric is - SageMakerVariantInvocationsPerInstance\n",
    "        \"CustomizedMetricSpecification\": {\n",
    "            \"MetricName\": \"ApproximateBacklogSizePerInstance\",\n",
    "            \"Namespace\": \"AWS/SageMaker\",\n",
    "            \"Dimensions\": [{\"Name\": \"EndpointName\", \"Value\": predictor.endpoint_name}],\n",
    "            \"Statistic\": \"Average\",\n",
    "        },\n",
    "        \"ScaleInCooldown\": 600,  # The cooldown period helps you prevent your Auto Scaling group from launching or terminating\n",
    "        # additional instances before the effects of previous activities are visible.\n",
    "        # You can configure the length of time based on your instance startup time or other application needs.\n",
    "        # ScaleInCooldown - The amount of time, in seconds, after a scale in activity completes before another scale in activity can start.\n",
    "        \"ScaleOutCooldown\": 100  # ScaleOutCooldown - The amount of time, in seconds, after a scale out activity completes before another scale out activity can start.\n",
    "        # 'DisableScaleIn': True|False - ndicates whether scale in by the target tracking policy is disabled.\n",
    "        # If the value is true , scale in is disabled and the target tracking policy won't remove capacity from the scalable resource.\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582c8338-e4b5-4d88-bae2-c8560b0cbc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_duration = 60 * 15 # 15 minutes\n",
    "end_time = time.time() + request_duration\n",
    "print(f\"test will run for {request_duration} seconds\")\n",
    "while time.time() < end_time:\n",
    "    predictor.predict(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1e1085-a424-4e11-8fb8-4aff7796c760",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client = boto3.client(service_name='sagemaker')\n",
    "response = sm_client.describe_endpoint(EndpointName=predictorendpoint_name)\n",
    "status = response['EndpointStatus']\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "\n",
    "while status=='Updating':\n",
    "    time.sleep(60)\n",
    "    response = sm_client.describe_endpoint(EndpointName=predictor.endpoint_name)\n",
    "    status = response['EndpointStatus']\n",
    "    instance_count = response['ProductionVariants'][0]['CurrentInstanceCount']\n",
    "    print(f\"Status: {status}\")\n",
    "    print(f\"Current Instance count: {instance_count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
