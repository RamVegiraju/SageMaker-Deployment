{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57b14eee",
   "metadata": {},
   "source": [
    "# TF MME W/ Boto3\n",
    "\n",
    "<b>Setting</b>: conda_tf2_p38 & Classic Notebook Instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a07ce7d",
   "metadata": {},
   "source": [
    "## Local Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e4fe544d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting local_tf.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile local_tf.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "#ANN Model\n",
    "def createModel():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, input_shape=(13,), activation='relu'))\n",
    "    model.add(Dense(28, activation='relu'))\n",
    "    model.add(Dense(13, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "#Load data\n",
    "boston = datasets.load_boston()\n",
    "df = pd.DataFrame(boston.data, columns = boston.feature_names)\n",
    "df['MEDV'] = boston.target \n",
    "\n",
    "#Split Model\n",
    "X = df.drop(['MEDV'], axis = 1) \n",
    "y = df['MEDV']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state = 42)\n",
    "\n",
    "#minimum epochs 15\n",
    "model = createModel()\n",
    "monitor_val_acc = EarlyStopping(monitor = 'val_loss', patience=15)\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), callbacks=[monitor_val_acc], epochs=5)\n",
    "model.save('0000001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ec900efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "Epoch 1/5\n",
      "13/13 [==============================] - 1s 14ms/step - loss: 3494.7942 - val_loss: 447.2986\n",
      "Epoch 2/5\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 270.0934 - val_loss: 406.3280\n",
      "Epoch 3/5\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 284.1220 - val_loss: 130.1467\n",
      "Epoch 4/5\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 104.4156 - val_loss: 132.8439\n",
      "Epoch 5/5\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 95.1160 - val_loss: 93.2577\n"
     ]
    }
   ],
   "source": [
    "!python local_tf.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ec6177",
   "metadata": {},
   "source": [
    "## Package for SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ef589a",
   "metadata": {},
   "source": [
    "### Script for TF Serving to control pre/post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0dd000c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile inference.py\n",
    "\n",
    "import json\n",
    "\n",
    "def input_handler(data, context):\n",
    "    \"\"\" Pre-process request input before it is sent to TensorFlow Serving REST API\n",
    "    Args:\n",
    "        data (obj): the request data, in format of dict or string\n",
    "        context (Context): an object containing request and configuration details\n",
    "    Returns:\n",
    "        (dict): a JSON-serializable dict that contains request body and headers\n",
    "    \"\"\"\n",
    "    if context.request_content_type == 'application/json':\n",
    "        # pass through json (assumes it's correctly formed)\n",
    "        d = data.read().decode('utf-8')\n",
    "        print(\"------------\")\n",
    "        print(d)\n",
    "        print(type(d))\n",
    "        print(\"------------\")\n",
    "        return d if len(d) else ''\n",
    "\n",
    "    if context.request_content_type == 'text/csv':\n",
    "        # very simple csv handler\n",
    "        return json.dumps({\n",
    "            'instances': [float(x) for x in data.read().decode('utf-8').split(',')]\n",
    "        })\n",
    "\n",
    "    raise ValueError('{{\"error\": \"unsupported content type {}\"}}'.format(\n",
    "        context.request_content_type or \"unknown\"))\n",
    "\n",
    "\n",
    "def output_handler(data, context):\n",
    "    \"\"\"Post-process TensorFlow Serving output before it is returned to the client.\n",
    "    Args:\n",
    "        data (obj): the TensorFlow serving response\n",
    "        context (Context): an object containing request and configuration details\n",
    "    Returns:\n",
    "        (bytes, string): data to return to client, response content type\n",
    "    \"\"\"\n",
    "    if data.status_code != 200:\n",
    "        raise ValueError(data.content.decode('utf-8'))\n",
    "\n",
    "    response_content_type = context.accept_header\n",
    "    print(\"-------\")\n",
    "    print(data)\n",
    "    print(type(data))\n",
    "    print(\"-----\")\n",
    "    prediction = data.content\n",
    "    return prediction, response_content_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "77139c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-east-1\n"
     ]
    }
   ],
   "source": [
    "#Setup\n",
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import joblib\n",
    "import pickle\n",
    "import tarfile\n",
    "import sagemaker\n",
    "from sagemaker.estimator import Estimator\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "import subprocess\n",
    "import shlex\n",
    "\n",
    "client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime = boto3.client(service_name=\"sagemaker-runtime\")\n",
    "boto_session = boto3.session.Session()\n",
    "s3 = boto_session.resource('s3')\n",
    "region = boto_session.region_name\n",
    "print(region)\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "#role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c2bbed51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy inference.py into a code file for sm to recognize\n",
    "createDirectory = \"mkdir code\"\n",
    "copyInference = \"cp inference.py code\"\n",
    "p1 = subprocess.call(createDirectory, shell=True)\n",
    "p2 = subprocess.call(copyInference, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b9123cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tar file with model artifacts\n",
    "bashCommand = \"tar -cvpzf model.tar.gz ./0000001\"\n",
    "process = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)\n",
    "output, error = process.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c216bfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tar ball for inference code, inference.py is consistent across all models in an MME\n",
    "bashCommand = \"tar -cvpzf source.tar.gz inference.py\"\n",
    "process = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)\n",
    "output, error = process.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "10b43991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-us-east-1-474422712127\n"
     ]
    }
   ],
   "source": [
    "#Bucket for model artifacts\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "print(default_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb926dd",
   "metadata": {},
   "source": [
    "### Make Copies Of Model Data In S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "57b13ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "for i in range(0,5):\n",
    "    with open(\"model.tar.gz\", \"rb\") as f:\n",
    "        s3.upload_fileobj(f, default_bucket, \"mme-tensorflow/tf-{}.tar.gz\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7295302c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-04 19:30:11      24502 tf-0.tar.gz\n",
      "2022-08-04 19:30:11      24502 tf-1.tar.gz\n",
      "2022-08-04 19:30:11      24502 tf-2.tar.gz\n",
      "2022-08-04 19:30:11      24502 tf-3.tar.gz\n",
      "2022-08-04 19:30:11      24502 tf-4.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://{default_bucket}/mme-tensorflow/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "86beda5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./source.tar.gz to s3://sagemaker-us-east-1-474422712127/mme-tf-inf-script/source.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp source.tar.gz s3://{default_bucket}/mme-tf-inf-script/source.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d19c9d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-04 19:30:22        792 source.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://{default_bucket}/mme-tf-inf-script/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "74dd0398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-474422712127/mme-tf-inf-script/'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_dir = 's3://{}/mme-tf-inf-script/'.format(default_bucket)\n",
    "source_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e80db9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-04 19:30:22        792 source.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls {source_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a193a49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_url = 's3://{}/mme-tensorflow/'.format(default_bucket) ## MODEL S3 URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2ec9d6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-04 19:30:11      24502 tf-0.tar.gz\n",
      "2022-08-04 19:30:11      24502 tf-1.tar.gz\n",
      "2022-08-04 19:30:11      24502 tf-2.tar.gz\n",
      "2022-08-04 19:30:11      24502 tf-3.tar.gz\n",
      "2022-08-04 19:30:11      24502 tf-4.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls {model_url}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3117e8",
   "metadata": {},
   "source": [
    "### Retrieve Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4e2d247c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.3.0-cpu'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve tf image\n",
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"tensorflow\",\n",
    "    region=\"us-east-1\",\n",
    "    version=\"2.3.0\",\n",
    "    py_version=\"py3\",\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    image_scope=\"inference\"\n",
    ")\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae23b737",
   "metadata": {},
   "source": [
    "### SageMaker Model Entity Creation\n",
    "\n",
    "API Call: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "57b21501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: mme-tf2022-08-04-19-30-49\n",
      "Model data Url: s3://sagemaker-us-east-1-474422712127/mme-tensorflow/\n",
      "Model Arn: arn:aws:sagemaker:us-east-1:474422712127:model/mme-tf2022-08-04-19-30-49\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "model_name = 'mme-tf' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "print('Model name: ' + model_name)\n",
    "print('Model data Url: ' + model_url)\n",
    "\n",
    "create_model_response = client.create_model(\n",
    "    ModelName=model_name,\n",
    "    Containers=[\n",
    "        {\n",
    "            \"Image\": image_uri,\n",
    "            \"Mode\": \"MultiModel\",\n",
    "            \"ModelDataUrl\": model_url,\n",
    "            \"Environment\": {'SAGEMAKER_SUBMIT_DIRECTORY': source_dir,\n",
    "                           'SAGEMAKER_PROGRAM': 'inference.py'} \n",
    "        }\n",
    "    ],\n",
    "    ExecutionRoleArn=role,\n",
    ")\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e2a1ac",
   "metadata": {},
   "source": [
    "### Endpoint Config Creation\n",
    "\n",
    "API Call: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_endpoint_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e6b2e035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Configuration Arn: arn:aws:sagemaker:us-east-1:474422712127:endpoint-config/mme-tf2022-08-04-19-30-53\n"
     ]
    }
   ],
   "source": [
    "#Step 2: EPC Creation\n",
    "tf_epc_name = \"mme-tf\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "endpoint_config_response = client.create_endpoint_config(\n",
    "    EndpointConfigName=tf_epc_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"tfvariant\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InstanceType\": \"ml.c5.large\",\n",
    "            \"InitialInstanceCount\": 1\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(\"Endpoint Configuration Arn: \" + endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c092144",
   "metadata": {},
   "source": [
    "### Endpoint Creation\n",
    "\n",
    "API Call: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c9cb2755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Arn: arn:aws:sagemaker:us-east-1:474422712127:endpoint/mme-tf2022-08-04-19-30-56\n"
     ]
    }
   ],
   "source": [
    "#Step 3: EP Creation\n",
    "endpoint_name = \"mme-tf\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "create_endpoint_response = client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=tf_epc_name,\n",
    ")\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ce2bab5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "InService\n",
      "{'EndpointName': 'mme-tf2022-08-04-19-30-56', 'EndpointArn': 'arn:aws:sagemaker:us-east-1:474422712127:endpoint/mme-tf2022-08-04-19-30-56', 'EndpointConfigName': 'mme-tf2022-08-04-19-30-53', 'ProductionVariants': [{'VariantName': 'tfvariant', 'DeployedImages': [{'SpecifiedImage': '763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.3.0-cpu', 'ResolvedImage': '763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference@sha256:91ebb7428846c5f7b515d5d9b8389a14c73d0c5d02657f4a6413592124333278', 'ResolutionTime': datetime.datetime(2022, 8, 4, 19, 31, 3, 1000, tzinfo=tzlocal())}], 'CurrentWeight': 1.0, 'DesiredWeight': 1.0, 'CurrentInstanceCount': 1, 'DesiredInstanceCount': 1}], 'EndpointStatus': 'InService', 'CreationTime': datetime.datetime(2022, 8, 4, 19, 30, 56, 679000, tzinfo=tzlocal()), 'LastModifiedTime': datetime.datetime(2022, 8, 4, 19, 37, 58, 254000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': '5d5b2e40-1077-463a-a616-90ede7f11b2c', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '5d5b2e40-1077-463a-a616-90ede7f11b2c', 'content-type': 'application/x-amz-json-1.1', 'content-length': '729', 'date': 'Thu, 04 Aug 2022 19:38:00 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "#Monitor creation\n",
    "describe_endpoint_response = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "while describe_endpoint_response[\"EndpointStatus\"] == \"Creating\":\n",
    "    describe_endpoint_response = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    print(describe_endpoint_response[\"EndpointStatus\"])\n",
    "    time.sleep(15)\n",
    "print(describe_endpoint_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd64f9e3",
   "metadata": {},
   "source": [
    "### Invoke MME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2c5ce3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'inputs': [[0.00632, 18.0, 2.31, 0.0, 0.538, 6.575, 65.2, 4.09, 1.0, 296.0, 15.3, 396.9, 4.98]]}\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "import pandas as pd\n",
    "\n",
    "test = df[:1]\n",
    "testX = test.drop(\"TARGET\", axis=1)\n",
    "testX = testX[:1].values.tolist()\n",
    "sampInput = {\"inputs\": testX}\n",
    "print(sampInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e3d5bc0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[27.3511848]]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runtime_sm_client = boto3.client(service_name='sagemaker-runtime')\n",
    "jsons = JSONSerializer()\n",
    "content_type = \"application/json\"\n",
    "payload = jsons.serialize(sampInput)\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=payload,\n",
    "        ContentType=content_type,\n",
    "        TargetModel = \"tf-1.tar.gz\")\n",
    "result = json.loads(response['Body'].read().decode())['outputs']\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1e08c1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-0.tar.gz\n",
      "[[27.3511848]]\n",
      "tf-1.tar.gz\n",
      "[[27.3511848]]\n",
      "tf-2.tar.gz\n",
      "[[27.3511848]]\n",
      "tf-3.tar.gz\n",
      "[[27.3511848]]\n",
      "tf-4.tar.gz\n",
      "[[27.3511848]]\n"
     ]
    }
   ],
   "source": [
    "for i in range (0,5):\n",
    "    target_model = \"tf-{}.tar.gz\".format(i)\n",
    "    print(target_model)\n",
    "    response = runtime_sm_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=payload,\n",
    "        ContentType=content_type,\n",
    "        TargetModel = target_model)\n",
    "    print(json.loads(response['Body'].read().decode())['outputs'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p38",
   "language": "python",
   "name": "conda_tensorflow2_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
