{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4ec87e9-27f3-4cdd-afc0-96848f7e6fdc",
   "metadata": {},
   "source": [
    "# MME GPU SageMaker Real-Time Inference\n",
    "\n",
    "In this example we take a BERT NLP model and make hundreds of copies of it and run inference with SageMaker Multi-Model Endpoints. We run this notebook on a conda_pytorch_p310 kernel on a classic SageMaker Notebook Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43feca72-fc10-4a96-960b-0712ac2baffd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1faf6bd-9ff4-449e-9e15-1f0a76aa2fd9",
   "metadata": {},
   "source": [
    "## Local BERT Inference & Model Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df289459-e026-455b-9b54-f4957155246d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load bert model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "model = BertModel.from_pretrained(model_name, torchscript = True)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Sample Input\n",
    "text = \"I am super happy right now to be trying out BERT.\"\n",
    "\n",
    "# Tokenize sample text\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6017b0f1-137e-4cd4-8214-e0c0c7aa3b63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# jit trace model\n",
    "traced_model = torch.jit.trace(model, (inputs[\"input_ids\"], inputs[\"attention_mask\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57096a95-6eb8-4b08-9187-48b73020b987",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save traced model\n",
    "torch.jit.save(traced_model, \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b88cc43-8b85-4389-8741-f80831d6151d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sample inference with loaded model\n",
    "loaded_model = torch.jit.load(\"model.pt\")\n",
    "res = loaded_model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fadd4e-c8ba-45f9-b39f-b89fd1d803d8",
   "metadata": {},
   "source": [
    "### Model Configuration\n",
    "\n",
    "We can understand the input and output shapes by observing the model configuration from the transformers library. This will help us shape our config.pbtxt file for our Triton Inference Server configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f4025c-1452-4045-9064-2262e7882c74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BertConfig\n",
    "bert_config = BertConfig.from_pretrained(model_name)\n",
    "max_sequence_length = bert_config.max_position_embeddings\n",
    "output_shape = bert_config.hidden_size\n",
    "print(f\"Maximum Input Sequence Length: {max_sequence_length}\")\n",
    "print(f\"Output Shape: {output_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21aadd2d-e94d-4691-a7ae-257fedf07df8",
   "metadata": {},
   "source": [
    "## Local Triton Setup\n",
    "\n",
    "We want to check Triton Inference Server and ensure we can run local inference with the container beforehand, this will help us quickly debug any issues rather than discovering post SageMaker Endpoint creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5882f875-5161-4776-8fc0-7527fd92212e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    encoded_text = tokenizer(text, padding=\"max_length\", max_length=512, truncation=True)\n",
    "    return encoded_text[\"input_ids\"], encoded_text[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9830eee8-305d-4b84-8d83-c07d84ea4ea0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_text = \"\"\"\n",
    "                We are testing some sample text for BERT.\n",
    "                This is a test with SageMaker MME GPU.\n",
    "              \"\"\"\n",
    "\n",
    "input_ids, attention_mask = tokenize_text(sample_text)\n",
    "\n",
    "# for shape refer to configuration code above, our max sequence length for BERT is 512\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        {\"name\": \"input_ids\", \"shape\": [1, 512], \"datatype\": \"INT32\", \"data\": input_ids},\n",
    "        {\"name\": \"attention_mask\", \"shape\": [1, 512], \"datatype\": \"INT32\", \"data\": attention_mask},\n",
    "    ]\n",
    "}\n",
    "\n",
    "#payload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dada72b-2260-420f-ac66-828ef704de20",
   "metadata": {},
   "source": [
    "### Create Proper Directory Structure for Triton\n",
    "\n",
    "PyTorch models are expected to be in following folder format for Triton:\n",
    "\n",
    "- bert_model\n",
    "    - 1 (model_version)\n",
    "        - model.pt\n",
    "        - model.py (optionally add)\n",
    "    - config.pbtxt\n",
    "    \n",
    "We can create our config file and move the serialized model artifact to where necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd37ce5-9897-4b66-b6dc-38c03dc04444",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile config.pbtxt\n",
    "name: \"bert_model\"\n",
    "platform: \"pytorch_libtorch\"\n",
    "\n",
    "input [\n",
    "  {\n",
    "    name: \"input_ids\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [1, 512]\n",
    "  },\n",
    "  {\n",
    "    name: \"attention_mask\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [1, 512]\n",
    "  }\n",
    "]\n",
    "\n",
    "output [\n",
    "  {\n",
    "    name: \"OUTPUT\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [512, 768]\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fac0d70-e178-45ae-942d-e311d0c930c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "mkdir bert_model\n",
    "mv config.pbtxt model.pt bert_model\n",
    "cd bert_model\n",
    "mkdir 1\n",
    "mv model.pt 1/\n",
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51839e70-f963-4c91-9de5-dee2b3dacbdf",
   "metadata": {},
   "source": [
    "### Start Triton Container\n",
    "\n",
    "Make sure to start the Server with the following Docker command before running the local Inference cells.\n",
    "\n",
    "```\n",
    "docker run --gpus all --rm -p 8000:8000 -p 8001:8001 -p 8002:8002 -v /home/ec2-user/SageMaker:/models nvcr.io/nvidia/tritonserver:23.08-py3 tritonserver --model-repository=/models --exit-on-error=false --log-verbose=1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e79bbba-69e1-4d8c-b80a-3ff52946f151",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce4a8ef-27f0-4a74-9f20-596d0a8525de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify the model name and version\n",
    "model_name = \"bert_model\" #specified in config.pbtxt\n",
    "model_version = \"1\"\n",
    "\n",
    "# Set the inference URL based on the Triton server's address\n",
    "url = f\"http://localhost:8000/v2/models/{model_name}/versions/{model_version}/infer\"\n",
    "\n",
    "# sample invoke\n",
    "output = requests.post(url, data=json.dumps(payload))\n",
    "res = output.json()\n",
    "#print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba588c2-0aa5-434c-b9e6-a5e3c93629b5",
   "metadata": {},
   "source": [
    "## SageMaker MME GPU\n",
    "\n",
    "First we create our model tarball which we will make copies of to create our MME GPU based endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76313f9a-88ee-498d-94ac-ea95296a636c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import json\n",
    "sess = boto3.Session()\n",
    "sm = sess.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.Session(boto_session=sess)\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "s3_model_prefix = \"triton-bert\"\n",
    "\n",
    "client = boto3.client(\"sagemaker\", region_name=region)\n",
    "runtime_client = boto3.client(\"sagemaker-runtime\")\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "account_id_map = {\n",
    "    \"us-east-1\": \"785573368785\",\n",
    "    \"us-east-2\": \"007439368137\",\n",
    "    \"us-west-1\": \"710691900526\",\n",
    "    \"us-west-2\": \"301217895009\",\n",
    "    \"eu-west-1\": \"802834080501\",\n",
    "    \"eu-west-2\": \"205493899709\",\n",
    "    \"eu-west-3\": \"254080097072\",\n",
    "    \"eu-north-1\": \"601324751636\",\n",
    "    \"eu-south-1\": \"966458181534\",\n",
    "    \"eu-central-1\": \"746233611703\",\n",
    "    \"ap-east-1\": \"110948597952\",\n",
    "    \"ap-south-1\": \"763008648453\",\n",
    "    \"ap-northeast-1\": \"941853720454\",\n",
    "    \"ap-northeast-2\": \"151534178276\",\n",
    "    \"ap-southeast-1\": \"324986816169\",\n",
    "    \"ap-southeast-2\": \"355873309152\",\n",
    "    \"cn-northwest-1\": \"474822919863\",\n",
    "    \"cn-north-1\": \"472730292857\",\n",
    "    \"sa-east-1\": \"756306329178\",\n",
    "    \"ca-central-1\": \"464438896020\",\n",
    "    \"me-south-1\": \"836785723513\",\n",
    "    \"af-south-1\": \"774647643957\",\n",
    "}\n",
    "\n",
    "\n",
    "if region not in account_id_map.keys():\n",
    "    raise (\"UNSUPPORTED REGION\")\n",
    "\n",
    "print(f\"SageMaker Role: {role}\")\n",
    "print(f\"Region Name: {region}\")\n",
    "\n",
    "base = \"amazonaws.com.cn\" if region.startswith(\"cn-\") else \"amazonaws.com\"\n",
    "triton_image_uri = \"{account_id}.dkr.ecr.{region}.{base}/sagemaker-tritonserver:23.07-py3\".format(\n",
    "    account_id=account_id_map[region], region=region, base=base\n",
    ")\n",
    "\n",
    "print(f\"Triton Inference server DLC image: {triton_image_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08353cfe-76c0-46e5-9036-d3b5bcd50c49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tar -cvzf model.tar.gz bert_model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60509191-b2c6-44bd-b027-a433dfb12248",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# we make a 200 copies of the tarball, this will take about ~6 minutes to finish (can vary depending on model size)\n",
    "for i in range(200):\n",
    "    with open(\"model.tar.gz\", \"rb\") as f:\n",
    "        s3_client.upload_fileobj(f, bucket, \"{}/model-{}.tar.gz\".format(s3_model_prefix,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2101a101-76fa-4806-a883-bc6d9c967b3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mme_artifacts = \"s3://{}/{}/\".format(bucket, s3_model_prefix) #location of model data\n",
    "mme_artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ba946f-c5d0-4e7d-b4e6-892fc0a4eeb6",
   "metadata": {},
   "source": [
    "### Endpoint Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8586f702-5b7c-4504-ba6b-d6f9c5f7122e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Step 1: Model Creation\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "\n",
    "model_name = \"triton-bert-mme\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "container = {\n",
    "    \"Image\": triton_image_uri,\n",
    "    \"ModelDataUrl\": mme_artifacts,\n",
    "    \"Mode\": \"MultiModel\"\n",
    "}\n",
    "\n",
    "create_model_response = client.create_model(\n",
    "    ModelName=model_name, ExecutionRoleArn=role, PrimaryContainer=container\n",
    ")\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c122ee-8b08-497f-8b37-da84dcf551df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_config_name = \"triton-epc-mme-gpu\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "endpoint_config_response = client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"tritontraffic\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InstanceType\": \"ml.g4dn.4xlarge\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"InitialVariantWeight\": 1\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(\"Endpoint Configuration Arn: \" + endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5b690b-c48c-41d2-9c5a-b54a8ed6aea0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = \"triton-mme-gpu-ep\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "create_endpoint_response = client.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a05126a-00d7-4441-82c0-bab7443addc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Monitor creation\n",
    "describe_endpoint_response = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "while describe_endpoint_response[\"EndpointStatus\"] == \"Creating\":\n",
    "    describe_endpoint_response = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    print(describe_endpoint_response[\"EndpointStatus\"])\n",
    "    time.sleep(60)\n",
    "print(describe_endpoint_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12adc617-ebeb-40a2-bced-c31ec0e8c432",
   "metadata": {},
   "source": [
    "### Sample Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3569b2-9c7d-41f1-8d8d-910bed7ae697",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, ContentType=\"application/octet-stream\", \n",
    "    Body=json.dumps(payload), TargetModel='model-199.tar.gz'\n",
    ")\n",
    "print(json.loads(response[\"Body\"].read().decode(\"utf8\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
