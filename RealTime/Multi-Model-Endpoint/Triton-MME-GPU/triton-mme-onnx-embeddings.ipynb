{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dee04f1-b7ef-4e8f-9a4c-3fe0296f614c",
   "metadata": {},
   "source": [
    "## Triton Embeddings Onnx Model MME Example\n",
    "\n",
    "In this example we take the following sample embeddings model and make a 30 copies of it and load it into SageMaker MME for GPU inference with Triton Inference Server. Note that you want to adjust this depending on a realistic use-case with different models (thereby model sizes).\n",
    "\n",
    "<b>Sample Model</b>: https://huggingface.co/sentence-transformers/msmarco-bert-base-dot-v5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a4b7c7-d3eb-4fac-88d6-c4e6d036ec96",
   "metadata": {},
   "source": [
    "### Setup & Local Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219b59e1-58e6-474d-b840-6089512aa512",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU pip awscli boto3 sagemaker\n",
    "!pip install nvidia-pyindex --quiet\n",
    "!pip install tritonclient[http] --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767eccc6-8723-4bf0-b7ad-be3977d5eef8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import boto3, json, sagemaker, time\n",
    "from sagemaker import get_execution_role\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tritonclient.http as httpclient\n",
    "\n",
    "# variables\n",
    "s3_client = boto3.client(\"s3\")\n",
    "ts = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "# sagemaker variables\n",
    "role = get_execution_role()\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime_sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "sagemaker_session = sagemaker.Session(boto_session=boto3.Session())\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"onnx-mme-embed\"\n",
    "# endpoint variables\n",
    "sm_model_name = f\"{prefix}-mdl-{ts}\"\n",
    "endpoint_config_name = f\"{prefix}-epc-{ts}\"\n",
    "endpoint_name = f\"{prefix}-ep-{ts}\"\n",
    "model_data_url = f\"s3://{bucket}/{prefix}/\"\n",
    "\n",
    "# account mapping for SageMaker MME Triton Image\n",
    "account_id_map = {\n",
    "    \"us-east-1\": \"785573368785\",\n",
    "    \"us-east-2\": \"007439368137\",\n",
    "    \"us-west-1\": \"710691900526\",\n",
    "    \"us-west-2\": \"301217895009\",\n",
    "    \"eu-west-1\": \"802834080501\",\n",
    "    \"eu-west-2\": \"205493899709\",\n",
    "    \"eu-west-3\": \"254080097072\",\n",
    "    \"eu-north-1\": \"601324751636\",\n",
    "    \"eu-south-1\": \"966458181534\",\n",
    "    \"eu-central-1\": \"746233611703\",\n",
    "    \"ap-east-1\": \"110948597952\",\n",
    "    \"ap-south-1\": \"763008648453\",\n",
    "    \"ap-northeast-1\": \"941853720454\",\n",
    "    \"ap-northeast-2\": \"151534178276\",\n",
    "    \"ap-southeast-1\": \"324986816169\",\n",
    "    \"ap-southeast-2\": \"355873309152\",\n",
    "    \"cn-northwest-1\": \"474822919863\",\n",
    "    \"cn-north-1\": \"472730292857\",\n",
    "    \"sa-east-1\": \"756306329178\",\n",
    "    \"ca-central-1\": \"464438896020\",\n",
    "    \"me-south-1\": \"836785723513\",\n",
    "    \"af-south-1\": \"774647643957\",\n",
    "}\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "if region not in account_id_map.keys():\n",
    "    raise (\"UNSUPPORTED REGION\")\n",
    "\n",
    "base = \"amazonaws.com.cn\" if region.startswith(\"cn-\") else \"amazonaws.com\"\n",
    "\n",
    "# triton image being utilized, latest images available are here: https://github.com/aws/deep-learning-containers/blob/master/available_images.md\n",
    "mme_triton_image_uri = (\n",
    "    \"{account_id}.dkr.ecr.{region}.{base}/sagemaker-tritonserver:22.12-py3\".format(\n",
    "        account_id=account_id_map[region], region=region, base=base\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69849e0-1580-4612-85c3-5e82c1537d69",
   "metadata": {},
   "source": [
    "## Local Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f69ce7-f0aa-4d94-a7d9-525c6af69db7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/msmarco-bert-base-dot-v5\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/msmarco-bert-base-dot-v5\")\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "# Sentences we want sentence embeddings for\n",
    "query = \"How many people live in London?\"\n",
    "encoded_input = tokenizer(query, padding=True, truncation=True, return_tensors='pt')\n",
    "#print(encoded_input)\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input, return_dict=True)\n",
    "    #print(model_output)\n",
    "# Perform pooling\n",
    "embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "#embeddings.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7a8b0c-c656-45bc-94a1-1cfedd3422d2",
   "metadata": {},
   "source": [
    "### Export Onnx\n",
    "\n",
    "Reference: https://huggingface.co/docs/transformers/v4.29.1/serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60de053e-803f-4c64-ace8-c0c73842998d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import transformers\n",
    "from transformers.onnx import FeaturesManager\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/msmarco-bert-base-dot-v5\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/msmarco-bert-base-dot-v5\")\n",
    "\n",
    "# load config\n",
    "model_kind, model_onnx_config = FeaturesManager.check_supported_model_or_raise(model)\n",
    "onnx_config = model_onnx_config(model.config)\n",
    "\n",
    "# export\n",
    "onnx_inputs, onnx_outputs = transformers.onnx.export(\n",
    "        preprocessor=tokenizer,\n",
    "        model=model,\n",
    "        config=onnx_config,\n",
    "        opset=13,\n",
    "        output=Path(\"model.onnx\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30a64a4-0006-4d60-ac88-09e5c6542fac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "mkdir workspace\n",
    "mv model.onnx workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b131b477-3330-41a1-9b72-36f6ea70f584",
   "metadata": {},
   "source": [
    "### Triton Setup\n",
    "\n",
    "Adjust the config.pbtxt for the inputs/outputs your onnx model is expecting. Recommend testing the config with Docker before SageMaker Deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01558c36-452b-4125-938b-b697d27beed1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p triton-serve-onnx/sentence/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061d12df-8f00-4bce-b863-b97b6459376b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile triton-serve-onnx/sentence/config.pbtxt\n",
    "name: \"sentence_onnx\"\n",
    "platform: \"onnxruntime_onnx\"\n",
    "input: [\n",
    "    {\n",
    "        name: \"input_ids\"\n",
    "        data_type: TYPE_INT64\n",
    "        dims: [ -1, -1 ]\n",
    "    },\n",
    "    {\n",
    "        name: \"token_type_ids\"\n",
    "        data_type: TYPE_INT64\n",
    "        dims: [ -1, -1 ]\n",
    "    },\n",
    "    {\n",
    "        name: \"attention_mask\"\n",
    "        data_type: TYPE_INT64\n",
    "        dims: [ -1, -1 ]\n",
    "    }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"last_hidden_state\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ -1, -1, 768 ]\n",
    "  }\n",
    "]\n",
    "instance_group {\n",
    "  count: 1\n",
    "  kind: KIND_GPU\n",
    "}\n",
    "dynamic_batching {\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0edc9d1-5448-4225-891e-9f5ff7e9e618",
   "metadata": {},
   "source": [
    "### SageMaker Endpoint Creation\n",
    "\n",
    "SageMaker expects the model artifacts in a model.tar.gz format. There are three steps in a SageMaker Endpoint Config:\n",
    "\n",
    "- <b>SageMaker Model</b>: Points towards the model data and any inference scripts.\n",
    "- <b>SageMaker Endpoint Config</b>: Any variants and defines hardware for endpoint.\n",
    "- <b>SageMaker Endpoint</b>: REST Endpoint that you can invoke and specify TargetModel as a header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af32edd9-5cf4-45b0-ad59-197b8b532dc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p triton-serve-onnx/sentence/1/\n",
    "!cp -f workspace/model.onnx triton-serve-onnx/sentence/1/\n",
    "!tar -C triton-serve-onnx/ -czf model.tar.gz sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6841e4c2-7dab-4bc4-baba-4955075f4853",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "for i in range(0,30):\n",
    "    with open(\"model.tar.gz\", \"rb\") as f:\n",
    "        s3.upload_fileobj(f, bucket, \"mme-onnx/onnx-{}.tar.gz\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ac8f2a-d718-403a-a3aa-c77773d28799",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_data_url = f\"s3://{bucket}/mme-onnx/\"\n",
    "model_data_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911cb30d-97a3-478a-a9fc-b9cc54f95924",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 ls {model_data_url} # can see 30 copies of model, replace with actual tarballs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef40c87c-bca5-4d6b-9041-7ebb78a12634",
   "metadata": {},
   "outputs": [],
   "source": [
    "container = {\"Image\": mme_triton_image_uri, \"ModelDataUrl\": model_data_url, \"Mode\": \"MultiModel\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ca153a-8ded-4e83-b822-29176ec0a477",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=sm_model_name, ExecutionRoleArn=role, PrimaryContainer=container\n",
    ")\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a93cfb-6152-4607-ad55-83d8a19a3ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.g5.2xlarge\",\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": sm_model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9361d285-43cc-424c-8477-f14f766cd3d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5016090f-7f76-4bef-b72b-1fa2b43dbda2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(90)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2ebd32-2be2-4a23-a747-b7e9a8302c5b",
   "metadata": {},
   "source": [
    "### Sample Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb8e855-d0f1-40b1-9ecf-2e79eef536cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prepare client payload\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/msmarco-bert-base-dot-v5\")\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokenized_text = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "    payload = {}\n",
    "    payload[\"inputs\"] = []\n",
    "    payload[\"inputs\"].append(\n",
    "        {\n",
    "            \"name\": \"input_ids\",\n",
    "            \"shape\": tokenized_text.input_ids.shape,\n",
    "            \"datatype\": \"INT64\",\n",
    "            \"data\": tokenized_text.input_ids.tolist(),\n",
    "        }\n",
    "    )\n",
    "    payload[\"inputs\"].append(\n",
    "        {\n",
    "            \"name\": \"token_type_ids\",\n",
    "            \"shape\": tokenized_text.token_type_ids.shape,\n",
    "            \"datatype\": \"INT64\",\n",
    "            \"data\": tokenized_text.token_type_ids.tolist(),\n",
    "        }\n",
    "    )\n",
    "    payload[\"inputs\"].append(\n",
    "        {\n",
    "            \"name\": \"attention_mask\",\n",
    "            \"shape\": tokenized_text.attention_mask.shape,\n",
    "            \"datatype\": \"INT64\",\n",
    "            \"data\": tokenized_text.attention_mask.tolist(),\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return payload\n",
    "sampPayload = tokenize_text([\"This is a test\"])\n",
    "sampPayload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c440a6-1f2c-4d68-9f4b-a7a4c84a1d60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/octet-stream\",\n",
    "    Body=json.dumps(sampPayload),\n",
    "    TargetModel=\"onnx-5.tar.gz\", #replace with s3 target model\n",
    ")\n",
    "\n",
    "response = json.loads(response[\"Body\"].read().decode(\"utf8\"))\n",
    "output = response[\"outputs\"][0][\"data\"]\n",
    "\n",
    "#print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b503f8b-bed7-4868-80bf-c83d948e18b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# randomly invoke models behind MME\n",
    "for i in range (0,30):\n",
    "    target_model = \"onnx-{}.tar.gz\".format(i)\n",
    "    response = runtime_sm_client.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType=\"application/octet-stream\",\n",
    "            TargetModel=target_model,\n",
    "            Body=json.dumps(sampPayload))\n",
    "    print(f\"Target Model Invoked: {target_model}\")\n",
    "    response = json.loads(response[\"Body\"].read().decode(\"utf8\"))\n",
    "    output = response[\"outputs\"][0][\"data\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
