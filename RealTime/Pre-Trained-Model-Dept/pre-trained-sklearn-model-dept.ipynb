{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a24d9f72-a40d-41b3-bf4e-77c4652d90b4",
   "metadata": {},
   "source": [
    "# Pre-Trained SKLearn Model Deployment on SageMaker Real-Time Endpoints\n",
    "\n",
    "In this sample we take a dummy SKLearn regression model and showcase how you can deploy it to a [SageMaker Real-Time Endpoint](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html) using the [Boto3 AWS Python SDK](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html) and higher level [SageMaker Python SDK](https://github.com/aws/sagemaker-python-sdk) in conjunction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f1e3c4-3b0b-4b97-8907-a1e0cfebba3d",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We will be working in a ml.c5.large in SageMaker Studio using JupyterLab. We then install the SDKs we are utilizing to interact with SageMaker along with scikit-learn for some dummy local model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9398e4-0819-49a6-85bd-994b63a5c135",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U sagemaker boto3 scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6941896-5228-4f6d-aa3d-40ee34359bbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import image_uris\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import joblib\n",
    "import pickle\n",
    "import tarfile\n",
    "import sagemaker\n",
    "from sagemaker.estimator import Estimator\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "import subprocess\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "region = sess._region_name\n",
    "account_id = sess.account_id()\n",
    "s3_model_prefix = \"djl-sme-sklearn-regression\" \n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1317989e-0f20-4cbb-acd5-b15aa03128bc",
   "metadata": {},
   "source": [
    "## Sample Local Model Training\n",
    "Here we generate some artificial data and train a SKLearn Linear Regression model on it and capture the model artifacts which is a joblib file in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08aa25be-9fa9-4187-b00e-c57c0fe449b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dd4b20-8bc3-4bc4-ab51-6adc6083602f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate dummy data\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 1)\n",
    "y = 2 * X + 1 + 0.1 * np.random.randn(100, 1)  \n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Linear Regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125ee655-498d-44ef-aa00-6a9a50bc465a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the trained model to a file\n",
    "import joblib\n",
    "model_filename = \"model.joblib\"\n",
    "joblib.dump(model, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c7d4cd-3307-41bf-a277-9671c7a90ffd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serialized_model = joblib.load(model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b76c1b-1774-449a-ad3a-bcab5d72fa20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sample inference\n",
    "payload = [[0.5]]\n",
    "res = serialized_model.predict(payload).tolist()[0]\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93a8c62-6be1-4837-8b2d-67f504ea0a25",
   "metadata": {},
   "source": [
    "## SageMaker Artifact Setup\n",
    "SageMaker expects a model.tar.gz with the model data/weights and any inference scripts. Here we prepare our inference script in the format that our Model Server/Container in [DJL Serving](https://github.com/deepjavalibrary/djl-serving/tree/master) expects. Note that each model server has a different protocol or format for which it may expect the artifacts to be packaged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10663588-716a-4cfd-8703-6763f0f63dc9",
   "metadata": {},
   "source": [
    "### Inference Script Creation\n",
    "Here we can customize model loading, pre/post processing, for DJL Serving the handle method is what must be implemented and picked up on by the model server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700c4928-c4d9-45a4-affe-b6a62c7aff6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile model.py\n",
    "#!/usr/bin/env python\n",
    "#\n",
    "# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file\n",
    "# except in compliance with the License. A copy of the License is located at\n",
    "#\n",
    "# http://aws.amazon.com/apache2.0/\n",
    "#\n",
    "# or in the \"LICENSE.txt\" file accompanying this file. This file is distributed on an \"AS IS\"\n",
    "# BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, express or implied. See the License for\n",
    "# the specific language governing permissions and limitations under the License.\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import joblib\n",
    "from djl_python import Input\n",
    "from djl_python import Output\n",
    "\n",
    "\n",
    "class SKLearnRegressor(object):\n",
    "    def __init__(self):\n",
    "        self.initialized = False\n",
    "\n",
    "    def initialize(self, properties: dict):\n",
    "        \"\"\"\n",
    "        Initialize model.\n",
    "        \"\"\"\n",
    "        print(os.listdir())\n",
    "        if os.path.exists(\"model.joblib\"):\n",
    "            self.model = joblib.load(os.path.join(\"model.joblib\"))\n",
    "        else:\n",
    "            raise ValueError(\"Expecting a model.joblib artifact for SKLearn Model Loading\")\n",
    "        self.initialized = True\n",
    "\n",
    "    def inference(self, inputs):\n",
    "        \"\"\"\n",
    "        Custom service entry point function.\n",
    "\n",
    "        :param inputs: the Input object holds a list of numpy array\n",
    "        :return: the Output object to be send back\n",
    "        \"\"\"\n",
    "\n",
    "        #sample input: [[0.5]]\n",
    "        \n",
    "        try:\n",
    "            data = inputs.get_as_json()\n",
    "            print(data)\n",
    "            print(type(data))\n",
    "            res = self.model.predict(data).tolist()[0]\n",
    "            outputs = Output()\n",
    "            outputs.add_as_json(res)\n",
    "        except Exception as e:\n",
    "            logging.exception(\"inference failed\")\n",
    "            # error handling\n",
    "            outputs = Output().error(str(e))\n",
    "        \n",
    "        print(outputs)\n",
    "        print(type(outputs))\n",
    "        print(\"Returning inference---------\")\n",
    "        return outputs\n",
    "\n",
    "\n",
    "_service = SKLearnRegressor()\n",
    "\n",
    "\n",
    "def handle(inputs: Input):\n",
    "    \"\"\"\n",
    "    Default handler function\n",
    "    \"\"\"\n",
    "    if not _service.initialized:\n",
    "        # stateful model\n",
    "        _service.initialize(inputs.get_properties())\n",
    "    \n",
    "    if inputs.is_empty():\n",
    "        return None\n",
    "\n",
    "    return _service.inference(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c853edb-6654-4f5b-9b03-f71abd54af6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "numpy\n",
    "joblib\n",
    "scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9105b1d4-5a6e-4cfa-936d-fec0c59c53c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile serving.properties\n",
    "engine=Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29074849-fd8d-474c-8de9-98ec5a9cc366",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Build tar file with model data + inference code, replace this cell with your model.joblib\n",
    "bashCommand = \"tar -cvpzf model.tar.gz model.joblib requirements.txt model.py serving.properties\"\n",
    "process = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)\n",
    "output, error = process.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fc57c0-e25e-48e8-a13b-75e25f8d24d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# upload model data to S3\n",
    "with open(\"model.tar.gz\", \"rb\") as f:\n",
    "    s3_client.upload_fileobj(f, bucket, \"{}/model.tar.gz\".format(s3_model_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d02493-2b45-4b8d-b35c-95b53ffdb2bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sme_artifacts = \"s3://{}/{}/{}\".format(bucket, s3_model_prefix, \"model.tar.gz\")\n",
    "sme_artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245e7b7e-8587-4eb8-b34b-4de30df4ff2e",
   "metadata": {},
   "source": [
    "### Container Specification\n",
    "This is where you specify the container/model server for your model, in this case we use the DJL CPU based image as we are dealing with a smaller CPU based model. For a list of all the managed images by AWS please refer to this link: https://github.com/aws/deep-learning-containers/blob/master/available_images.md. You can also optionally bring your own container where you have your own serving logic implemented, here's a sample of that: https://github.com/RamVegiraju/SageMaker-Deployment/tree/master/RealTime/BYOC/PreTrained-Examples/SpacyNER?source=post_page-----37211d8412f4--------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15411da2-9bef-495f-996a-d6dd8b9c488b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# replace this with your ECR image URI based off of your region, we are utilizing the CPU image here\n",
    "inference_image_uri = '763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.29.0-cpu-full'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a54f705-1696-4bea-a006-c0314b4aa102",
   "metadata": {},
   "source": [
    "## SageMaker Constructs\n",
    "There are three SageMaker constructs for endpoints, we've linked the three respective API calls as well:\n",
    "\n",
    "1. [SageMaker Model Object](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker/client/create_model.html): Points towards model data (model.tar.gz) and container\n",
    "2. [SageMaker Endpoint Configuration](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker/client/create_endpoint_config.html): Specifies the hardware and any production variants\n",
    "3. [SageMaker Endpoint](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker/client/create_endpoint.html): The persistent REST endpoint that you can invoke and attach scaling policies to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372d6240-c5e5-4872-aef7-1bd047de0eda",
   "metadata": {},
   "source": [
    "### SageMaker Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe02e24-0e3f-4b87-830c-c09c21e58df7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Step 1: Model Creation\n",
    "sme_model_name = \"sklearn-djl-sme\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(\"Model name: \" + sme_model_name)\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=sme_model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\"Image\": inference_image_uri, \"Mode\": \"SingleModel\", \"ModelDataUrl\": sme_artifacts},\n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Created Model: {model_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8e7ec0-f792-457e-a8da-020335c22760",
   "metadata": {},
   "source": [
    "### SageMaker Endpoint Config Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4676f296-e22c-4854-ac27-486e09d91ce0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Step 2: EPC Creation\n",
    "sme_epc_name = \"sklearn-djl-sme-epc\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=sme_epc_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"sklearnvariant\",\n",
    "            \"ModelName\": sme_model_name,\n",
    "            \"InstanceType\": \"ml.c5.xlarge\",\n",
    "            \"InitialInstanceCount\": 1\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(\"Endpoint Configuration Arn: \" + endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855afa60-eb75-4e3c-bb06-20eb0b540264",
   "metadata": {},
   "source": [
    "### SageMaker Endpoint Creation\n",
    "This step can take a few minutes as the endpoint resources are being prepared (can vary depending on hardware you have behind endpoint)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ea9b1a-313d-4cef-b878-f2646f51d4bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Step 3: EP Creation\n",
    "sme_endpoint_name = \"sklearn-djl-ep-sme\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=sme_endpoint_name,\n",
    "    EndpointConfigName=sme_epc_name,\n",
    ")\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60c2641-5d38-4d4d-ad7d-11a551b9dcd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Monitor creation\n",
    "describe_endpoint_response = sm_client.describe_endpoint(EndpointName=sme_endpoint_name)\n",
    "while describe_endpoint_response[\"EndpointStatus\"] == \"Creating\":\n",
    "    describe_endpoint_response = sm_client.describe_endpoint(EndpointName=sme_endpoint_name)\n",
    "    print(describe_endpoint_response[\"EndpointStatus\"])\n",
    "    time.sleep(15)\n",
    "print(describe_endpoint_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa03cbe-8c3e-4201-8955-0218448f558f",
   "metadata": {},
   "source": [
    "## Sample Invocation\n",
    "We use the boto3 runtime client (different from client we used to create resources) to invoke the model with the following API call: https://boto3.amazonaws.com/v1/documentation/api/1.35.9/reference/services/sagemaker-runtime/client/invoke_endpoint.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946d9b57-a4ed-414c-bb0c-829f8bd8dcde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "content_type = \"application/json\"\n",
    "request_body = '[[0.5]]' #replace with your request body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb97f06-adad-428e-8436-91ae32f02336",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = smr_client.invoke_endpoint(\n",
    "    EndpointName=sme_endpoint_name,\n",
    "    ContentType=content_type,\n",
    "    Body=request_body)\n",
    "result = json.loads(response['Body'].read().decode())\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4fcb33-1a10-413f-9af4-1bb4b5414a2f",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "Ensure to delete your endpoint to avoid incurring further costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfe4b49-49f6-41a3-b6db-603a53689a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint(EndpointName = sme_endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
