{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bring Your Own Model with SageMaker Script Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will demonstrate how you can bring your own model by using custom training and inference scripts, similar to those you would use outside of SageMaker, with SageMaker's prebuilt containers for various frameworks like Scikit-learn, PyTorch, and XGBoost.\n",
    "\n",
    "SageMaker Script Mode is flexible so you'll also be seeing examples of how to include your own dependencies, such as a custom Python library, in your training and inference.\n",
    "\n",
    "The following diagram provides a solution overview:\n",
    "\n",
    "<img title=\"SageMaker Script Mode\" alt=\"Solution diagram\" src=\"solution-diagram.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To follow along, you need to create an IAM role, SageMaker Notebook instance, and S3 bucket. You may click on the CloudFormation button which will create the aforementioned resources and clone the `amazon-sagemaker-examples` GitHub repo into the notebook instance. [![Launch Stack](https://s3.amazonaws.com/cloudformation-examples/cloudformation-launch-stack.png)](https://console.aws.amazon.com/cloudformation/home#/stacks/new?stackName=ScriptModeDemo&templateURL=https://script-mode-blog.s3.amazonaws.com/script-mode-blog-cfn.yml). Give the S3bucket a unique name; you can also give the CloudFormation stack and notebook unique names such as \"script mode\". You can leave the other default settings in the CloudFormation template.\n",
    "\n",
    "Once the SageMaker Notebook instance is created, choose `conda_python3` as the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import subprocess\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import os\n",
    "import boto3\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.xgboost import XGBoost\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.serializers import NumpySerializer, JSONSerializer, CSVSerializer\n",
    "from sagemaker.deserializers import NumpyDeserializer, JSONDeserializer\n",
    "from sagemaker.predictor import Predictor\n",
    "from generate_synthetic_housing_data import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure your SageMaker version is updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker Python SDK version 2.x is required\n",
    "original_version = sagemaker.__version__\n",
    "if sagemaker.__version__ != \"2.24.1\":\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"sagemaker==2.24.1\"])\n",
    "    import importlib\n",
    "\n",
    "    importlib.reload(sagemaker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "# Useful SageMaker variables\n",
    "try:\n",
    "    # You're using a SageMaker notebook\n",
    "    sess = sagemaker.Session()\n",
    "    bucket = sess.default_bucket()\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    # You're using a notebook somewhere else\n",
    "    print(\"Setting role and SageMaker session manually...\")\n",
    "    bucket = \"bobby-demo\"\n",
    "    region = \"us-west-2\"\n",
    "\n",
    "    iam = boto3.client(\"iam\")\n",
    "    sagemaker_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "    sagemaker_execution_role_name = (\n",
    "        \"AmazonSageMaker-ExecutionRole-20200630T141851\"  # Change this to your role name\n",
    "    )\n",
    "    role = iam.get_role(RoleName=sagemaker_execution_role_name)[\"Role\"][\"Arn\"]\n",
    "    boto3.setup_default_session(region_name=region, profile_name=\"default\")\n",
    "    sess = sagemaker.Session(sagemaker_client=sagemaker_client, default_bucket=bucket)\n",
    "\n",
    "# Local data paths\n",
    "train_dir = os.path.join(os.getcwd(), \"data/train\")\n",
    "test_dir = os.path.join(os.getcwd(), \"data/test\")\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# Data paths in S3\n",
    "s3_prefix = \"script-mode-workflow\"\n",
    "csv_s3_prefix = f\"{s3_prefix}/csv\"\n",
    "csv_s3_uri = f\"s3://{bucket}/{s3_prefix}/csv\"\n",
    "numpy_train_s3_prefix = f\"{s3_prefix}/numpy/train\"\n",
    "numpy_train_s3_uri = f\"s3://{bucket}/{numpy_train_s3_prefix}\"\n",
    "numpy_test_s3_prefix = f\"{s3_prefix}/numpy/test\"\n",
    "numpy_test_s3_uri = f\"s3://{bucket}/{numpy_test_s3_prefix}\"\n",
    "csv_train_s3_uri = f\"{csv_s3_uri}/train\"\n",
    "csv_test_s3_uri = f\"{csv_s3_uri}/test\"\n",
    "\n",
    "# Enable Local Mode training\n",
    "enable_local_mode_training = False\n",
    "\n",
    "# Endpoint names\n",
    "pytorch_endpoint_name = \"pytorch-endpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker instance route table setup is ok. We are good to go.\n",
      "SageMaker instance routing for Docker is ok. We are good to go!\n"
     ]
    }
   ],
   "source": [
    "!wget -q https://raw.githubusercontent.com/aws-samples/amazon-sagemaker-script-mode/master/local_mode_setup.sh\n",
    "!wget -q https://raw.githubusercontent.com/aws-samples/amazon-sagemaker-script-mode/master/daemon.json\n",
    "!/bin/bash ./local_mode_setup.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Synthetic Housing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all the examples below, we'll be generating a synthetic housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = generate_houses(1506)\n",
    "\n",
    "# Get training columns\n",
    "train_cols = list(df.columns)\n",
    "del train_cols[-1]\n",
    "train_cols\n",
    "\n",
    "# Split data\n",
    "training_index = math.floor(0.8 * df.shape[0])\n",
    "x_train, y_train = df[train_cols][:training_index], df.PRICE[:training_index]\n",
    "x_test, y_test = df[train_cols][training_index:], df.PRICE[training_index:]\n",
    "\n",
    "# Scale price\n",
    "y_train = y_train / 100000\n",
    "y_test = y_test / 100000\n",
    "\n",
    "# Standardize data\n",
    "x_train_np = StandardScaler().fit_transform(x_train)\n",
    "x_test_np = StandardScaler().fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR_BUILT</th>\n",
       "      <th>SQUARE_FEET</th>\n",
       "      <th>NUM_BEDROOMS</th>\n",
       "      <th>NUM_BATHROOMS</th>\n",
       "      <th>LOT_ACRES</th>\n",
       "      <th>GARAGE_SPACES</th>\n",
       "      <th>FRONT_PORCH</th>\n",
       "      <th>DECK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1992</td>\n",
       "      <td>2803.582434</td>\n",
       "      <td>2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1994</td>\n",
       "      <td>2542.606331</td>\n",
       "      <td>4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.41</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1986</td>\n",
       "      <td>2791.261353</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.54</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2005</td>\n",
       "      <td>3842.292603</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.07</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2005</td>\n",
       "      <td>2495.968753</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   YEAR_BUILT  SQUARE_FEET  NUM_BEDROOMS  NUM_BATHROOMS  LOT_ACRES  \\\n",
       "0        1992  2803.582434             2            1.5       1.19   \n",
       "1        1994  2542.606331             4            1.5       1.41   \n",
       "2        1986  2791.261353             3            1.0       0.54   \n",
       "3        2005  3842.292603             6            1.0       1.07   \n",
       "4        2005  2495.968753             3            1.0       1.15   \n",
       "\n",
       "   GARAGE_SPACES  FRONT_PORCH  DECK  \n",
       "0              0            1     1  \n",
       "1              0            0     1  \n",
       "2              2            1     1  \n",
       "3              1            1     1  \n",
       "4              1            0     1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rearrange dataframe for SageMaker training and scale price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(data=x_train_np)\n",
    "train_df.columns = x_train.columns\n",
    "train_df[\"PRICE\"] = y_train / 100000\n",
    "first_col = train_df.pop(\"PRICE\")\n",
    "train_df.insert(0, \"PRICE\", first_col)\n",
    "\n",
    "test_df = pd.DataFrame(data=x_test_np)\n",
    "test_df.columns = x_test.columns\n",
    "test_df[\"PRICE\"] = y_test.reset_index(drop=True) / 100000\n",
    "first_col = test_df.pop(\"PRICE\")\n",
    "test_df.insert(0, \"PRICE\", first_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save as both CSV and Numpy data types to demonstrate data type flexibility in model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as CSV\n",
    "train_df.to_csv(f\"{train_dir}/train.csv\", header=False, index=False)\n",
    "test_df.to_csv(f\"{test_dir}/test.csv\", header=False, index=False)\n",
    "\n",
    "# Save as Numpy\n",
    "np.save(os.path.join(train_dir, \"x_train.npy\"), x_train_np)\n",
    "np.save(os.path.join(test_dir, \"x_test.npy\"), x_test_np)\n",
    "np.save(os.path.join(train_dir, \"y_train.npy\"), y_train)\n",
    "np.save(os.path.join(test_dir, \"y_test.npy\"), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_resource_bucket = boto3.Session().resource(\"s3\").Bucket(bucket)\n",
    "s3_resource_bucket.Object(os.path.join(csv_s3_prefix, \"train.csv\")).upload_file(\n",
    "    \"data/train/train.csv\"\n",
    ")\n",
    "s3_resource_bucket.Object(os.path.join(csv_s3_prefix, \"test.csv\")).upload_file(\"data/test/test.csv\")\n",
    "s3_resource_bucket.Object(os.path.join(numpy_train_s3_prefix, \"x_train.npy\")).upload_file(\n",
    "    \"data/train/x_train.npy\"\n",
    ")\n",
    "s3_resource_bucket.Object(os.path.join(numpy_train_s3_prefix, \"y_train.npy\")).upload_file(\n",
    "    \"data/train/y_train.npy\"\n",
    ")\n",
    "s3_resource_bucket.Object(os.path.join(numpy_test_s3_prefix, \"x_test.npy\")).upload_file(\n",
    "    \"data/test/x_test.npy\"\n",
    ")\n",
    "s3_resource_bucket.Object(os.path.join(numpy_test_s3_prefix, \"y_test.npy\")).upload_file(\n",
    "    \"data/test/y_test.npy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second \"level\" of script mode is the ability to modularize and logically organize your custom training jobs, models, and inference processes.\n",
    "\n",
    "Sometimes keeping all your code in one Python file can be unwieldy. Script Mode gives you the flexibility to parse out your code into multiple Python files. To illustrate this feature we build a custom PyTorch model and logically separate the model definition from the the training and inference logic. This is done by stipulating the source directory when defining your SageMaker training estimator (illustrated below). Once again, the model is not supported \"out-of-the-box\", but the PyTorch framework is and can be leveraged in the same manner as scikit-learn was in the previous example.\n",
    "\n",
    "In this PyTorch example, we want to separate the actual neural network definition from the rest of the code by putting it into its own file as demonstrated in the `pytorch_script/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-08 21:28:38 Starting - Starting the training job...\n",
      "2021-08-08 21:28:39 Starting - Launching requested ML instancesProfilerReport-1628458117: InProgress\n",
      "......\n",
      "2021-08-08 21:30:04 Starting - Preparing the instances for training......\n",
      "2021-08-08 21:31:08 Downloading - Downloading input data\n",
      "2021-08-08 21:31:08 Training - Downloading the training image...\n",
      "2021-08-08 21:31:34 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-08-08 21:31:34,111 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-08-08 21:31:34,113 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-08-08 21:31:34,121 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-08-08 21:31:40,340 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-08-08 21:31:40,565 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-08-08 21:31:40,575 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-08-08 21:31:40,585 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-08-08 21:31:40,593 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 128,\n",
      "        \"epochs\": 25,\n",
      "        \"learning_rate\": 0.01\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-model-2021-08-08-21-28-37-586\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-474422712127/pytorch-model-2021-08-08-21-28-37-586/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_deploy_pytorch_without_dependencies\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_deploy_pytorch_without_dependencies.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch_size\":128,\"epochs\":25,\"learning_rate\":0.01}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_deploy_pytorch_without_dependencies.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_deploy_pytorch_without_dependencies\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-474422712127/pytorch-model-2021-08-08-21-28-37-586/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch_size\":128,\"epochs\":25,\"learning_rate\":0.01},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-model-2021-08-08-21-28-37-586\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-474422712127/pytorch-model-2021-08-08-21-28-37-586/source/sourcedir.tar.gz\",\"module_name\":\"train_deploy_pytorch_without_dependencies\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_deploy_pytorch_without_dependencies.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch_size\",\"128\",\"--epochs\",\"25\",\"--learning_rate\",\"0.01\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=128\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=25\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.01\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 train_deploy_pytorch_without_dependencies.py --batch_size 128 --epochs 25 --learning_rate 0.01\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mx train: (1204, 8), y train: (1204,)\u001b[0m\n",
      "\u001b[34mx test: (302, 8), y test: (302,)\u001b[0m\n",
      "\u001b[34mbatch_size = 128, epochs = 25, learning rate = 0.01\u001b[0m\n",
      "\u001b[34m[2021-08-08 21:31:41.673 algo-1:26 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2021-08-08 21:31:41.673 algo-1:26 INFO hook.py:193] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2021-08-08 21:31:41.673 algo-1:26 INFO hook.py:238] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2021-08-08 21:31:41.673 algo-1:26 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2021-08-08 21:31:41.674 algo-1:26 INFO hook.py:398] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2021-08-08 21:31:41.674 algo-1:26 INFO hook.py:461] Hook is writing from the hook with pid: 26\n",
      "\u001b[0m\n",
      "\u001b[34mepoch: 1 -> loss: 8.011098861694336\u001b[0m\n",
      "\u001b[34mepoch: 2 -> loss: 3.452284574508667\u001b[0m\n",
      "\u001b[34mepoch: 3 -> loss: 2.3769378662109375\u001b[0m\n",
      "\u001b[34mepoch: 4 -> loss: 2.2539479732513428\u001b[0m\n",
      "\u001b[34mepoch: 5 -> loss: 1.9640647172927856\u001b[0m\n",
      "\u001b[34mepoch: 6 -> loss: 1.117935299873352\u001b[0m\n",
      "\u001b[34mepoch: 7 -> loss: 0.903545081615448\u001b[0m\n",
      "\u001b[34mepoch: 8 -> loss: 1.4612172842025757\u001b[0m\n",
      "\u001b[34mepoch: 9 -> loss: 1.385868787765503\u001b[0m\n",
      "\u001b[34mepoch: 10 -> loss: 1.2838661670684814\u001b[0m\n",
      "\u001b[34mepoch: 11 -> loss: 1.0817348957061768\u001b[0m\n",
      "\u001b[34mepoch: 12 -> loss: 0.8309293985366821\u001b[0m\n",
      "\u001b[34mepoch: 13 -> loss: 0.48936134576797485\u001b[0m\n",
      "\u001b[34mepoch: 14 -> loss: 0.8022270202636719\u001b[0m\n",
      "\u001b[34mepoch: 15 -> loss: 0.587830662727356\u001b[0m\n",
      "\u001b[34mepoch: 16 -> loss: 0.6456330418586731\u001b[0m\n",
      "\u001b[34mepoch: 17 -> loss: 0.6790144443511963\u001b[0m\n",
      "\u001b[34mepoch: 18 -> loss: 0.5996259450912476\u001b[0m\n",
      "\u001b[34mepoch: 19 -> loss: 0.4668491780757904\u001b[0m\n",
      "\u001b[34mepoch: 20 -> loss: 0.6089950799942017\u001b[0m\n",
      "\u001b[34mepoch: 21 -> loss: 0.3741406798362732\u001b[0m\n",
      "\u001b[34mepoch: 22 -> loss: 0.43471309542655945\u001b[0m\n",
      "\u001b[34mepoch: 23 -> loss: 0.38339707255363464\u001b[0m\n",
      "\u001b[34mepoch: 24 -> loss: 0.3418116569519043\u001b[0m\n",
      "\u001b[34mepoch: 25 -> loss: 0.3627524971961975\n",
      "\u001b[0m\n",
      "\u001b[34mTest MSE: 0.331673860651884\u001b[0m\n",
      "\u001b[34mCreated a folder at /opt/ml/model/code/!\u001b[0m\n",
      "\u001b[34mSaving models files to /opt/ml/model/code/\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 1 -> loss: 8.011098861694336\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 2 -> loss: 3.452284574508667\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 3 -> loss: 2.3769378662109375\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 4 -> loss: 2.2539479732513428\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 5 -> loss: 1.9640647172927856\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 6 -> loss: 1.117935299873352\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 7 -> loss: 0.903545081615448\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 8 -> loss: 1.4612172842025757\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 9 -> loss: 1.385868787765503\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 10 -> loss: 1.2838661670684814\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 11 -> loss: 1.0817348957061768\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 12 -> loss: 0.8309293985366821\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 13 -> loss: 0.48936134576797485\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 14 -> loss: 0.8022270202636719\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 15 -> loss: 0.587830662727356\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 16 -> loss: 0.6456330418586731\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 17 -> loss: 0.6790144443511963\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 18 -> loss: 0.5996259450912476\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 19 -> loss: 0.4668491780757904\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 20 -> loss: 0.6089950799942017\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 21 -> loss: 0.3741406798362732\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 22 -> loss: 0.43471309542655945\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 23 -> loss: 0.38339707255363464\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 24 -> loss: 0.3418116569519043\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch: 25 -> loss: 0.3627524971961975\u001b[0m\n",
      "\u001b[34mINFO:__main__:Created a folder at /opt/ml/model/code/!\u001b[0m\n",
      "\u001b[34mINFO:__main__:Saving models files to /opt/ml/model/code/\n",
      "\u001b[0m\n",
      "\u001b[34m2021-08-08 21:31:42,295 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2021-08-08 21:32:05 Uploading - Uploading generated training model\n",
      "2021-08-08 21:32:05 Completed - Training job completed\n",
      "Training seconds: 63\n",
      "Billable seconds: 63\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = {\"epochs\": 25, \"batch_size\": 128, \"learning_rate\": 0.01}\n",
    "\n",
    "if enable_local_mode_training:\n",
    "    train_instance_type = \"local\"\n",
    "    inputs = {\"train\": f\"file://{train_dir}\", \"test\": f\"file://{test_dir}\"}\n",
    "else:\n",
    "    train_instance_type = \"ml.c5.xlarge\"\n",
    "    inputs = {\"train\": numpy_train_s3_uri, \"test\": numpy_test_s3_uri}\n",
    "\n",
    "estimator_parameters = {\n",
    "    \"entry_point\": \"train_deploy_pytorch_without_dependencies.py\",\n",
    "    \"source_dir\": \"pytorch_script\",\n",
    "    \"instance_type\": train_instance_type,\n",
    "    \"instance_count\": 1,\n",
    "    \"hyperparameters\": hyperparameters,\n",
    "    \"role\": role,\n",
    "    \"base_job_name\": \"pytorch-model\",\n",
    "    \"framework_version\": \"1.5\",\n",
    "    \"py_version\": \"py3\",\n",
    "}\n",
    "\n",
    "estimator = PyTorch(**estimator_parameters)\n",
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, after the estimator finishes training, we can deploy it to a SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "existing_endpoints = sess.sagemaker_client.list_endpoints(\n",
    "    NameContains=pytorch_endpoint_name, MaxResults=30\n",
    ")[\"Endpoints\"]\n",
    "if not existing_endpoints:\n",
    "    pytorch_predictor = estimator.deploy(\n",
    "        initial_instance_count=1, instance_type=\"ml.m5.xlarge\", endpoint_name=pytorch_endpoint_name\n",
    "    )\n",
    "else:\n",
    "    pytorch_predictor = Predictor(\n",
    "        endpoint_name=\"pytorch-endpoint\",\n",
    "        sagemaker_session=sess,\n",
    "        serializer=JSONSerializer(),\n",
    "        deserializer=JSONDeserializer(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use the endpoint to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.109783172607422"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_predictor.serializer = JSONSerializer()\n",
    "pytorch_predictor.deserializer = JSONDeserializer()\n",
    "\n",
    "pytorch_predictor.predict(x_test.values[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
